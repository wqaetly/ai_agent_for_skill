"""
LangGraph HTTP æœåŠ¡å™¨
ä¸º agent-chat-ui æä¾›å…¼å®¹çš„ HTTP API æ¥å£
æ”¯æŒæŠ€èƒ½åˆ†æã€ç”Ÿæˆã€æœç´¢ç­‰åŠŸèƒ½
"""

import os
import sys
import logging
import asyncio
import json
import warnings
from typing import Dict, Any, List, Optional, AsyncIterator
from datetime import datetime
from contextlib import asynccontextmanager

# è¿‡æ»¤å·²çŸ¥çš„å¼ƒç”¨è­¦å‘Š
warnings.filterwarnings("ignore", category=DeprecationWarning, module="asyncio")
warnings.filterwarnings("ignore", message="Core Pydantic V1 functionality")

# Windows å…¼å®¹æ€§ï¼špsycopg éœ€è¦ SelectorEventLoop
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from orchestration import (
    get_skill_generation_graph,
    get_progressive_skill_generation_graph,
    get_action_batch_skill_generation_graph,
    get_skill_search_graph,
    get_skill_detail_graph,
)
from orchestration.smart_router import (
    smart_route,
    get_available_graphs,
    GRAPH_SKILL_GENERATION,
    GRAPH_PROGRESSIVE,
    GRAPH_ACTION_BATCH,
    GRAPH_SKILL_SEARCH,
    GRAPH_SKILL_DETAIL,
)
from orchestration.graphs.utils import init_checkpointer, close_checkpointer
from config import retry, server, rag, timeout, cors

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ğŸ”¥ P0æ”¹è¿›ï¼šçŠ¶æ€æŒä¹…åŒ–å·²è¿ç§»åˆ° LangGraph Checkpoint
# ä¸å†éœ€è¦å…¨å±€å†…å­˜å­˜å‚¨ï¼Œæ‰€æœ‰çŠ¶æ€ç”± SqliteSaver è‡ªåŠ¨ç®¡ç†
# æ¯ä¸ªå›¾åœ¨ compile() æ—¶å·²é…ç½® checkpointerï¼ŒçŠ¶æ€ä¼šè‡ªåŠ¨æŒä¹…åŒ–åˆ° SQLite


# ==================== æ•°æ®æ¨¡å‹ ====================

class Message(BaseModel):
    """æ¶ˆæ¯æ¨¡å‹ï¼ˆå…¼å®¹ LangGraph æ ‡å‡†ï¼‰"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: human, ai, system")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")
    id: Optional[str] = Field(None, description="æ¶ˆæ¯ID")
    name: Optional[str] = Field(None, description="å‘é€è€…åç§°")


class ThreadState(BaseModel):
    """çº¿ç¨‹çŠ¶æ€æ¨¡å‹"""
    messages: List[Message] = Field(default_factory=list, description="å¯¹è¯å†å²")
    requirement: Optional[str] = Field(None, description="ç”¨æˆ·éœ€æ±‚")
    similar_skills: List[Dict[str, Any]] = Field(default_factory=list)
    generated_json: Optional[str] = Field(None)
    validation_errors: List[str] = Field(default_factory=list)
    retry_count: int = Field(0)
    max_retries: int = Field(default=retry.MAX_RETRIES, description="æœ€å¤§é‡è¯•æ¬¡æ•°")
    final_result: Optional[Dict[str, Any]] = Field(None)


class StreamInput(BaseModel):
    """æµå¼è¾“å…¥æ¨¡å‹"""
    input: Dict[str, Any] = Field(..., description="è¾“å…¥æ•°æ®")
    config: Optional[Dict[str, Any]] = Field(default_factory=dict)
    stream_mode: str = Field("values", description="æµå¼æ¨¡å¼")


class RunsStreamRequest(BaseModel):
    """è¿è¡Œæµè¯·æ±‚ï¼ˆå…¼å®¹ LangGraph APIï¼‰"""
    input: Dict[str, Any]
    config: Optional[Dict[str, Any]] = None
    stream_mode: Optional[List[str]] = ["values"]
    assistant_id: Optional[str] = None


# ==================== åº”ç”¨ç”Ÿå‘½å‘¨æœŸ ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    logger.info("ğŸš€ LangGraph Server starting...")

    # åˆå§‹åŒ– PostgreSQL checkpointer
    try:
        await init_checkpointer()
        logger.info("âœ… PostgreSQL checkpointer initialized")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize PostgreSQL checkpointer: {e}")
        logger.warning("âš ï¸  è¯·ç¡®ä¿ PostgreSQL å·²å¯åŠ¨: docker-compose -f docker-compose.pgvector.yml up -d")

    # é¢„åŠ è½½å›¾
    try:
        get_skill_generation_graph()
        get_progressive_skill_generation_graph()  # æ¸è¿›å¼ç”Ÿæˆå›¾
        get_action_batch_skill_generation_graph()  # Actionæ‰¹é‡å¼ç”Ÿæˆå›¾
        get_skill_search_graph()
        get_skill_detail_graph()
        logger.info("âœ… All graphs loaded successfully (including progressive and action-batch generation)")
    except Exception as e:
        logger.error(f"âŒ Failed to load graphs: {e}")

    # è‡ªåŠ¨åˆå§‹åŒ– RAG ç´¢å¼•
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        logger.info("ğŸ” Checking RAG index status...")
        engine = get_rag_engine()

        # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        try:
            skill_count = engine.vector_store.count()
            logger.info(f"Skill index count: {skill_count}")
        except Exception as e:
            logger.warning(f"Failed to get skill count: {e}")
            skill_count = 0

        try:
            action_count = engine.action_vector_store.count()
            logger.info(f"Action index count: {action_count}")
        except Exception as e:
            logger.warning(f"Failed to get action count: {e}")
            action_count = 0

        # å¦‚æœç´¢å¼•ä¸ºç©ºï¼Œè‡ªåŠ¨é‡å»º
        if skill_count == 0 or action_count == 0:
            logger.info("ğŸ“¦ Empty index detected, initializing...")

            # é‡å»ºæŠ€èƒ½ç´¢å¼•
            if skill_count == 0:
                logger.info("  â†’ Indexing skills...")
                skill_result = engine.index_skills(force_rebuild=False)
                logger.info(f"  âœ… Skills indexed: {skill_result.get('count', 0)} items in {skill_result.get('elapsed_time', 0):.2f}s")

            # é‡å»º Action ç´¢å¼•
            if action_count == 0:
                logger.info("  â†’ Indexing actions...")
                action_result = engine.index_actions(force_rebuild=False)
                logger.info(f"  âœ… Actions indexed: {action_result.get('count', 0)} items in {action_result.get('elapsed_time', 0):.2f}s")

            logger.info("ğŸ‰ RAG index initialization complete")
        else:
            logger.info(f"âœ… RAG index ready (Skills: {skill_count}, Actions: {action_count})")

    except Exception as e:
        import traceback
        logger.error(f"âŒ Failed to initialize RAG index: {e}")
        logger.error(traceback.format_exc())
        logger.warning("âš ï¸  RAGåŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œï¼Œè¯·æ‰‹åŠ¨è°ƒç”¨ POST /rag/index/rebuild")

    yield

    # å…³é—­èµ„æº
    logger.info("ğŸ›‘ LangGraph Server shutting down...")
    await close_checkpointer()


# ==================== FastAPI åº”ç”¨ ====================

app = FastAPI(
    title="SkillRAG LangGraph Server",
    description="æŠ€èƒ½åˆ†æä¸ç”Ÿæˆçš„ LangGraph æœåŠ¡å™¨",
    version="1.0.0",
    lifespan=lifespan
)

# CORS é…ç½®ï¼ˆä½¿ç”¨é…ç½®æ¨¡å—ï¼Œæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
# ç”Ÿäº§ç¯å¢ƒè®¾ç½®: ALLOWED_ORIGINS=https://your-domain.com
app.add_middleware(
    CORSMiddleware,
    allow_origins=cors.origins_list,
    allow_credentials=cors.ALLOW_CREDENTIALS,
    allow_methods=cors.methods_list,
    allow_headers=cors.headers_list,
)


# ==================== è¾…åŠ©å‡½æ•° ====================

def convert_to_langgraph_messages(messages: List[Message]) -> List[Dict[str, Any]]:
    """å°†æ¶ˆæ¯è½¬æ¢ä¸º LangGraph æ ¼å¼"""
    from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

    result = []
    for msg in messages:
        if msg.role == "human":
            result.append(HumanMessage(content=msg.content))
        elif msg.role == "ai":
            result.append(AIMessage(content=msg.content))
        elif msg.role == "system":
            result.append(SystemMessage(content=msg.content))
    return result


def normalize_langgraph_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å°† LangGraph Cloud æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼

    å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
    - role å¯èƒ½åœ¨ "type" æˆ– "role" å­—æ®µ
    - content å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨

    Args:
        messages: LangGraph Cloud æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨

    Returns:
        æ ‡å‡†åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨
    """
    normalized = []
    for msg in messages:
        # æå– role (type å­—æ®µä¼˜å…ˆ)
        role = msg.get("type", msg.get("role", "human"))

        # æå– content (å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨)
        content = msg.get("content", "")
        if isinstance(content, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæå–ç¬¬ä¸€ä¸ª text å†…å®¹
            content = content[0].get("text", "") if content else ""

        normalized.append({
            "id": msg.get("id"),
            "role": role,
            "content": content,
            "name": msg.get("name")
        })

    return normalized


def build_initial_state(
    assistant_id: str,
    requirement: str,
    thread_id: str,
    normalized_messages: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    æ ¹æ® assistant_id æ„å»ºåˆå§‹çŠ¶æ€

    Args:
        assistant_id: åŠ©æ‰‹ç±»å‹ ("progressive-skill-generation" æˆ–å…¶ä»–)
        requirement: ç”¨æˆ·éœ€æ±‚æè¿°
        thread_id: çº¿ç¨‹ID
        normalized_messages: æ ‡å‡†åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨

    Returns:
        å¯¹åº”ç±»å‹çš„åˆå§‹çŠ¶æ€å­—å…¸
    """
    # è½¬æ¢æ¶ˆæ¯ä¸º LangGraph æ ¼å¼
    langgraph_messages = convert_to_langgraph_messages([
        Message(
            role=msg["role"],
            content=msg["content"],
            id=msg.get("id"),
            name=msg.get("name")
        )
        for msg in normalized_messages
    ]) if normalized_messages else []

    # å…¬å…±å­—æ®µ
    base_state = {
        "requirement": requirement,
        "similar_skills": [],
        "thread_id": thread_id,
        "messages": langgraph_messages,
    }

    if assistant_id == "progressive-skill-generation":
        # æ¸è¿›å¼ç”Ÿæˆä½¿ç”¨ ProgressiveSkillGenerationState
        return {
            **base_state,
            # é˜¶æ®µ1è¾“å‡º
            "skill_skeleton": {},
            "skeleton_validation_errors": [],
            # é˜¶æ®µ2çŠ¶æ€
            "track_plan": [],
            "current_track_index": 0,
            "current_track_data": {},
            "generated_tracks": [],
            "current_track_errors": [],
            "track_retry_count": 0,
            "max_track_retries": retry.MAX_TRACK_RETRIES,
            # é˜¶æ®µ3è¾“å‡º
            "assembled_skill": {},
            "final_validation_errors": [],
        }
    elif assistant_id == "action-batch-skill-generation":
        # Actionæ‰¹é‡å¼ç”Ÿæˆä½¿ç”¨ ActionBatchProgressiveState
        return {
            **base_state,
            # é˜¶æ®µ1
            "skill_skeleton": {},
            "skeleton_validation_errors": [],
            "track_plan": [],
            # é˜¶æ®µ2
            "current_track_index": 0,
            "current_track_batch_plan": [],
            # é˜¶æ®µ3
            "current_batch_index": 0,
            "current_batch_actions": [],
            "current_batch_errors": [],
            "batch_retry_count": 0,
            "max_batch_retries": 2,
            # è¯­ä¹‰ä¸Šä¸‹æ–‡
            "batch_context": {},
            # Tokenç›‘æ§
            "total_tokens_used": 0,
            "batch_token_history": [],
            "token_budget": 100000,
            "adaptive_batch_size": 3,
            # é˜¶æ®µ4
            "accumulated_track_actions": [],
            "generated_tracks": [],
            # é˜¶æ®µ5
            "assembled_skill": {},
            "final_validation_errors": [],
            # å…¼å®¹å­—æ®µ
            "final_result": {},
            "is_valid": False,
        }
    else:
        # æ ‡å‡†æŠ€èƒ½ç”Ÿæˆä½¿ç”¨ SkillGenerationState
        return {
            **base_state,
            "generated_json": "",
            "validation_errors": [],
            "retry_count": 0,
            "max_retries": retry.MAX_RETRIES,
            "final_result": {},
        }


def convert_from_langgraph_messages(messages: List[Any]) -> List[Dict[str, str]]:
    """å°† LangGraph æ¶ˆæ¯è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼"""
    result = []
    for msg in messages:
        msg_type = msg.__class__.__name__.lower()
        message_type = "human" if "human" in msg_type else "ai" if "ai" in msg_type else "system"
        result.append({
            "type": message_type,  # å‰ç«¯æœŸæœ› type å­—æ®µï¼Œä¸æ˜¯ role
            "content": msg.content,
            "id": getattr(msg, "id", None)
        })
    return result


def serialize_event_data(data: Any) -> Any:
    """
    é€’å½’åºåˆ—åŒ–äº‹ä»¶æ•°æ®ï¼Œå¤„ç† LangChain æ¶ˆæ¯å¯¹è±¡

    Args:
        data: å¾…åºåˆ—åŒ–çš„æ•°æ®

    Returns:
        å¯ JSON åºåˆ—åŒ–çš„æ•°æ®
    """
    from langchain_core.messages import BaseMessage

    if isinstance(data, BaseMessage):
        # è½¬æ¢ LangChain æ¶ˆæ¯ä¸ºå­—å…¸
        msg_type = data.__class__.__name__.lower()
        message_type = "human" if "human" in msg_type else "ai" if "ai" in msg_type else "system"

        # æå– thinking æ ‡è®°
        is_thinking = False
        if hasattr(data, 'additional_kwargs') and isinstance(data.additional_kwargs, dict):
            is_thinking = data.additional_kwargs.get("thinking", False)

        result = {
            "type": message_type,  # å‰ç«¯æœŸæœ› type å­—æ®µï¼Œä¸æ˜¯ role
            "content": data.content,
            "id": getattr(data, "id", None)
        }

        # å¦‚æœæ˜¯æ€è€ƒæ¶ˆæ¯ï¼Œæ·»åŠ  thinking å­—æ®µ
        if is_thinking:
            result["thinking"] = True

        return result
    elif isinstance(data, dict):
        # é€’å½’å¤„ç†å­—å…¸
        return {k: serialize_event_data(v) for k, v in data.items()}
    elif isinstance(data, list):
        # é€’å½’å¤„ç†åˆ—è¡¨
        return [serialize_event_data(item) for item in data]
    else:
        # å…¶ä»–ç±»å‹ç›´æ¥è¿”å›
        return data


async def stream_graph_updates(
    graph,
    initial_state: Dict[str, Any],
    thread_id: str,
    http_request: Request = None  # P1-4: æ·»åŠ  Request å¯¹è±¡ç”¨äºæ£€æµ‹å®¢æˆ·ç«¯æ–­å¼€
) -> AsyncIterator[str]:
    """
    æµå¼è¾“å‡ºå›¾çš„æ›´æ–°

    Args:
        graph: LangGraph å›¾å®ä¾‹
        initial_state: åˆå§‹çŠ¶æ€
        thread_id: çº¿ç¨‹ID
        http_request: FastAPI Request å¯¹è±¡ï¼ˆç”¨äºæ£€æµ‹å®¢æˆ·ç«¯æ–­å¼€ï¼‰

    Yields:
        SSE æ ¼å¼çš„äº‹ä»¶æ•°æ®
    """
    # ğŸ”¥ å¿ƒè·³æœºåˆ¶ï¼šé˜²æ­¢è¿æ¥è¶…æ—¶
    last_event_time = asyncio.get_event_loop().time()
    HEARTBEAT_INTERVAL = 15  # æ¯15ç§’å‘é€å¿ƒè·³
    
    # P1-4: å®¢æˆ·ç«¯æ–­å¼€æ£€æµ‹è®¡æ•°å™¨
    disconnect_check_counter = 0
    DISCONNECT_CHECK_INTERVAL = 10  # æ¯10ä¸ªäº‹ä»¶æ£€æµ‹ä¸€æ¬¡
    
    async def maybe_send_heartbeat():
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€å¿ƒè·³"""
        nonlocal last_event_time
        current_time = asyncio.get_event_loop().time()
        if current_time - last_event_time > HEARTBEAT_INTERVAL:
            last_event_time = current_time
            return True
        return False
    
    async def is_client_disconnected() -> bool:
        """P1-4: æ£€æµ‹å®¢æˆ·ç«¯æ˜¯å¦å·²æ–­å¼€è¿æ¥"""
        if http_request is None:
            return False
        try:
            return await http_request.is_disconnected()
        except Exception:
            return False

    try:
        logger.info(f"Starting stream for thread {thread_id}")
        event_count = 0
        accumulated_state = {}

        try:
            # ğŸ”¥ ä¼ é€’ thread_id åˆ° config
            config = {"configurable": {"thread_id": thread_id}}

            # ğŸ”¥ ä½¿ç”¨å¤šä¸ª stream_modeï¼š
            # - "values": å›¾çŠ¶æ€æ›´æ–°
            # - "messages": LLM token çº§åˆ«æµå¼è¾“å‡ºï¼ˆLangGraph Studio éœ€è¦ï¼‰
            # - "custom": è‡ªå®šä¹‰äº‹ä»¶
            async for stream_mode, event in graph.astream(
                initial_state,
                config=config,
                stream_mode=["values", "messages", "custom"]
            ):
                event_count += 1
                disconnect_check_counter += 1
                last_event_time = asyncio.get_event_loop().time()  # ğŸ”¥ æ›´æ–°æœ€åäº‹ä»¶æ—¶é—´
                
                # P1-4: å®šæœŸæ£€æµ‹å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€ï¼ˆé¿å…æ¯æ¬¡éƒ½æ£€æµ‹å½±å“æ€§èƒ½ï¼‰
                if disconnect_check_counter >= DISCONNECT_CHECK_INTERVAL:
                    disconnect_check_counter = 0
                    if await is_client_disconnected():
                        logger.info(f"Client disconnected for thread {thread_id}, stopping stream")
                        return
                
                # ğŸ”¥ å¤„ç† messages æ¨¡å¼ï¼ˆLLM token æµï¼‰
                if stream_mode == "messages":
                    # messages æ¨¡å¼è¿”å› (message_chunk, metadata) å…ƒç»„
                    try:
                        message_chunk, metadata = event
                        # è·å– token å†…å®¹
                        content = ""
                        if hasattr(message_chunk, 'content'):
                            content = message_chunk.content
                        elif isinstance(message_chunk, dict):
                            content = message_chunk.get('content', '')
                        
                        if content:
                            # ğŸ”¥ SDK æœŸæœ› messages äº‹ä»¶æ•°æ®æ˜¯ [message_dict, metadata] æ•°ç»„æ ¼å¼
                            # å‚è§: @langchain/langgraph-sdk/dist/ui/manager.js:
                            #   const [serialized, metadata] = data;
                            message_dict = {
                                "content": content,
                                "type": "ai",
                                "id": getattr(message_chunk, 'id', None) if hasattr(message_chunk, 'id') else None,
                            }
                            metadata_dict = {
                                "langgraph_node": metadata.get("langgraph_node", "unknown") if isinstance(metadata, dict) else "unknown"
                            }
                            # å‘é€ [message, metadata] æ•°ç»„æ ¼å¼
                            messages_event = [message_dict, metadata_dict]
                            event_json = json.dumps(messages_event, ensure_ascii=False)
                            yield f"event: messages\ndata: {event_json}\n\n"
                    except Exception as e:
                        logger.debug(f"Messages event processing: {e}")
                    continue

                # ğŸ”¥ å¤„ç† custom æ¨¡å¼ï¼ˆè‡ªå®šä¹‰äº‹ä»¶ï¼‰
                if stream_mode == "custom":
                    try:
                        event_json = json.dumps(event, ensure_ascii=False)
                        yield f"event: custom\ndata: {event_json}\n\n"
                    except Exception as e:
                        logger.error(f"Custom event encoding error: {e}")
                    continue

                # å¤„ç† values äº‹ä»¶ï¼ˆå›¾çŠ¶æ€æ›´æ–°ï¼‰
                # åºåˆ—åŒ–äº‹ä»¶æ•°æ®
                try:
                    serialized_event = serialize_event_data(event)
                except Exception as e:
                    logger.error(f"Serialization error: {e}")
                    continue

                # ä»èŠ‚ç‚¹è¾“å‡ºä¸­æå– messages åˆ°é¡¶å±‚
                # LangGraph è¾“å‡ºæ ¼å¼ï¼š{node_name: {messages: [...], ...}}
                # å‰ç«¯æœŸæœ›æ ¼å¼ï¼š{messages: [...], node_name: {...}}
                flattened_state = {}
                for node_name, node_output in serialized_event.items():
                    if isinstance(node_output, dict) and 'messages' in node_output:
                        # å°† messages æå‡åˆ°é¡¶å±‚
                        flattened_state['messages'] = node_output['messages']
                        # ä¿ç•™èŠ‚ç‚¹è¾“å‡ºï¼ˆä¸åŒ…å« messagesï¼‰
                        flattened_state[node_name] = {k: v for k, v in node_output.items() if k != 'messages'}
                    else:
                        # ä¿ç•™å…¶ä»–å­—æ®µ
                        flattened_state[node_name] = node_output

                # ğŸ”¥ ä¿®å¤ï¼švalues æ¨¡å¼è¿”å›å®Œæ•´çŠ¶æ€ï¼Œç›´æ¥è¦†ç›–è€Œéè¿½åŠ 
                # ä¹‹å‰çš„ extend é€»è¾‘ä¼šå¯¼è‡´æ¶ˆæ¯é‡å¤
                if 'messages' in flattened_state:
                    accumulated_state['messages'] = flattened_state['messages']
                    flattened_state = {k: v for k, v in flattened_state.items() if k != 'messages'}

                # æ›´æ–°å…¶ä»–çŠ¶æ€å­—æ®µ
                accumulated_state.update(flattened_state)

                # å‘é€æ ‡å‡† SSE äº‹ä»¶ï¼ˆå‘é€ç´¯ç§¯çŠ¶æ€ï¼‰
                try:
                    event_json = json.dumps(accumulated_state, ensure_ascii=False)
                    yield f"event: values\ndata: {event_json}\n\n"
                except Exception as e:
                    logger.error(f"JSON encoding error: {e}")
                    continue

                await asyncio.sleep(0.001)
        except Exception as e:
            logger.error(f"Stream iteration error: {e}", exc_info=True)
            raise

        # å‘é€ç»“æŸäº‹ä»¶
        logger.info(f"Stream completed for thread {thread_id} with {event_count} events")
        final_state_json = json.dumps(accumulated_state, ensure_ascii=False)
        yield f"event: end\ndata: {final_state_json}\n\n"

    except Exception as e:
        logger.error(f"Stream error: {e}", exc_info=True)
        error_data = {"error": str(e)}
        yield f"event: error\ndata: {json.dumps(error_data, ensure_ascii=False)}\n\n"


# ==================== API ç«¯ç‚¹ ====================

@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹"""
    return {
        "service": "SkillRAG LangGraph Server",
        "version": "1.0.0",
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "endpoints": {
            "threads": "/threads/{thread_id}/runs/stream",
            "assistants": "/assistants",
            "health": "/health",
            "thread_state": "/threads/{thread_id}/state",
            "thread_resume": "/threads/{thread_id}/resume",
            "thread_history": "/threads/{thread_id}/history",
            "rag": {
                "search": "/rag/search",
                "recommend_actions": "/rag/recommend-actions",
                "recommend_parameters": "/rag/recommend-parameters",
                "rebuild_index": "/rag/index/rebuild",
                "stats": "/rag/index/stats",
                "clear_cache": "/rag/cache",
                "health": "/rag/health"
            }
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }


@app.get("/metrics")
async def get_metrics():
    """
    P2-4: è·å–æ€§èƒ½ç›‘æ§æŒ‡æ ‡
    
    è¿”å› LLM è°ƒç”¨å»¶è¿Ÿã€RAG æ£€ç´¢è€—æ—¶ã€éªŒè¯å¾ªç¯æ¬¡æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯
    """
    from orchestration.metrics import get_performance_summary
    return {
        "timestamp": datetime.now().isoformat(),
        "metrics": get_performance_summary()
    }


@app.get("/info")
async def server_info():
    """
    æœåŠ¡å™¨ä¿¡æ¯ç«¯ç‚¹ï¼ˆå…¼å®¹ LangGraph Cloud/Studioï¼‰

    å‰ç«¯ä½¿ç”¨æ­¤ç«¯ç‚¹æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
    """
    return {
        "version": "1.0.0",
        "name": "SkillRAG LangGraph Server",
        "description": "æŠ€èƒ½åˆ†æä¸ç”Ÿæˆçš„ LangGraph æœåŠ¡å™¨",
        "status": "ready",
        "timestamp": datetime.now().isoformat()
    }


@app.get("/assistants")
async def list_assistants():
    """åˆ—å‡ºå¯ç”¨çš„åŠ©æ‰‹ï¼ˆå›¾ï¼‰"""
    return {
        "assistants": [
            {
                "assistant_id": "smart",
                "name": "æ™ºèƒ½è·¯ç”±",
                "description": "æ ¹æ®è¾“å…¥è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ç”Ÿæˆæ–¹å¼ï¼ˆæ¨èï¼‰",
                "graph_id": "smart",
                "default": True,
                "icon": "sparkles"
            },
            {
                "assistant_id": "skill-generation",
                "name": "æ ‡å‡†æŠ€èƒ½ç”Ÿæˆ",
                "description": "ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´æŠ€èƒ½ï¼Œé€‚åˆç®€å•æŠ€èƒ½",
                "graph_id": "skill_generation",
                "icon": "zap"
            },
            {
                "assistant_id": "progressive-skill-generation",
                "name": "æ¸è¿›å¼æŠ€èƒ½ç”Ÿæˆ",
                "description": "ä¸‰é˜¶æ®µæ¸è¿›å¼ç”Ÿæˆï¼šéª¨æ¶â†’Trackâ†’ç»„è£…ï¼ˆæ¨èç”¨äºå¤æ‚æŠ€èƒ½ï¼‰",
                "graph_id": "progressive_skill_generation",
                "recommended": True,
                "icon": "layers"
            },
            {
                "assistant_id": "action-batch-skill-generation",
                "name": "Actionæ‰¹é‡å¼ç”Ÿæˆ",
                "description": "æœ€ç»†ç²’åº¦çš„æ¸è¿›å¼ç”Ÿæˆï¼Œé€‚åˆè¶…å¤æ‚æŠ€èƒ½",
                "graph_id": "action_batch_skill_generation",
                "icon": "boxes"
            },
            {
                "assistant_id": "skill-search",
                "name": "æŠ€èƒ½æœç´¢",
                "description": "è¯­ä¹‰æœç´¢æŠ€èƒ½åº“",
                "graph_id": "skill_search",
                "icon": "search"
            },
            {
                "assistant_id": "skill-detail",
                "name": "æŠ€èƒ½è¯¦æƒ…",
                "description": "æŸ¥è¯¢æŠ€èƒ½è¯¦ç»†ä¿¡æ¯",
                "graph_id": "skill_detail",
                "icon": "file-text"
            }
        ]
    }


# ==================== æ™ºèƒ½è·¯ç”± API ====================

class SmartRouteRequest(BaseModel):
    """æ™ºèƒ½è·¯ç”±è¯·æ±‚"""
    query: str = Field(..., description="ç”¨æˆ·è¾“å…¥æ–‡æœ¬")
    prefer_progressive: bool = Field(True, description="æ˜¯å¦åå¥½æ¸è¿›å¼ç”Ÿæˆ")


@app.post("/route/smart")
async def smart_route_endpoint(request: SmartRouteRequest):
    """
    æ™ºèƒ½è·¯ç”±ç«¯ç‚¹
    
    æ ¹æ®ç”¨æˆ·è¾“å…¥åˆ†æå¹¶æ¨èæœ€åˆé€‚çš„ Graph
    """
    try:
        result = smart_route(
            user_input=request.query,
            prefer_progressive=request.prefer_progressive
        )
        return {
            "success": True,
            **result
        }
    except Exception as e:
        logger.error(f"Smart route error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/route/graphs")
async def list_available_graphs():
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„ Graph åˆ—è¡¨
    """
    return {
        "graphs": get_available_graphs()
    }


# ==================== å›¾ç»“æ„å¯è§†åŒ– API ====================

def _extract_graph_structure(graph) -> Dict[str, Any]:
    """
    ä»ç¼–è¯‘åçš„å›¾ä¸­æå–èŠ‚ç‚¹å’Œè¾¹ç»“æ„
    
    Args:
        graph: ç¼–è¯‘åçš„ LangGraph
        
    Returns:
        åŒ…å« nodes å’Œ edges çš„å­—å…¸
    """
    try:
        graph_data = graph.get_graph()
        nodes = []
        edges = []
        
        # æå–èŠ‚ç‚¹
        for node_id, node_data in graph_data.nodes.items():
            node_type = "default"
            if node_id == "__start__":
                node_type = "start"
            elif node_id == "__end__":
                node_type = "end"
            
            nodes.append({
                "id": node_id,
                "label": node_id.replace("_", " ").title() if node_id not in ["__start__", "__end__"] else node_id,
                "type": node_type
            })
        
        # æå–è¾¹
        for edge in graph_data.edges:
            source = edge.source
            target = edge.target
            
            # å¤„ç†æ¡ä»¶è¾¹
            is_conditional = hasattr(edge, 'conditional') and edge.conditional
            
            edges.append({
                "source": source,
                "target": target,
                "conditional": is_conditional,
                "label": getattr(edge, 'data', None) or ""
            })
        
        return {
            "nodes": nodes,
            "edges": edges
        }
    except Exception as e:
        logger.error(f"Failed to extract graph structure: {e}")
        return {"nodes": [], "edges": []}


@app.get("/graphs/{graph_id}/structure")
async def get_graph_structure(graph_id: str):
    """
    è·å–æŒ‡å®šå›¾çš„ç»“æ„ï¼ˆèŠ‚ç‚¹å’Œè¾¹ï¼‰
    
    ç”¨äºå‰ç«¯å¯è§†åŒ–å±•ç¤ºå›¾çš„æ‰§è¡Œæµç¨‹
    """
    try:
        # æ ¹æ® graph_id è·å–å¯¹åº”çš„å›¾
        graph_map = {
            "skill-generation": get_skill_generation_graph,
            "progressive-skill-generation": get_progressive_skill_generation_graph,
            "action-batch-skill-generation": get_action_batch_skill_generation_graph,
            "skill-search": get_skill_search_graph,
            "skill-detail": get_skill_detail_graph,
        }
        
        if graph_id not in graph_map:
            raise HTTPException(status_code=404, detail=f"Graph '{graph_id}' not found")
        
        graph = graph_map[graph_id]()
        structure = _extract_graph_structure(graph)
        
        # æ·»åŠ å›¾çš„å…ƒä¿¡æ¯
        graph_info = {
            "skill-generation": {
                "name": "æ ‡å‡†æŠ€èƒ½ç”Ÿæˆ",
                "description": "æ£€ç´¢ â†’ ç”Ÿæˆ â†’ éªŒè¯ â†’ ä¿®å¤å¾ªç¯"
            },
            "progressive-skill-generation": {
                "name": "æ¸è¿›å¼æŠ€èƒ½ç”Ÿæˆ", 
                "description": "éª¨æ¶ç”Ÿæˆ â†’ Tracké€ä¸ªç”Ÿæˆ â†’ ç»„è£…"
            },
            "action-batch-skill-generation": {
                "name": "Actionæ‰¹é‡å¼ç”Ÿæˆ",
                "description": "éª¨æ¶ â†’ Trackè§„åˆ’ â†’ æ‰¹æ¬¡ç”Ÿæˆ â†’ ç»„è£…"
            },
            "skill-search": {
                "name": "æŠ€èƒ½æœç´¢",
                "description": "è¯­ä¹‰æœç´¢æŠ€èƒ½åº“"
            },
            "skill-detail": {
                "name": "æŠ€èƒ½è¯¦æƒ…",
                "description": "æŸ¥è¯¢æŠ€èƒ½è¯¦ç»†ä¿¡æ¯"
            }
        }
        
        return {
            "graph_id": graph_id,
            "info": graph_info.get(graph_id, {}),
            "structure": structure,
            "mermaid": graph.get_graph().draw_mermaid() if hasattr(graph.get_graph(), 'draw_mermaid') else None
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get graph structure error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/threads/{thread_id}/runs/stream")
async def create_run_stream(
    thread_id: str,
    request: RunsStreamRequest,
    http_request: Request  # P1-4: æ·»åŠ  Request å¯¹è±¡ç”¨äºæ£€æµ‹å®¢æˆ·ç«¯æ–­å¼€
):
    """
    åˆ›å»ºæµå¼è¿è¡Œï¼ˆå…¼å®¹ agent-chat-uiï¼‰
    
    è¿™æ˜¯ agent-chat-ui è°ƒç”¨çš„ä¸»è¦ç«¯ç‚¹
    æ”¯æŒæ™ºèƒ½è·¯ç”±ï¼šassistant_id="smart" æ—¶è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„å›¾
    """
    try:
        logger.info(f"Stream request for thread {thread_id}: {request.input}")

        # è·å–åŠ©æ‰‹IDï¼ˆé»˜è®¤ä½¿ç”¨æ™ºèƒ½è·¯ç”±ï¼‰
        assistant_id = request.assistant_id or request.config.get("configurable", {}).get("assistant_id", "smart")

        # å‡†å¤‡åˆå§‹çŠ¶æ€
        input_data = request.input
        messages = input_data.get("messages", [])

        # ä½¿ç”¨æŠ½å–çš„è¾…åŠ©å‡½æ•°è½¬æ¢æ¶ˆæ¯æ ¼å¼
        normalized_messages = normalize_langgraph_messages(messages)

        # æå–æœ€æ–°æ¶ˆæ¯ä½œä¸ºéœ€æ±‚
        if normalized_messages:
            requirement = normalized_messages[-1].get("content", "")
        else:
            requirement = input_data.get("requirement", "")

        # ğŸ”¥ æ™ºèƒ½è·¯ç”±ï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥è‡ªåŠ¨é€‰æ‹©å›¾
        routed_assistant_id = assistant_id
        routing_info = None
        if assistant_id == "smart":
            routing_info = smart_route(requirement)
            routed_assistant_id = routing_info["graph_id"]
            logger.info(f"ğŸ§  Smart routing: '{requirement[:50]}...' -> {routed_assistant_id} (confidence: {routing_info['confidence']:.2f}, reason: {routing_info['reason']})")

        # æ ¹æ®åŠ©æ‰‹IDé€‰æ‹©å›¾
        if routed_assistant_id == "skill-generation":
            graph = get_skill_generation_graph()
        elif routed_assistant_id == "progressive-skill-generation":
            graph = get_progressive_skill_generation_graph()
        elif routed_assistant_id == "action-batch-skill-generation":
            graph = get_action_batch_skill_generation_graph()
        elif routed_assistant_id == "skill-search":
            graph = get_skill_search_graph()
        elif routed_assistant_id == "skill-detail":
            graph = get_skill_detail_graph()
        else:
            raise HTTPException(status_code=404, detail=f"Assistant '{routed_assistant_id}' not found")

        # ä½¿ç”¨æŠ½å–çš„è¾…åŠ©å‡½æ•°æ„å»ºåˆå§‹çŠ¶æ€
        initial_state = build_initial_state(
            assistant_id=routed_assistant_id,
            requirement=requirement,
            thread_id=thread_id,
            normalized_messages=normalized_messages
        )

        # å¦‚æœæ˜¯æ™ºèƒ½è·¯ç”±ï¼Œæ·»åŠ è·¯ç”±ä¿¡æ¯åˆ°çŠ¶æ€
        if routing_info:
            initial_state["routing_info"] = routing_info

        # è¿”å›æµå¼å“åº”
        return StreamingResponse(
            stream_graph_updates(graph, initial_state, thread_id, http_request),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
        
    except Exception as e:
        logger.error(f"Error in stream endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/threads/{thread_id}/runs")
async def create_run(
    thread_id: str,
    request: RunsStreamRequest
):
    """
    åˆ›å»ºéæµå¼è¿è¡Œï¼ˆæ”¯æŒæ™ºèƒ½è·¯ç”±ï¼‰
    """
    try:
        logger.info(f"Run request for thread {thread_id}: {request.input}")

        # è·å–åŠ©æ‰‹IDï¼ˆé»˜è®¤ä½¿ç”¨æ™ºèƒ½è·¯ç”±ï¼‰
        assistant_id = request.assistant_id or request.config.get("configurable", {}).get("assistant_id", "smart")
        
        # å‡†å¤‡åˆå§‹çŠ¶æ€
        input_data = request.input
        messages = input_data.get("messages", [])

        # ä½¿ç”¨æŠ½å–çš„è¾…åŠ©å‡½æ•°è½¬æ¢æ¶ˆæ¯æ ¼å¼
        normalized_messages = normalize_langgraph_messages(messages)

        # æå–æœ€æ–°æ¶ˆæ¯ä½œä¸ºéœ€æ±‚
        if normalized_messages:
            requirement = normalized_messages[-1].get("content", "")
        else:
            requirement = input_data.get("requirement", "")

        # ğŸ”¥ æ™ºèƒ½è·¯ç”±
        routed_assistant_id = assistant_id
        routing_info = None
        if assistant_id == "smart":
            routing_info = smart_route(requirement)
            routed_assistant_id = routing_info["graph_id"]
            logger.info(f"ğŸ§  Smart routing: '{requirement[:50]}...' -> {routed_assistant_id}")

        # æ ¹æ®åŠ©æ‰‹IDé€‰æ‹©å›¾
        if routed_assistant_id == "skill-generation":
            graph = get_skill_generation_graph()
        elif routed_assistant_id == "progressive-skill-generation":
            graph = get_progressive_skill_generation_graph()
        elif routed_assistant_id == "action-batch-skill-generation":
            graph = get_action_batch_skill_generation_graph()
        elif routed_assistant_id == "skill-search":
            graph = get_skill_search_graph()
        elif routed_assistant_id == "skill-detail":
            graph = get_skill_detail_graph()
        else:
            raise HTTPException(status_code=404, detail=f"Assistant '{routed_assistant_id}' not found")

        # ä½¿ç”¨æŠ½å–çš„è¾…åŠ©å‡½æ•°æ„å»ºåˆå§‹çŠ¶æ€
        initial_state = build_initial_state(
            assistant_id=routed_assistant_id,
            requirement=requirement,
            thread_id=thread_id,
            normalized_messages=normalized_messages
        )

        # æ‰§è¡Œå›¾
        result = await graph.ainvoke(initial_state)
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        result["messages"] = convert_from_langgraph_messages(result.get("messages", []))
        
        # æ·»åŠ è·¯ç”±ä¿¡æ¯
        if routing_info:
            result["routing_info"] = routing_info
        
        return {
            "thread_id": thread_id,
            "run_id": f"run_{datetime.now().timestamp()}",
            "status": "completed",
            "routed_assistant_id": routed_assistant_id,
            "result": result
        }
        
    except Exception as e:
        logger.error(f"Error in run endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/threads")
async def create_thread():
    """
    åˆ›å»ºæ–°çº¿ç¨‹ï¼ˆå…¼å®¹ LangGraph Cloud APIï¼‰

    å‰ç«¯ä½¿ç”¨æ­¤ç«¯ç‚¹åˆ›å»ºæ–°çš„å¯¹è¯çº¿ç¨‹
    """
    thread_id = f"thread_{datetime.now().timestamp()}"
    return {
        "thread_id": thread_id,
        "created_at": datetime.now().isoformat(),
        "metadata": {},
        "status": "idle"
    }


@app.get("/threads/{thread_id}")
async def get_thread(thread_id: str):
    """è·å–çº¿ç¨‹ä¿¡æ¯"""
    return {
        "thread_id": thread_id,
        "created_at": datetime.now().isoformat(),
        "metadata": {}
    }


@app.get("/threads/{thread_id}/state")
async def get_thread_state(thread_id: str, assistant_id: str = "skill-generation"):
    """
    è·å–çº¿ç¨‹çŠ¶æ€ï¼ˆå…¼å®¹ LangGraph SDKï¼‰

    å‰ç«¯åœ¨ stream å®Œæˆåä¼šè°ƒç”¨æ­¤ç«¯ç‚¹è·å–æœ€ç»ˆçŠ¶æ€
    ğŸ”¥ P0æ”¹è¿›ï¼šä» LangGraph Checkpoint è¯»å–æŒä¹…åŒ–çŠ¶æ€
    """
    try:
        # æ ¹æ® assistant_id é€‰æ‹©å¯¹åº”çš„å›¾
        if assistant_id == "skill-generation":
            graph = get_skill_generation_graph()
        elif assistant_id == "progressive-skill-generation":
            graph = get_progressive_skill_generation_graph()
        elif assistant_id == "skill-search":
            graph = get_skill_search_graph()
        elif assistant_id == "skill-detail":
            graph = get_skill_detail_graph()
        else:
            logger.warning(f"Unknown assistant_id: {assistant_id}, using skill-generation")
            graph = get_skill_generation_graph()

        # ğŸ”¥ ä» checkpoint è¯»å–çŠ¶æ€
        config = {"configurable": {"thread_id": thread_id}}
        state_snapshot = graph.get_state(config)

        if state_snapshot is None or not state_snapshot.values:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç©ºçŠ¶æ€
            logger.warning(f"No checkpoint state found for thread {thread_id}")
            return {
                "values": {},
                "next": [],
                "config": config,
                "metadata": {},
                "created_at": datetime.now().isoformat(),
                "parent_config": None
            }

        # è¿”å› LangGraph å…¼å®¹çš„çŠ¶æ€æ ¼å¼
        return {
            "values": state_snapshot.values,  # ä» checkpoint è¯»å–çš„çŠ¶æ€
            "next": state_snapshot.next,       # ä¸‹ä¸€æ­¥èŠ‚ç‚¹
            "config": config,
            "metadata": state_snapshot.metadata or {},
            "created_at": state_snapshot.created_at.isoformat() if hasattr(state_snapshot, 'created_at') else datetime.now().isoformat(),
            "parent_config": state_snapshot.parent_config
        }
    except Exception as e:
        logger.error(f"Get thread state error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/threads/search")
async def search_threads(request: Request):
    """
    æœç´¢çº¿ç¨‹ï¼ˆå…¼å®¹ LangGraph SDKï¼‰

    å‰ç«¯ä½¿ç”¨ client.threads.search() æ—¶è°ƒç”¨æ­¤ç«¯ç‚¹
    ğŸ”¥ P0æ”¹è¿›ï¼šä» Checkpoint æŸ¥è¯¢æŒä¹…åŒ–çš„çº¿ç¨‹åˆ—è¡¨
    """
    try:
        body = await request.json() if request.headers.get("content-type") == "application/json" else {}

        # ğŸ”¥ TODO: å®ç°ä» SqliteSaver æŸ¥è¯¢çº¿ç¨‹åˆ—è¡¨
        # SqliteSaver æä¾›äº† list() æ–¹æ³•ï¼Œä½†éœ€è¦éå†æ‰€æœ‰å›¾çš„ checkpoint
        # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œæœªæ¥å¯ä»¥é€šè¿‡æŸ¥è¯¢ SQLite æ•°æ®åº“å®ç°
        logger.info("Thread search requested, returning empty list (TODO: implement checkpoint query)")
        return []

    except Exception as e:
        logger.error(f"Search threads error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/threads/{thread_id}/history")
async def get_thread_history(thread_id: str, request: Request, assistant_id: str = "skill-generation"):
    """
    è·å–çº¿ç¨‹å†å²æ¶ˆæ¯ï¼ˆå…¼å®¹ LangGraph SDKï¼‰

    è¿”å›æŒ‡å®šçº¿ç¨‹çš„å®Œæ•´å¯¹è¯å†å²
    LangGraph SDK æœŸæœ›è¿”å›ä¸€ä¸ªæ•°ç»„æ ¼å¼çš„å†å²è®°å½•
    ğŸ”¥ P0æ”¹è¿›ï¼šä» Checkpoint è¯»å–å†å²çŠ¶æ€å¿«ç…§
    """
    try:
        # æ ¹æ® assistant_id é€‰æ‹©å¯¹åº”çš„å›¾
        if assistant_id == "skill-generation":
            graph = get_skill_generation_graph()
        elif assistant_id == "progressive-skill-generation":
            graph = get_progressive_skill_generation_graph()
        elif assistant_id == "skill-search":
            graph = get_skill_search_graph()
        elif assistant_id == "skill-detail":
            graph = get_skill_detail_graph()
        else:
            graph = get_skill_generation_graph()

        # ğŸ”¥ ä» checkpoint è¯»å–å†å²çŠ¶æ€
        config = {"configurable": {"thread_id": thread_id}}

        # ä½¿ç”¨ get_state_history() è·å–æ‰€æœ‰å†å²å¿«ç…§
        history = []
        try:
            state_history = graph.get_state_history(config)
            for state_snapshot in state_history:
                history.append({
                    "values": state_snapshot.values,
                    "next": state_snapshot.next,
                    "config": state_snapshot.config,
                    "metadata": state_snapshot.metadata or {},
                    "parent_config": state_snapshot.parent_config,
                    "created_at": state_snapshot.created_at.isoformat() if hasattr(state_snapshot, 'created_at') else None
                })
        except Exception as e:
            logger.warning(f"Failed to get history for thread {thread_id}: {e}")

        return history

    except Exception as e:
        logger.error(f"Get thread history error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/threads/{thread_id}/resume")
async def resume_thread(thread_id: str, assistant_id: str = "skill-generation"):
    """
    æ¢å¤çº¿ç¨‹ä¼šè¯ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰

    è¿”å›æŒ‡å®šçº¿ç¨‹çš„æœ€æ–°çŠ¶æ€å’Œå¯¹è¯å†å²,ç”¨äºæœåŠ¡é‡å¯åæ¢å¤å¯¹è¯
    ğŸ”¥ P0æ–°åŠŸèƒ½ï¼šåˆ©ç”¨ Checkpoint å®ç°ä¼šè¯æ¢å¤
    """
    try:
        # æ ¹æ® assistant_id é€‰æ‹©å¯¹åº”çš„å›¾
        if assistant_id == "skill-generation":
            graph = get_skill_generation_graph()
        elif assistant_id == "progressive-skill-generation":
            graph = get_progressive_skill_generation_graph()
        elif assistant_id == "skill-search":
            graph = get_skill_search_graph()
        elif assistant_id == "skill-detail":
            graph = get_skill_detail_graph()
        else:
            logger.warning(f"Unknown assistant_id: {assistant_id}, using skill-generation")
            graph = get_skill_generation_graph()

        # ä» checkpoint è¯»å–æœ€æ–°çŠ¶æ€
        config = {"configurable": {"thread_id": thread_id}}
        state_snapshot = graph.get_state(config)

        if state_snapshot is None or not state_snapshot.values:
            logger.warning(f"No checkpoint found for thread {thread_id}")
            raise HTTPException(status_code=404, detail=f"Thread {thread_id} not found or has no state")

        # æå–å¯¹è¯å†å²ï¼ˆä» messages å­—æ®µï¼‰
        messages = state_snapshot.values.get("messages", [])
        converted_messages = convert_from_langgraph_messages(messages) if messages else []

        # æ„å»ºæ¢å¤å“åº”
        return {
            "thread_id": thread_id,
            "assistant_id": assistant_id,
            "status": "resumed",
            "messages": converted_messages,
            "state_summary": {
                "requirement": state_snapshot.values.get("requirement", ""),
                "retry_count": state_snapshot.values.get("retry_count", 0),
                "is_valid": state_snapshot.values.get("is_valid", None),
                "has_result": bool(state_snapshot.values.get("final_result")),
            },
            "next_nodes": state_snapshot.next,
            "created_at": state_snapshot.created_at.isoformat() if hasattr(state_snapshot, 'created_at') else datetime.now().isoformat()
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Resume thread error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# ==================== RAG ä¸“ç”¨ç«¯ç‚¹ ====================

class RAGSearchRequest(BaseModel):
    """RAGæœç´¢è¯·æ±‚"""
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢æ–‡æœ¬")
    top_k: int = Field(default=rag.DEFAULT_TOP_K, description="è¿”å›ç»“æœæ•°é‡")
    filters: Optional[Dict[str, Any]] = Field(None, description="è¿‡æ»¤æ¡ä»¶")


class RAGActionRecommendRequest(BaseModel):
    """Actionæ¨èè¯·æ±‚"""
    context: str = Field(..., description="ä¸Šä¸‹æ–‡æè¿°")
    top_k: int = Field(default=rag.RECOMMEND_TOP_K, description="æ¨èæ•°é‡")


class RAGParameterRecommendRequest(BaseModel):
    """å‚æ•°æ¨èè¯·æ±‚"""
    action_type: str = Field(..., description="Actionç±»å‹åç§°")
    skill_context: Optional[str] = Field(None, description="æŠ€èƒ½ä¸Šä¸‹æ–‡")
    parameter_name: Optional[str] = Field(None, description="å‚æ•°åç§°")


@app.post("/rag/search")
async def rag_search(request: RAGSearchRequest):
    """
    æŠ€èƒ½è¯­ä¹‰æœç´¢

    æ ¹æ®è‡ªç„¶è¯­è¨€æŸ¥è¯¢æœç´¢ç›¸ä¼¼çš„æŠ€èƒ½é…ç½®
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        logger.info(f"RAG search: query='{request.query}', top_k={request.top_k}")

        engine = get_rag_engine()
        results = engine.search_skills(
            query=request.query,
            top_k=request.top_k,
            filters=request.filters,
            return_details=True
        )

        return {
            "success": True,
            "query": request.query,
            "results": results,
            "count": len(results)
        }

    except Exception as e:
        logger.error(f"RAG search error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/rag/recommend-actions")
async def rag_recommend_actions(request: RAGActionRecommendRequest):
    """
    Actionç±»å‹æ™ºèƒ½æ¨è

    æ ¹æ®ä¸Šä¸‹æ–‡æè¿°æ¨èåˆé€‚çš„Actionç±»å‹
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        logger.info(f"Action recommendation: context='{request.context}'")

        engine = get_rag_engine()
        recommendations = engine.recommend_actions(
            context=request.context,
            top_k=request.top_k
        )

        return {
            "success": True,
            "context": request.context,
            "recommendations": recommendations,
            "count": len(recommendations)
        }

    except Exception as e:
        logger.error(f"Action recommendation error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/rag/recommend-parameters")
async def rag_recommend_parameters(request: RAGParameterRecommendRequest):
    """
    å‚æ•°æ™ºèƒ½æ¨èï¼ˆåŸUnity InspectoråŠŸèƒ½ï¼‰

    ä¸ºæŒ‡å®šActionç±»å‹æ¨èåˆé€‚çš„å‚æ•°é…ç½®
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        logger.info(f"Parameter recommendation: action_type='{request.action_type}'")

        engine = get_rag_engine()

        # æœç´¢åŒ…å«è¯¥Actionç±»å‹çš„æŠ€èƒ½
        action_search_results = engine.search_actions(
            query=request.action_type,
            top_k=rag.PARAMETER_TOP_K
        )

        # æå–å‚æ•°ç¤ºä¾‹
        parameter_examples = []
        for result in action_search_results:
            if 'action_data' in result:
                action_data = result['action_data']
                if 'parameters' in action_data:
                    parameter_examples.append({
                        "action_type": result.get('actionType', request.action_type),
                        "parameters": action_data['parameters'],
                        "source_skill": result.get('skill_name', 'Unknown'),
                        "similarity": result.get('similarity', 0.0)
                    })

        return {
            "success": True,
            "action_type": request.action_type,
            "parameter_examples": parameter_examples,
            "count": len(parameter_examples)
        }

    except Exception as e:
        logger.error(f"Parameter recommendation error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/rag/index/rebuild")
async def rag_rebuild_index():
    """
    é‡å»ºRAGç´¢å¼•

    é‡æ–°ç´¢å¼•æ‰€æœ‰æŠ€èƒ½é…ç½®æ–‡ä»¶
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        logger.info("Rebuilding RAG index...")

        engine = get_rag_engine()

        # é‡å»ºæŠ€èƒ½ç´¢å¼•
        skill_result = engine.index_skills(force_rebuild=True)

        # é‡å»ºActionç´¢å¼•
        action_result = engine.index_actions(force_rebuild=True)

        # é‡å»ºç»“æ„åŒ–ç´¢å¼•
        structured_result = engine.rebuild_structured_index(force=True)

        return {
            "success": True,
            "skill_index": skill_result,
            "action_index": action_result,
            "structured_index": structured_result,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Index rebuild error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/rag/index/stats")
async def rag_index_stats():
    """
    è·å–RAGç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine
        import os

        engine = get_rag_engine()
        stats = engine.get_statistics()

        # æå–å…³é”®ç»Ÿè®¡æ•°æ®
        vector_stats = stats.get('vector_store', {})
        action_stats = stats.get('action_stats', {})

        # è®¡ç®—ç´¢å¼•å¤§å°ï¼ˆä¼°ç®—ï¼‰
        persist_dir = vector_stats.get('persist_directory', '')
        index_size_mb = 0.0
        if persist_dir and os.path.exists(persist_dir):
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(persist_dir):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    if os.path.exists(filepath):
                        total_size += os.path.getsize(filepath)
            index_size_mb = total_size / (1024 * 1024)  # è½¬æ¢ä¸º MB

        # è¿”å›å‰ç«¯æœŸæœ›çš„æ ¼å¼
        return {
            "total_skills": vector_stats.get('total_documents', 0),
            "total_actions": action_stats.get('total_actions', 0),
            "total_parameters": int(action_stats.get('avg_params_per_action', 0) * action_stats.get('total_actions', 0)),
            "index_size_mb": round(index_size_mb, 2)
        }

    except Exception as e:
        logger.error(f"Stats retrieval error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/rag/cache")
async def rag_clear_cache():
    """
    æ¸…ç©ºRAGæŸ¥è¯¢ç¼“å­˜
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        engine = get_rag_engine()

        # æ¸…ç©ºæŸ¥è¯¢ç¼“å­˜
        if hasattr(engine, '_query_cache') and engine._query_cache is not None:
            cache_size = len(engine._query_cache)
            engine._query_cache.clear()
            logger.info(f"Cleared {cache_size} cached queries")
        else:
            cache_size = 0
            logger.info("Query cache is disabled or empty")

        return {
            "success": True,
            "cleared_entries": cache_size,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Cache clear error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/rag/health")
async def rag_health_check():
    """
    RAGæœåŠ¡å¥åº·æ£€æŸ¥

    ä¸ webui/src/app/rag/page.tsx HealthStatus æ¥å£å¯¹æ¥
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        engine = get_rag_engine()
        stats = engine.get_statistics()

        # æå–åµŒå¥—ç»Ÿè®¡æ•°æ®
        vector_stats = stats.get('vector_store', {})
        action_stats = stats.get('action_stats', {})

        return {
            "status": "healthy",
            "skill_count": vector_stats.get('total_documents', 0),
            "action_count": action_stats.get('total_actions', 0),
            "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

    except Exception as e:
        logger.warning(f"RAG health check failed: {e}")
        return {
            "status": "unhealthy",
            "skill_count": 0,
            "action_count": 0,
            "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }


# ==================== ä¸»å‡½æ•° ====================

def kill_process_on_port(port: int) -> bool:
    """
    æ€æ‰å ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹

    Args:
        port: ç«¯å£å·

    Returns:
        æ˜¯å¦æˆåŠŸæ€æ‰è¿›ç¨‹
    """
    try:
        import subprocess
        import re

        # åœ¨Windowsä¸ŠæŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
        if sys.platform == "win32":
            # ä½¿ç”¨ netstat æŸ¥æ‰¾ç«¯å£
            result = subprocess.run(
                ['netstat', '-ano'],
                capture_output=True,
                text=True,
                timeout=5
            )

            # è§£æè¾“å‡ºï¼ŒæŸ¥æ‰¾ç›‘å¬è¯¥ç«¯å£çš„è¿›ç¨‹
            for line in result.stdout.splitlines():
                if f':{port}' in line and 'LISTENING' in line:
                    # æå–PIDï¼ˆæœ€åä¸€åˆ—ï¼‰
                    parts = line.split()
                    if parts:
                        pid = parts[-1]
                        logger.info(f"ğŸ” Found process {pid} using port {port}")

                        # æ€æ‰è¿›ç¨‹
                        kill_result = subprocess.run(
                            ['taskkill', '/F', '/PID', pid],
                            capture_output=True,
                            text=True,
                            timeout=5
                        )

                        if kill_result.returncode == 0:
                            logger.info(f"âœ… Successfully killed process {pid}")
                            return True
                        else:
                            logger.warning(f"âš ï¸  Failed to kill process {pid}: {kill_result.stderr}")

            return False
        else:
            # Linux/Mac ä½¿ç”¨ lsof
            result = subprocess.run(
                ['lsof', '-ti', f':{port}'],
                capture_output=True,
                text=True,
                timeout=5
            )

            if result.stdout.strip():
                pid = result.stdout.strip()
                logger.info(f"ğŸ” Found process {pid} using port {port}")

                subprocess.run(['kill', '-9', pid], timeout=5)
                logger.info(f"âœ… Successfully killed process {pid}")
                return True

            return False

    except Exception as e:
        logger.error(f"âŒ Error killing process on port {port}: {e}")
        return False


def main():
    """å¯åŠ¨æœåŠ¡å™¨"""
    # ä½¿ç”¨é…ç½®æ¨¡å—ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
    host = server.LANGGRAPH_HOST
    port = server.LANGGRAPH_PORT

    logger.info(f"Starting LangGraph server on {host}:{port}")

    # æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
    import socket
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    result = sock.connect_ex(('localhost', port))
    sock.close()

    if result == 0:
        logger.warning(f"âš ï¸  Port {port} is already in use!")
        logger.info(f"ğŸ”„ Attempting to kill existing process...")

        if kill_process_on_port(port):
            logger.info(f"âœ… Port {port} is now free, starting server...")
            # ç­‰å¾…ç«¯å£å®Œå…¨é‡Šæ”¾
            import time
            time.sleep(timeout.PORT_RELEASE_WAIT)
        else:
            logger.error(f"âŒ Failed to free port {port}")
            logger.warning(f"    To manually find the process: netstat -ano | findstr :{port}")
            logger.warning(f"    To manually kill it: taskkill /F /PID <PID>")
            sys.exit(1)

    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info"
    )


if __name__ == "__main__":
    main()
