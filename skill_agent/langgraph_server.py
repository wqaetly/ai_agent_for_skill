"""
LangGraph HTTP æœåŠ¡å™¨
ä¸º agent-chat-ui æä¾›å…¼å®¹çš„ HTTP API æ¥å£
æ”¯æŒæŠ€èƒ½åˆ†æã€ç”Ÿæˆã€æœç´¢ç­‰åŠŸèƒ½
"""

import os
import sys
import logging
import asyncio
import json
from typing import Dict, Any, List, Optional, AsyncIterator
from datetime import datetime
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from orchestration import (
    get_skill_generation_graph,
    get_progressive_skill_generation_graph,
    get_skill_search_graph,
    get_skill_detail_graph,
)
from config import retry, server, rag, timeout, cors

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ğŸ”¥ P0æ”¹è¿›ï¼šçŠ¶æ€æŒä¹…åŒ–å·²è¿ç§»åˆ° LangGraph Checkpoint
# ä¸å†éœ€è¦å…¨å±€å†…å­˜å­˜å‚¨ï¼Œæ‰€æœ‰çŠ¶æ€ç”± SqliteSaver è‡ªåŠ¨ç®¡ç†
# æ¯ä¸ªå›¾åœ¨ compile() æ—¶å·²é…ç½® checkpointerï¼ŒçŠ¶æ€ä¼šè‡ªåŠ¨æŒä¹…åŒ–åˆ° SQLite


# ==================== æ•°æ®æ¨¡å‹ ====================

class Message(BaseModel):
    """æ¶ˆæ¯æ¨¡å‹ï¼ˆå…¼å®¹ LangGraph æ ‡å‡†ï¼‰"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: human, ai, system")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")
    id: Optional[str] = Field(None, description="æ¶ˆæ¯ID")
    name: Optional[str] = Field(None, description="å‘é€è€…åç§°")


class ThreadState(BaseModel):
    """çº¿ç¨‹çŠ¶æ€æ¨¡å‹"""
    messages: List[Message] = Field(default_factory=list, description="å¯¹è¯å†å²")
    requirement: Optional[str] = Field(None, description="ç”¨æˆ·éœ€æ±‚")
    similar_skills: List[Dict[str, Any]] = Field(default_factory=list)
    generated_json: Optional[str] = Field(None)
    validation_errors: List[str] = Field(default_factory=list)
    retry_count: int = Field(0)
    max_retries: int = Field(default=retry.MAX_RETRIES, description="æœ€å¤§é‡è¯•æ¬¡æ•°")
    final_result: Optional[Dict[str, Any]] = Field(None)


class StreamInput(BaseModel):
    """æµå¼è¾“å…¥æ¨¡å‹"""
    input: Dict[str, Any] = Field(..., description="è¾“å…¥æ•°æ®")
    config: Optional[Dict[str, Any]] = Field(default_factory=dict)
    stream_mode: str = Field("values", description="æµå¼æ¨¡å¼")


class RunsStreamRequest(BaseModel):
    """è¿è¡Œæµè¯·æ±‚ï¼ˆå…¼å®¹ LangGraph APIï¼‰"""
    input: Dict[str, Any]
    config: Optional[Dict[str, Any]] = None
    stream_mode: Optional[List[str]] = ["values"]
    assistant_id: Optional[str] = None


# ==================== åº”ç”¨ç”Ÿå‘½å‘¨æœŸ ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    logger.info("ğŸš€ LangGraph Server starting...")

    # é¢„åŠ è½½å›¾
    try:
        get_skill_generation_graph()
        get_progressive_skill_generation_graph()  # æ¸è¿›å¼ç”Ÿæˆå›¾
        get_skill_search_graph()
        get_skill_detail_graph()
        logger.info("âœ… All graphs loaded successfully (including progressive generation)")
    except Exception as e:
        logger.error(f"âŒ Failed to load graphs: {e}")

    # è‡ªåŠ¨åˆå§‹åŒ– RAG ç´¢å¼•
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        logger.info("ğŸ” Checking RAG index status...")
        engine = get_rag_engine()

        # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        try:
            skill_count = engine.vector_store.count()
            logger.info(f"Skill index count: {skill_count}")
        except Exception as e:
            logger.warning(f"Failed to get skill count: {e}")
            skill_count = 0

        try:
            action_count = engine.action_vector_store.count()
            logger.info(f"Action index count: {action_count}")
        except Exception as e:
            logger.warning(f"Failed to get action count: {e}")
            action_count = 0

        # å¦‚æœç´¢å¼•ä¸ºç©ºï¼Œè‡ªåŠ¨é‡å»º
        if skill_count == 0 or action_count == 0:
            logger.info("ğŸ“¦ Empty index detected, initializing...")

            # é‡å»ºæŠ€èƒ½ç´¢å¼•
            if skill_count == 0:
                logger.info("  â†’ Indexing skills...")
                skill_result = engine.index_skills(force_rebuild=False)
                logger.info(f"  âœ… Skills indexed: {skill_result.get('count', 0)} items in {skill_result.get('elapsed_time', 0):.2f}s")

            # é‡å»º Action ç´¢å¼•
            if action_count == 0:
                logger.info("  â†’ Indexing actions...")
                action_result = engine.index_actions(force_rebuild=False)
                logger.info(f"  âœ… Actions indexed: {action_result.get('count', 0)} items in {action_result.get('elapsed_time', 0):.2f}s")

            logger.info("ğŸ‰ RAG index initialization complete")
        else:
            logger.info(f"âœ… RAG index ready (Skills: {skill_count}, Actions: {action_count})")

    except Exception as e:
        import traceback
        logger.error(f"âŒ Failed to initialize RAG index: {e}")
        logger.error(traceback.format_exc())
        logger.warning("âš ï¸  RAGåŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œï¼Œè¯·æ‰‹åŠ¨è°ƒç”¨ POST /rag/index/rebuild")

    yield

    logger.info("ğŸ›‘ LangGraph Server shutting down...")


# ==================== FastAPI åº”ç”¨ ====================

app = FastAPI(
    title="SkillRAG LangGraph Server",
    description="æŠ€èƒ½åˆ†æä¸ç”Ÿæˆçš„ LangGraph æœåŠ¡å™¨",
    version="1.0.0",
    lifespan=lifespan
)

# CORS é…ç½®ï¼ˆä½¿ç”¨é…ç½®æ¨¡å—ï¼Œæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
# ç”Ÿäº§ç¯å¢ƒè®¾ç½®: ALLOWED_ORIGINS=https://your-domain.com
app.add_middleware(
    CORSMiddleware,
    allow_origins=cors.origins_list,
    allow_credentials=cors.ALLOW_CREDENTIALS,
    allow_methods=cors.methods_list,
    allow_headers=cors.headers_list,
)


# ==================== è¾…åŠ©å‡½æ•° ====================

def convert_to_langgraph_messages(messages: List[Message]) -> List[Dict[str, Any]]:
    """å°†æ¶ˆæ¯è½¬æ¢ä¸º LangGraph æ ¼å¼"""
    from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

    result = []
    for msg in messages:
        if msg.role == "human":
            result.append(HumanMessage(content=msg.content))
        elif msg.role == "ai":
            result.append(AIMessage(content=msg.content))
        elif msg.role == "system":
            result.append(SystemMessage(content=msg.content))
    return result


def normalize_langgraph_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å°† LangGraph Cloud æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸ºå†…éƒ¨æ ¼å¼

    å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
    - role å¯èƒ½åœ¨ "type" æˆ– "role" å­—æ®µ
    - content å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨

    Args:
        messages: LangGraph Cloud æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨

    Returns:
        æ ‡å‡†åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨
    """
    normalized = []
    for msg in messages:
        # æå– role (type å­—æ®µä¼˜å…ˆ)
        role = msg.get("type", msg.get("role", "human"))

        # æå– content (å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨)
        content = msg.get("content", "")
        if isinstance(content, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæå–ç¬¬ä¸€ä¸ª text å†…å®¹
            content = content[0].get("text", "") if content else ""

        normalized.append({
            "id": msg.get("id"),
            "role": role,
            "content": content,
            "name": msg.get("name")
        })

    return normalized


def build_initial_state(
    assistant_id: str,
    requirement: str,
    thread_id: str,
    normalized_messages: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    æ ¹æ® assistant_id æ„å»ºåˆå§‹çŠ¶æ€

    Args:
        assistant_id: åŠ©æ‰‹ç±»å‹ ("progressive-skill-generation" æˆ–å…¶ä»–)
        requirement: ç”¨æˆ·éœ€æ±‚æè¿°
        thread_id: çº¿ç¨‹ID
        normalized_messages: æ ‡å‡†åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨

    Returns:
        å¯¹åº”ç±»å‹çš„åˆå§‹çŠ¶æ€å­—å…¸
    """
    # è½¬æ¢æ¶ˆæ¯ä¸º LangGraph æ ¼å¼
    langgraph_messages = convert_to_langgraph_messages([
        Message(
            role=msg["role"],
            content=msg["content"],
            id=msg.get("id"),
            name=msg.get("name")
        )
        for msg in normalized_messages
    ]) if normalized_messages else []

    # å…¬å…±å­—æ®µ
    base_state = {
        "requirement": requirement,
        "similar_skills": [],
        "thread_id": thread_id,
        "messages": langgraph_messages,
    }

    if assistant_id == "progressive-skill-generation":
        # æ¸è¿›å¼ç”Ÿæˆä½¿ç”¨ ProgressiveSkillGenerationState
        return {
            **base_state,
            # é˜¶æ®µ1è¾“å‡º
            "skill_skeleton": {},
            "skeleton_validation_errors": [],
            # é˜¶æ®µ2çŠ¶æ€
            "track_plan": [],
            "current_track_index": 0,
            "current_track_data": {},
            "generated_tracks": [],
            "current_track_errors": [],
            "track_retry_count": 0,
            "max_track_retries": retry.MAX_TRACK_RETRIES,
            # é˜¶æ®µ3è¾“å‡º
            "assembled_skill": {},
            "final_validation_errors": [],
        }
    else:
        # æ ‡å‡†æŠ€èƒ½ç”Ÿæˆä½¿ç”¨ SkillGenerationState
        return {
            **base_state,
            "generated_json": "",
            "validation_errors": [],
            "retry_count": 0,
            "max_retries": retry.MAX_RETRIES,
            "final_result": {},
        }


def convert_from_langgraph_messages(messages: List[Any]) -> List[Dict[str, str]]:
    """å°† LangGraph æ¶ˆæ¯è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼"""
    result = []
    for msg in messages:
        msg_type = msg.__class__.__name__.lower()
        message_type = "human" if "human" in msg_type else "ai" if "ai" in msg_type else "system"
        result.append({
            "type": message_type,  # å‰ç«¯æœŸæœ› type å­—æ®µï¼Œä¸æ˜¯ role
            "content": msg.content,
            "id": getattr(msg, "id", None)
        })
    return result


def serialize_event_data(data: Any) -> Any:
    """
    é€’å½’åºåˆ—åŒ–äº‹ä»¶æ•°æ®ï¼Œå¤„ç† LangChain æ¶ˆæ¯å¯¹è±¡

    Args:
        data: å¾…åºåˆ—åŒ–çš„æ•°æ®

    Returns:
        å¯ JSON åºåˆ—åŒ–çš„æ•°æ®
    """
    from langchain_core.messages import BaseMessage

    if isinstance(data, BaseMessage):
        # è½¬æ¢ LangChain æ¶ˆæ¯ä¸ºå­—å…¸
        msg_type = data.__class__.__name__.lower()
        message_type = "human" if "human" in msg_type else "ai" if "ai" in msg_type else "system"

        # æå– thinking æ ‡è®°
        is_thinking = False
        if hasattr(data, 'additional_kwargs') and isinstance(data.additional_kwargs, dict):
            is_thinking = data.additional_kwargs.get("thinking", False)

        result = {
            "type": message_type,  # å‰ç«¯æœŸæœ› type å­—æ®µï¼Œä¸æ˜¯ role
            "content": data.content,
            "id": getattr(data, "id", None)
        }

        # å¦‚æœæ˜¯æ€è€ƒæ¶ˆæ¯ï¼Œæ·»åŠ  thinking å­—æ®µ
        if is_thinking:
            result["thinking"] = True

        return result
    elif isinstance(data, dict):
        # é€’å½’å¤„ç†å­—å…¸
        return {k: serialize_event_data(v) for k, v in data.items()}
    elif isinstance(data, list):
        # é€’å½’å¤„ç†åˆ—è¡¨
        return [serialize_event_data(item) for item in data]
    else:
        # å…¶ä»–ç±»å‹ç›´æ¥è¿”å›
        return data


async def stream_graph_updates(
    graph,
    initial_state: Dict[str, Any],
    thread_id: str
) -> AsyncIterator[str]:
    """
    æµå¼è¾“å‡ºå›¾çš„æ›´æ–°

    Args:
        graph: LangGraph å›¾å®ä¾‹
        initial_state: åˆå§‹çŠ¶æ€
        thread_id: çº¿ç¨‹ID

    Yields:
        SSE æ ¼å¼çš„äº‹ä»¶æ•°æ®
    """
    try:
        logger.info(f"Starting stream for thread {thread_id}, initial_state: {initial_state.get('requirement', 'N/A')}")
        event_count = 0

        # ä½¿ç”¨ astream è¿›è¡Œæµå¼å¤„ç†
        # ç»´æŠ¤ä¸€ä¸ªç´¯ç§¯çš„ state
        accumulated_state = {}

        try:
            # ğŸ”¥ ä¼ é€’ thread_id åˆ° config
            config = {"configurable": {"thread_id": thread_id}}

            # ğŸ”¥ ä½¿ç”¨å¤šä¸ª stream_modeï¼š
            # - "values": å›¾çŠ¶æ€æ›´æ–°
            # - "messages": LLM token çº§åˆ«æµå¼è¾“å‡ºï¼ˆLangGraph Studio éœ€è¦ï¼‰
            # - "custom": è‡ªå®šä¹‰äº‹ä»¶
            async for stream_mode, event in graph.astream(
                initial_state,
                config=config,
                stream_mode=["values", "messages", "custom"]
            ):
                event_count += 1
                
                # ğŸ”¥ å¤„ç† messages æ¨¡å¼ï¼ˆLLM token æµï¼‰
                if stream_mode == "messages":
                    # messages æ¨¡å¼è¿”å› (message_chunk, metadata) å…ƒç»„
                    try:
                        message_chunk, metadata = event
                        # è·å– token å†…å®¹
                        content = ""
                        if hasattr(message_chunk, 'content'):
                            content = message_chunk.content
                        elif isinstance(message_chunk, dict):
                            content = message_chunk.get('content', '')
                        
                        if content:
                            # ğŸ”¥ SDK æœŸæœ› messages äº‹ä»¶æ•°æ®æ˜¯ [message_dict, metadata] æ•°ç»„æ ¼å¼
                            # å‚è§: @langchain/langgraph-sdk/dist/ui/manager.js:
                            #   const [serialized, metadata] = data;
                            message_dict = {
                                "content": content,
                                "type": "ai",
                                "id": getattr(message_chunk, 'id', None) if hasattr(message_chunk, 'id') else None,
                            }
                            metadata_dict = {
                                "langgraph_node": metadata.get("langgraph_node", "unknown") if isinstance(metadata, dict) else "unknown"
                            }
                            # å‘é€ [message, metadata] æ•°ç»„æ ¼å¼
                            messages_event = [message_dict, metadata_dict]
                            event_json = json.dumps(messages_event, ensure_ascii=False)
                            yield f"event: messages\ndata: {event_json}\n\n"
                    except Exception as e:
                        logger.debug(f"Messages event processing: {e}")
                    continue

                # ğŸ”¥ å¤„ç† custom æ¨¡å¼ï¼ˆè‡ªå®šä¹‰äº‹ä»¶ï¼‰
                if stream_mode == "custom":
                    logger.info(f"ğŸ“¨ Received custom event: {event}")
                    try:
                        event_json = json.dumps(event, ensure_ascii=False)
                        logger.info(f"ğŸ“¤ Forwarding custom event with data: {event_json[:200]}...")
                        yield f"event: custom\ndata: {event_json}\n\n"
                    except Exception as e:
                        logger.error(f"âŒ Custom event encoding error: {e}", exc_info=True)
                    continue

                # å¤„ç† values äº‹ä»¶ï¼ˆå›¾çŠ¶æ€æ›´æ–°ï¼‰
                logger.debug(f"Raw values event: {event}")

                # åºåˆ—åŒ–äº‹ä»¶æ•°æ®
                try:
                    serialized_event = serialize_event_data(event)
                    logger.info(f"âœ… Event serialized successfully")
                except Exception as e:
                    logger.error(f"âŒ Serialization error: {e}", exc_info=True)
                    continue

                # ä»èŠ‚ç‚¹è¾“å‡ºä¸­æå– messages åˆ°é¡¶å±‚
                # LangGraph è¾“å‡ºæ ¼å¼ï¼š{node_name: {messages: [...], ...}}
                # å‰ç«¯æœŸæœ›æ ¼å¼ï¼š{messages: [...], node_name: {...}}
                flattened_state = {}
                for node_name, node_output in serialized_event.items():
                    if isinstance(node_output, dict) and 'messages' in node_output:
                        # å°† messages æå‡åˆ°é¡¶å±‚
                        flattened_state['messages'] = node_output['messages']
                        # ä¿ç•™èŠ‚ç‚¹è¾“å‡ºï¼ˆä¸åŒ…å« messagesï¼‰
                        flattened_state[node_name] = {k: v for k, v in node_output.items() if k != 'messages'}
                    else:
                        # ä¿ç•™å…¶ä»–å­—æ®µ
                        flattened_state[node_name] = node_output

                # ç´¯ç§¯æ¶ˆæ¯ï¼ˆè¿½åŠ è€Œéè¦†ç›–ï¼‰
                if 'messages' in flattened_state:
                    if 'messages' not in accumulated_state:
                        accumulated_state['messages'] = []

                    # è®°å½•æ–°å¢çš„æ¶ˆæ¯å†…å®¹
                    new_messages = flattened_state['messages']
                    logger.info(f"ğŸ“¨ Event contains {len(new_messages)} messages")
                    for i, msg in enumerate(new_messages):
                        content = msg.get('content', '')
                        content_preview = content[:200] if len(content) > 200 else content
                        # ğŸ” æ£€æŸ¥ thinking å­—æ®µ
                        thinking_flag = msg.get('thinking', False)
                        msg_id = msg.get('id', 'N/A')
                        logger.info(f"  Message {i+1}: type={msg.get('type', 'unknown')}, id={msg_id}, thinking={thinking_flag}, content={content_preview}...")

                    # è¿½åŠ åˆ°ç´¯ç§¯çŠ¶æ€
                    accumulated_state['messages'].extend(new_messages)
                    # ç§»é™¤ flattened_state ä¸­çš„ messagesï¼Œé¿å…é‡å¤ update
                    flattened_state = {k: v for k, v in flattened_state.items() if k != 'messages'}

                # æ›´æ–°å…¶ä»–çŠ¶æ€å­—æ®µ
                accumulated_state.update(flattened_state)

                # å‘é€æ ‡å‡† SSE äº‹ä»¶ï¼ˆå‘é€ç´¯ç§¯çŠ¶æ€ï¼‰
                try:
                    event_json = json.dumps(accumulated_state, ensure_ascii=False)
                    logger.info(f"ğŸ“¤ Sending SSE values event (size: {len(event_json)} bytes)")
                    logger.info(f"ğŸ“‹ Event data keys: {list(accumulated_state.keys())}")
                    # æ ‡å‡† SSE æ ¼å¼ï¼ševent: <type>\ndata: <json>\n\n
                    yield f"event: values\ndata: {event_json}\n\n"
                except Exception as e:
                    logger.error(f"âŒ JSON encoding error: {e}", exc_info=True)
                    continue

                # æ·»åŠ å°å»¶è¿Ÿä»¥ç¡®ä¿æµå¼ä¼ è¾“ï¼ˆå‡å°‘åˆ° 1ms é™ä½ç´¯ç§¯å»¶è¿Ÿï¼‰
                await asyncio.sleep(0.001)
        except Exception as e:
            logger.error(f"âŒ Stream iteration error: {e}", exc_info=True)
            raise

        # ğŸ”¥ P0æ”¹è¿›ï¼šçŠ¶æ€å·²ç”± LangGraph Checkpoint è‡ªåŠ¨æŒä¹…åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨ä¿å­˜
        # LangGraph åœ¨æ¯æ¬¡èŠ‚ç‚¹æ‰§è¡Œåè‡ªåŠ¨è°ƒç”¨ checkpointer.put()
        logger.info(f"âœ… Stream completed for thread {thread_id}, state auto-persisted by checkpoint")

        # å‘é€ç»“æŸäº‹ä»¶ï¼ˆä¿ç•™æœ€ç»ˆçŠ¶æ€ï¼‰
        logger.info(f"Stream completed with {event_count} events, sending end signal")
        final_state_json = json.dumps(accumulated_state, ensure_ascii=False)
        yield f"event: end\ndata: {final_state_json}\n\n"
        logger.info("End signal sent successfully")

    except Exception as e:
        logger.error(f"Stream error: {e}", exc_info=True)
        error_data = {"error": str(e)}
        yield f"event: error\ndata: {json.dumps(error_data, ensure_ascii=False)}\n\n"


# ==================== API ç«¯ç‚¹ ====================

@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹"""
    return {
        "service": "SkillRAG LangGraph Server",
        "version": "1.0.0",
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "endpoints": {
            "threads": "/threads/{thread_id}/runs/stream",
            "assistants": "/assistants",
            "health": "/health",
            "thread_state": "/threads/{thread_id}/state",
            "thread_resume": "/threads/{thread_id}/resume",
            "thread_history": "/threads/{thread_id}/history",
            "rag": {
                "search": "/rag/search",
                "recommend_actions": "/rag/recommend-actions",
                "recommend_parameters": "/rag/recommend-parameters",
                "rebuild_index": "/rag/index/rebuild",
                "stats": "/rag/index/stats",
                "clear_cache": "/rag/cache",
                "health": "/rag/health"
            }
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }


@app.get("/info")
async def server_info():
    """
    æœåŠ¡å™¨ä¿¡æ¯ç«¯ç‚¹ï¼ˆå…¼å®¹ LangGraph Cloud/Studioï¼‰

    å‰ç«¯ä½¿ç”¨æ­¤ç«¯ç‚¹æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
    """
    return {
        "version": "1.0.0",
        "name": "SkillRAG LangGraph Server",
        "description": "æŠ€èƒ½åˆ†æä¸ç”Ÿæˆçš„ LangGraph æœåŠ¡å™¨",
        "status": "ready",
        "timestamp": datetime.now().isoformat()
    }


@app.get("/assistants")
async def list_assistants():
    """åˆ—å‡ºå¯ç”¨çš„åŠ©æ‰‹ï¼ˆå›¾ï¼‰"""
    return {
        "assistants": [
            {
                "assistant_id": "skill-generation",
                "name": "æŠ€èƒ½ç”ŸæˆåŠ©æ‰‹",
                "description": "æ ¹æ®éœ€æ±‚æè¿°ç”ŸæˆæŠ€èƒ½é…ç½®JSONï¼ˆä¸€æ¬¡æ€§ç”Ÿæˆï¼‰",
                "graph_id": "skill_generation"
            },
            {
                "assistant_id": "progressive-skill-generation",
                "name": "æ¸è¿›å¼æŠ€èƒ½ç”ŸæˆåŠ©æ‰‹",
                "description": "ä¸‰é˜¶æ®µæ¸è¿›å¼ç”ŸæˆæŠ€èƒ½ï¼šéª¨æ¶â†’Trackâ†’ç»„è£…ï¼ˆæ¨èç”¨äºå¤æ‚æŠ€èƒ½ï¼‰",
                "graph_id": "progressive_skill_generation",
                "recommended": True
            },
            {
                "assistant_id": "skill-search",
                "name": "æŠ€èƒ½æœç´¢åŠ©æ‰‹",
                "description": "è¯­ä¹‰æœç´¢æŠ€èƒ½åº“",
                "graph_id": "skill_search"
            },
            {
                "assistant_id": "skill-detail",
                "name": "æŠ€èƒ½è¯¦æƒ…åŠ©æ‰‹",
                "description": "æŸ¥è¯¢æŠ€èƒ½è¯¦ç»†ä¿¡æ¯",
                "graph_id": "skill_detail"
            }
        ]
    }


@app.post("/threads/{thread_id}/runs/stream")
async def create_run_stream(
    thread_id: str,
    request: RunsStreamRequest
):
    """
    åˆ›å»ºæµå¼è¿è¡Œï¼ˆå…¼å®¹ agent-chat-uiï¼‰
    
    è¿™æ˜¯ agent-chat-ui è°ƒç”¨çš„ä¸»è¦ç«¯ç‚¹
    """
    try:
        logger.info(f"Stream request for thread {thread_id}: {request.input}")

        # è·å–åŠ©æ‰‹IDï¼ˆé»˜è®¤ä½¿ç”¨æŠ€èƒ½ç”Ÿæˆï¼‰
        assistant_id = request.assistant_id or request.config.get("configurable", {}).get("assistant_id", "skill-generation")

        # æ ¹æ®åŠ©æ‰‹IDé€‰æ‹©å›¾
        if assistant_id == "skill-generation":
            graph = get_skill_generation_graph()
        elif assistant_id == "progressive-skill-generation":
            graph = get_progressive_skill_generation_graph()
        elif assistant_id == "skill-search":
            graph = get_skill_search_graph()
        elif assistant_id == "skill-detail":
            graph = get_skill_detail_graph()
        else:
            raise HTTPException(status_code=404, detail=f"Assistant '{assistant_id}' not found")

        # å‡†å¤‡åˆå§‹çŠ¶æ€
        input_data = request.input
        messages = input_data.get("messages", [])

        # ä½¿ç”¨æŠ½å–çš„è¾…åŠ©å‡½æ•°è½¬æ¢æ¶ˆæ¯æ ¼å¼
        normalized_messages = normalize_langgraph_messages(messages)

        # æå–æœ€æ–°æ¶ˆæ¯ä½œä¸ºéœ€æ±‚
        if normalized_messages:
            requirement = normalized_messages[-1].get("content", "")
        else:
            requirement = input_data.get("requirement", "")

        # ä½¿ç”¨æŠ½å–çš„è¾…åŠ©å‡½æ•°æ„å»ºåˆå§‹çŠ¶æ€
        initial_state = build_initial_state(
            assistant_id=assistant_id,
            requirement=requirement,
            thread_id=thread_id,
            normalized_messages=normalized_messages
        )

        # è¿”å›æµå¼å“åº”
        return StreamingResponse(
            stream_graph_updates(graph, initial_state, thread_id),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
        
    except Exception as e:
        logger.error(f"Error in stream endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/threads/{thread_id}/runs")
async def create_run(
    thread_id: str,
    request: RunsStreamRequest
):
    """
    åˆ›å»ºéæµå¼è¿è¡Œ
    """
    try:
        logger.info(f"Run request for thread {thread_id}: {request.input}")

        # è·å–åŠ©æ‰‹ID
        assistant_id = request.assistant_id or request.config.get("configurable", {}).get("assistant_id", "skill-generation")

        # æ ¹æ®åŠ©æ‰‹IDé€‰æ‹©å›¾
        if assistant_id == "skill-generation":
            graph = get_skill_generation_graph()
        elif assistant_id == "progressive-skill-generation":
            graph = get_progressive_skill_generation_graph()
        elif assistant_id == "skill-search":
            graph = get_skill_search_graph()
        elif assistant_id == "skill-detail":
            graph = get_skill_detail_graph()
        else:
            raise HTTPException(status_code=404, detail=f"Assistant '{assistant_id}' not found")
        
        # å‡†å¤‡åˆå§‹çŠ¶æ€
        input_data = request.input
        messages = input_data.get("messages", [])

        # ä½¿ç”¨æŠ½å–çš„è¾…åŠ©å‡½æ•°è½¬æ¢æ¶ˆæ¯æ ¼å¼
        normalized_messages = normalize_langgraph_messages(messages)

        # æå–æœ€æ–°æ¶ˆæ¯ä½œä¸ºéœ€æ±‚
        if normalized_messages:
            requirement = normalized_messages[-1].get("content", "")
        else:
            requirement = input_data.get("requirement", "")

        # ä½¿ç”¨æŠ½å–çš„è¾…åŠ©å‡½æ•°æ„å»ºåˆå§‹çŠ¶æ€
        initial_state = build_initial_state(
            assistant_id=assistant_id,
            requirement=requirement,
            thread_id=thread_id,
            normalized_messages=normalized_messages
        )

        # æ‰§è¡Œå›¾
        result = await graph.ainvoke(initial_state)
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        result["messages"] = convert_from_langgraph_messages(result.get("messages", []))
        
        return {
            "thread_id": thread_id,
            "run_id": f"run_{datetime.now().timestamp()}",
            "status": "completed",
            "result": result
        }
        
    except Exception as e:
        logger.error(f"Error in run endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/threads")
async def create_thread():
    """
    åˆ›å»ºæ–°çº¿ç¨‹ï¼ˆå…¼å®¹ LangGraph Cloud APIï¼‰

    å‰ç«¯ä½¿ç”¨æ­¤ç«¯ç‚¹åˆ›å»ºæ–°çš„å¯¹è¯çº¿ç¨‹
    """
    thread_id = f"thread_{datetime.now().timestamp()}"
    return {
        "thread_id": thread_id,
        "created_at": datetime.now().isoformat(),
        "metadata": {},
        "status": "idle"
    }


@app.get("/threads/{thread_id}")
async def get_thread(thread_id: str):
    """è·å–çº¿ç¨‹ä¿¡æ¯"""
    return {
        "thread_id": thread_id,
        "created_at": datetime.now().isoformat(),
        "metadata": {}
    }


@app.get("/threads/{thread_id}/state")
async def get_thread_state(thread_id: str, assistant_id: str = "skill-generation"):
    """
    è·å–çº¿ç¨‹çŠ¶æ€ï¼ˆå…¼å®¹ LangGraph SDKï¼‰

    å‰ç«¯åœ¨ stream å®Œæˆåä¼šè°ƒç”¨æ­¤ç«¯ç‚¹è·å–æœ€ç»ˆçŠ¶æ€
    ğŸ”¥ P0æ”¹è¿›ï¼šä» LangGraph Checkpoint è¯»å–æŒä¹…åŒ–çŠ¶æ€
    """
    try:
        # æ ¹æ® assistant_id é€‰æ‹©å¯¹åº”çš„å›¾
        if assistant_id == "skill-generation":
            graph = get_skill_generation_graph()
        elif assistant_id == "progressive-skill-generation":
            graph = get_progressive_skill_generation_graph()
        elif assistant_id == "skill-search":
            graph = get_skill_search_graph()
        elif assistant_id == "skill-detail":
            graph = get_skill_detail_graph()
        else:
            logger.warning(f"Unknown assistant_id: {assistant_id}, using skill-generation")
            graph = get_skill_generation_graph()

        # ğŸ”¥ ä» checkpoint è¯»å–çŠ¶æ€
        config = {"configurable": {"thread_id": thread_id}}
        state_snapshot = graph.get_state(config)

        if state_snapshot is None or not state_snapshot.values:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç©ºçŠ¶æ€
            logger.warning(f"No checkpoint state found for thread {thread_id}")
            return {
                "values": {},
                "next": [],
                "config": config,
                "metadata": {},
                "created_at": datetime.now().isoformat(),
                "parent_config": None
            }

        # è¿”å› LangGraph å…¼å®¹çš„çŠ¶æ€æ ¼å¼
        return {
            "values": state_snapshot.values,  # ä» checkpoint è¯»å–çš„çŠ¶æ€
            "next": state_snapshot.next,       # ä¸‹ä¸€æ­¥èŠ‚ç‚¹
            "config": config,
            "metadata": state_snapshot.metadata or {},
            "created_at": state_snapshot.created_at.isoformat() if hasattr(state_snapshot, 'created_at') else datetime.now().isoformat(),
            "parent_config": state_snapshot.parent_config
        }
    except Exception as e:
        logger.error(f"Get thread state error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/threads/search")
async def search_threads(request: Request):
    """
    æœç´¢çº¿ç¨‹ï¼ˆå…¼å®¹ LangGraph SDKï¼‰

    å‰ç«¯ä½¿ç”¨ client.threads.search() æ—¶è°ƒç”¨æ­¤ç«¯ç‚¹
    ğŸ”¥ P0æ”¹è¿›ï¼šä» Checkpoint æŸ¥è¯¢æŒä¹…åŒ–çš„çº¿ç¨‹åˆ—è¡¨
    """
    try:
        body = await request.json() if request.headers.get("content-type") == "application/json" else {}

        # ğŸ”¥ TODO: å®ç°ä» SqliteSaver æŸ¥è¯¢çº¿ç¨‹åˆ—è¡¨
        # SqliteSaver æä¾›äº† list() æ–¹æ³•ï¼Œä½†éœ€è¦éå†æ‰€æœ‰å›¾çš„ checkpoint
        # æš‚æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œæœªæ¥å¯ä»¥é€šè¿‡æŸ¥è¯¢ SQLite æ•°æ®åº“å®ç°
        logger.info("Thread search requested, returning empty list (TODO: implement checkpoint query)")
        return []

    except Exception as e:
        logger.error(f"Search threads error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/threads/{thread_id}/history")
async def get_thread_history(thread_id: str, request: Request, assistant_id: str = "skill-generation"):
    """
    è·å–çº¿ç¨‹å†å²æ¶ˆæ¯ï¼ˆå…¼å®¹ LangGraph SDKï¼‰

    è¿”å›æŒ‡å®šçº¿ç¨‹çš„å®Œæ•´å¯¹è¯å†å²
    LangGraph SDK æœŸæœ›è¿”å›ä¸€ä¸ªæ•°ç»„æ ¼å¼çš„å†å²è®°å½•
    ğŸ”¥ P0æ”¹è¿›ï¼šä» Checkpoint è¯»å–å†å²çŠ¶æ€å¿«ç…§
    """
    try:
        # æ ¹æ® assistant_id é€‰æ‹©å¯¹åº”çš„å›¾
        if assistant_id == "skill-generation":
            graph = get_skill_generation_graph()
        elif assistant_id == "progressive-skill-generation":
            graph = get_progressive_skill_generation_graph()
        elif assistant_id == "skill-search":
            graph = get_skill_search_graph()
        elif assistant_id == "skill-detail":
            graph = get_skill_detail_graph()
        else:
            graph = get_skill_generation_graph()

        # ğŸ”¥ ä» checkpoint è¯»å–å†å²çŠ¶æ€
        config = {"configurable": {"thread_id": thread_id}}

        # ä½¿ç”¨ get_state_history() è·å–æ‰€æœ‰å†å²å¿«ç…§
        history = []
        try:
            state_history = graph.get_state_history(config)
            for state_snapshot in state_history:
                history.append({
                    "values": state_snapshot.values,
                    "next": state_snapshot.next,
                    "config": state_snapshot.config,
                    "metadata": state_snapshot.metadata or {},
                    "parent_config": state_snapshot.parent_config,
                    "created_at": state_snapshot.created_at.isoformat() if hasattr(state_snapshot, 'created_at') else None
                })
        except Exception as e:
            logger.warning(f"Failed to get history for thread {thread_id}: {e}")

        return history

    except Exception as e:
        logger.error(f"Get thread history error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/threads/{thread_id}/resume")
async def resume_thread(thread_id: str, assistant_id: str = "skill-generation"):
    """
    æ¢å¤çº¿ç¨‹ä¼šè¯ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰

    è¿”å›æŒ‡å®šçº¿ç¨‹çš„æœ€æ–°çŠ¶æ€å’Œå¯¹è¯å†å²,ç”¨äºæœåŠ¡é‡å¯åæ¢å¤å¯¹è¯
    ğŸ”¥ P0æ–°åŠŸèƒ½ï¼šåˆ©ç”¨ Checkpoint å®ç°ä¼šè¯æ¢å¤
    """
    try:
        # æ ¹æ® assistant_id é€‰æ‹©å¯¹åº”çš„å›¾
        if assistant_id == "skill-generation":
            graph = get_skill_generation_graph()
        elif assistant_id == "progressive-skill-generation":
            graph = get_progressive_skill_generation_graph()
        elif assistant_id == "skill-search":
            graph = get_skill_search_graph()
        elif assistant_id == "skill-detail":
            graph = get_skill_detail_graph()
        else:
            logger.warning(f"Unknown assistant_id: {assistant_id}, using skill-generation")
            graph = get_skill_generation_graph()

        # ä» checkpoint è¯»å–æœ€æ–°çŠ¶æ€
        config = {"configurable": {"thread_id": thread_id}}
        state_snapshot = graph.get_state(config)

        if state_snapshot is None or not state_snapshot.values:
            logger.warning(f"No checkpoint found for thread {thread_id}")
            raise HTTPException(status_code=404, detail=f"Thread {thread_id} not found or has no state")

        # æå–å¯¹è¯å†å²ï¼ˆä» messages å­—æ®µï¼‰
        messages = state_snapshot.values.get("messages", [])
        converted_messages = convert_from_langgraph_messages(messages) if messages else []

        # æ„å»ºæ¢å¤å“åº”
        return {
            "thread_id": thread_id,
            "assistant_id": assistant_id,
            "status": "resumed",
            "messages": converted_messages,
            "state_summary": {
                "requirement": state_snapshot.values.get("requirement", ""),
                "retry_count": state_snapshot.values.get("retry_count", 0),
                "is_valid": state_snapshot.values.get("is_valid", None),
                "has_result": bool(state_snapshot.values.get("final_result")),
            },
            "next_nodes": state_snapshot.next,
            "created_at": state_snapshot.created_at.isoformat() if hasattr(state_snapshot, 'created_at') else datetime.now().isoformat()
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Resume thread error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# ==================== RAG ä¸“ç”¨ç«¯ç‚¹ ====================

class RAGSearchRequest(BaseModel):
    """RAGæœç´¢è¯·æ±‚"""
    query: str = Field(..., description="æœç´¢æŸ¥è¯¢æ–‡æœ¬")
    top_k: int = Field(default=rag.DEFAULT_TOP_K, description="è¿”å›ç»“æœæ•°é‡")
    filters: Optional[Dict[str, Any]] = Field(None, description="è¿‡æ»¤æ¡ä»¶")


class RAGActionRecommendRequest(BaseModel):
    """Actionæ¨èè¯·æ±‚"""
    context: str = Field(..., description="ä¸Šä¸‹æ–‡æè¿°")
    top_k: int = Field(default=rag.RECOMMEND_TOP_K, description="æ¨èæ•°é‡")


class RAGParameterRecommendRequest(BaseModel):
    """å‚æ•°æ¨èè¯·æ±‚"""
    action_type: str = Field(..., description="Actionç±»å‹åç§°")
    skill_context: Optional[str] = Field(None, description="æŠ€èƒ½ä¸Šä¸‹æ–‡")
    parameter_name: Optional[str] = Field(None, description="å‚æ•°åç§°")


@app.post("/rag/search")
async def rag_search(request: RAGSearchRequest):
    """
    æŠ€èƒ½è¯­ä¹‰æœç´¢

    æ ¹æ®è‡ªç„¶è¯­è¨€æŸ¥è¯¢æœç´¢ç›¸ä¼¼çš„æŠ€èƒ½é…ç½®
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        logger.info(f"RAG search: query='{request.query}', top_k={request.top_k}")

        engine = get_rag_engine()
        results = engine.search_skills(
            query=request.query,
            top_k=request.top_k,
            filters=request.filters,
            return_details=True
        )

        return {
            "success": True,
            "query": request.query,
            "results": results,
            "count": len(results)
        }

    except Exception as e:
        logger.error(f"RAG search error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/rag/recommend-actions")
async def rag_recommend_actions(request: RAGActionRecommendRequest):
    """
    Actionç±»å‹æ™ºèƒ½æ¨è

    æ ¹æ®ä¸Šä¸‹æ–‡æè¿°æ¨èåˆé€‚çš„Actionç±»å‹
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        logger.info(f"Action recommendation: context='{request.context}'")

        engine = get_rag_engine()
        recommendations = engine.recommend_actions(
            context=request.context,
            top_k=request.top_k
        )

        return {
            "success": True,
            "context": request.context,
            "recommendations": recommendations,
            "count": len(recommendations)
        }

    except Exception as e:
        logger.error(f"Action recommendation error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/rag/recommend-parameters")
async def rag_recommend_parameters(request: RAGParameterRecommendRequest):
    """
    å‚æ•°æ™ºèƒ½æ¨èï¼ˆåŸUnity InspectoråŠŸèƒ½ï¼‰

    ä¸ºæŒ‡å®šActionç±»å‹æ¨èåˆé€‚çš„å‚æ•°é…ç½®
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        logger.info(f"Parameter recommendation: action_type='{request.action_type}'")

        engine = get_rag_engine()

        # æœç´¢åŒ…å«è¯¥Actionç±»å‹çš„æŠ€èƒ½
        action_search_results = engine.search_actions(
            query=request.action_type,
            top_k=rag.PARAMETER_TOP_K
        )

        # æå–å‚æ•°ç¤ºä¾‹
        parameter_examples = []
        for result in action_search_results:
            if 'action_data' in result:
                action_data = result['action_data']
                if 'parameters' in action_data:
                    parameter_examples.append({
                        "action_type": result.get('actionType', request.action_type),
                        "parameters": action_data['parameters'],
                        "source_skill": result.get('skill_name', 'Unknown'),
                        "similarity": result.get('similarity', 0.0)
                    })

        return {
            "success": True,
            "action_type": request.action_type,
            "parameter_examples": parameter_examples,
            "count": len(parameter_examples)
        }

    except Exception as e:
        logger.error(f"Parameter recommendation error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/rag/index/rebuild")
async def rag_rebuild_index():
    """
    é‡å»ºRAGç´¢å¼•

    é‡æ–°ç´¢å¼•æ‰€æœ‰æŠ€èƒ½é…ç½®æ–‡ä»¶
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        logger.info("Rebuilding RAG index...")

        engine = get_rag_engine()

        # é‡å»ºæŠ€èƒ½ç´¢å¼•
        skill_result = engine.index_skills(force_rebuild=True)

        # é‡å»ºActionç´¢å¼•
        action_result = engine.index_actions(force_rebuild=True)

        # é‡å»ºç»“æ„åŒ–ç´¢å¼•
        structured_result = engine.rebuild_structured_index(force=True)

        return {
            "success": True,
            "skill_index": skill_result,
            "action_index": action_result,
            "structured_index": structured_result,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Index rebuild error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/rag/index/stats")
async def rag_index_stats():
    """
    è·å–RAGç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine
        import os

        engine = get_rag_engine()
        stats = engine.get_statistics()

        # æå–å…³é”®ç»Ÿè®¡æ•°æ®
        vector_stats = stats.get('vector_store', {})
        action_stats = stats.get('action_stats', {})

        # è®¡ç®—ç´¢å¼•å¤§å°ï¼ˆä¼°ç®—ï¼‰
        persist_dir = vector_stats.get('persist_directory', '')
        index_size_mb = 0.0
        if persist_dir and os.path.exists(persist_dir):
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(persist_dir):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    if os.path.exists(filepath):
                        total_size += os.path.getsize(filepath)
            index_size_mb = total_size / (1024 * 1024)  # è½¬æ¢ä¸º MB

        # è¿”å›å‰ç«¯æœŸæœ›çš„æ ¼å¼
        return {
            "total_skills": vector_stats.get('total_documents', 0),
            "total_actions": action_stats.get('total_actions', 0),
            "total_parameters": int(action_stats.get('avg_params_per_action', 0) * action_stats.get('total_actions', 0)),
            "index_size_mb": round(index_size_mb, 2)
        }

    except Exception as e:
        logger.error(f"Stats retrieval error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/rag/cache")
async def rag_clear_cache():
    """
    æ¸…ç©ºRAGæŸ¥è¯¢ç¼“å­˜
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        engine = get_rag_engine()

        # æ¸…ç©ºæŸ¥è¯¢ç¼“å­˜
        if hasattr(engine, '_query_cache') and engine._query_cache is not None:
            cache_size = len(engine._query_cache)
            engine._query_cache.clear()
            logger.info(f"Cleared {cache_size} cached queries")
        else:
            cache_size = 0
            logger.info("Query cache is disabled or empty")

        return {
            "success": True,
            "cleared_entries": cache_size,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"Cache clear error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/rag/health")
async def rag_health_check():
    """
    RAGæœåŠ¡å¥åº·æ£€æŸ¥

    ä¸ webui/src/app/rag/page.tsx HealthStatus æ¥å£å¯¹æ¥
    """
    try:
        from orchestration.tools.rag_tools import get_rag_engine

        engine = get_rag_engine()
        stats = engine.get_statistics()

        # æå–åµŒå¥—ç»Ÿè®¡æ•°æ®
        vector_stats = stats.get('vector_store', {})
        action_stats = stats.get('action_stats', {})

        return {
            "status": "healthy",
            "skill_count": vector_stats.get('total_documents', 0),
            "action_count": action_stats.get('total_actions', 0),
            "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

    except Exception as e:
        logger.warning(f"RAG health check failed: {e}")
        return {
            "status": "unhealthy",
            "skill_count": 0,
            "action_count": 0,
            "last_updated": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }


# ==================== ä¸»å‡½æ•° ====================

def kill_process_on_port(port: int) -> bool:
    """
    æ€æ‰å ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹

    Args:
        port: ç«¯å£å·

    Returns:
        æ˜¯å¦æˆåŠŸæ€æ‰è¿›ç¨‹
    """
    try:
        import subprocess
        import re

        # åœ¨Windowsä¸ŠæŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
        if sys.platform == "win32":
            # ä½¿ç”¨ netstat æŸ¥æ‰¾ç«¯å£
            result = subprocess.run(
                ['netstat', '-ano'],
                capture_output=True,
                text=True,
                timeout=5
            )

            # è§£æè¾“å‡ºï¼ŒæŸ¥æ‰¾ç›‘å¬è¯¥ç«¯å£çš„è¿›ç¨‹
            for line in result.stdout.splitlines():
                if f':{port}' in line and 'LISTENING' in line:
                    # æå–PIDï¼ˆæœ€åä¸€åˆ—ï¼‰
                    parts = line.split()
                    if parts:
                        pid = parts[-1]
                        logger.info(f"ğŸ” Found process {pid} using port {port}")

                        # æ€æ‰è¿›ç¨‹
                        kill_result = subprocess.run(
                            ['taskkill', '/F', '/PID', pid],
                            capture_output=True,
                            text=True,
                            timeout=5
                        )

                        if kill_result.returncode == 0:
                            logger.info(f"âœ… Successfully killed process {pid}")
                            return True
                        else:
                            logger.warning(f"âš ï¸  Failed to kill process {pid}: {kill_result.stderr}")

            return False
        else:
            # Linux/Mac ä½¿ç”¨ lsof
            result = subprocess.run(
                ['lsof', '-ti', f':{port}'],
                capture_output=True,
                text=True,
                timeout=5
            )

            if result.stdout.strip():
                pid = result.stdout.strip()
                logger.info(f"ğŸ” Found process {pid} using port {port}")

                subprocess.run(['kill', '-9', pid], timeout=5)
                logger.info(f"âœ… Successfully killed process {pid}")
                return True

            return False

    except Exception as e:
        logger.error(f"âŒ Error killing process on port {port}: {e}")
        return False


def main():
    """å¯åŠ¨æœåŠ¡å™¨"""
    # ä½¿ç”¨é…ç½®æ¨¡å—ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
    host = server.LANGGRAPH_HOST
    port = server.LANGGRAPH_PORT

    logger.info(f"Starting LangGraph server on {host}:{port}")

    # æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨
    import socket
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    result = sock.connect_ex(('localhost', port))
    sock.close()

    if result == 0:
        logger.warning(f"âš ï¸  Port {port} is already in use!")
        logger.info(f"ğŸ”„ Attempting to kill existing process...")

        if kill_process_on_port(port):
            logger.info(f"âœ… Port {port} is now free, starting server...")
            # ç­‰å¾…ç«¯å£å®Œå…¨é‡Šæ”¾
            import time
            time.sleep(timeout.PORT_RELEASE_WAIT)
        else:
            logger.error(f"âŒ Failed to free port {port}")
            logger.warning(f"    To manually find the process: netstat -ano | findstr :{port}")
            logger.warning(f"    To manually kill it: taskkill /F /PID <PID>")
            sys.exit(1)

    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info"
    )


if __name__ == "__main__":
    main()
