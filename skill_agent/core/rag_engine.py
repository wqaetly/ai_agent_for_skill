"""
RAGå¼•æ“æ ¸å¿ƒæ¨¡å—
æ•´åˆåµŒå…¥ç”Ÿæˆã€å‘é‡å­˜å‚¨å’ŒæŠ€èƒ½ç´¢å¼•ï¼Œæä¾›ç»Ÿä¸€çš„RAGæ¥å£
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from cachetools import TTLCache
import hashlib
import json

from .embeddings import EmbeddingGenerator
from .vector_store import VectorStore
from .skill_indexer import SkillIndexer
from .action_indexer import ActionIndexer
from .structured_query_engine import StructuredQueryEngine

logger = logging.getLogger(__name__)


class RAGEngine:
    """RAGå¼•æ“ï¼Œæä¾›æŠ€èƒ½æ£€ç´¢å’Œæ¨èåŠŸèƒ½"""

    def __init__(self, config: dict):
        """
        åˆå§‹åŒ–RAGå¼•æ“

        Args:
            config: å®Œæ•´é…ç½®å­—å…¸
        """
        self.config = config
        self.rag_config = config.get('rag', {})
        self.top_k = self.rag_config.get('top_k', 5)
        self.similarity_threshold = self.rag_config.get('similarity_threshold', 0.5)

        # åˆå§‹åŒ–å„ä¸ªç»„ä»?
        logger.info("Initializing RAG Engine components...")

        # 1. åµŒå…¥ç”Ÿæˆå™?
        self.embedding_generator = EmbeddingGenerator(config.get('embedding', {}))

        # 2. å‘é‡å­˜å‚¨ï¼ˆç”¨äºæŠ€èƒ½ï¼‰
        self.vector_store = VectorStore(
            config.get('vector_store', {}),
            embedding_dimension=self.embedding_generator.get_embedding_dimension()
        )

        # 3. æŠ€èƒ½ç´¢å¼•å™¨
        self.skill_indexer = SkillIndexer(config.get('skill_indexer', {}))

        # 4. Actionç´¢å¼•å™?
        self.action_indexer = ActionIndexer(config.get('action_indexer', {}))

        # 5. Actionå‘é‡å­˜å‚¨ï¼ˆç‹¬ç«‹collectionï¼?
        action_vector_config = config.get('vector_store', {}).copy()
        action_vector_config['collection_name'] = config.get('action_indexer', {}).get('collection_name', 'action_collection')
        self.action_vector_store = VectorStore(
            action_vector_config,
            embedding_dimension=self.embedding_generator.get_embedding_dimension()
        )

        # 6. ç»“æ„åŒ–æŸ¥è¯¢å¼•æ“ï¼ˆREQ-03ï¼?
        skills_dir = config.get('skill_indexer', {}).get('skills_directory', '../Data/Skills')
        cache_size = self.rag_config.get('structured_query_cache_size', 100)
        self.structured_query_engine = StructuredQueryEngine(
            skills_dir=skills_dir,
            cache_size=cache_size
        )
        logger.info("Structured query engine initialized (REQ-03)")

        # æŸ¥è¯¢ç¼“å­˜ï¼ˆTTLç¼“å­˜ï¼Œé»˜è®?å°æ—¶ï¼?
        cache_enabled = self.rag_config.get('cache_enabled', True)
        cache_ttl = self.rag_config.get('cache_ttl', 3600)

        if cache_enabled:
            self._query_cache = TTLCache(maxsize=1000, ttl=cache_ttl)
            logger.info(f"Query cache enabled with TTL={cache_ttl}s")
        else:
            self._query_cache = None

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            'total_queries': 0,
            'cache_hits': 0,
            'total_indexed': 0,
            'last_index_time': None
        }

        logger.info("RAG Engine initialized successfully")

    def _get_cache_key(self, query: str, top_k: int, filters: Optional[Dict] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”?""
        cache_str = f"{query}|{top_k}|{json.dumps(filters or {}, sort_keys=True)}"
        return hashlib.md5(cache_str.encode('utf-8')).hexdigest()

    def _extract_action_types(self, skill: Dict[str, Any]) -> List[str]:
        """
        ä»æŠ€èƒ½æ•°æ®ä¸­æå–æ‰€æœ‰Actionç±»å‹åˆ—è¡¨

        Args:
            skill: æŠ€èƒ½æ•°æ®å­—å…?

        Returns:
            Actionç±»å‹åˆ—è¡¨ï¼ˆå»é‡ä¸”æ’åºï¼?
        """
        action_types = set()

        for track in skill.get('tracks', []):
            if not track.get('enabled', True):
                continue

            for action in track.get('actions', []):
                action_type = action.get('type', '')
                if action_type:
                    action_types.add(action_type)

        return sorted(list(action_types))

    def index_skills(self, force_rebuild: bool = False) -> Dict[str, Any]:
        """
        ç´¢å¼•æ‰€æœ‰æŠ€èƒ½åˆ°å‘é‡æ•°æ®åº?

        Args:
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•

        Returns:
            ç´¢å¼•ç»“æœç»Ÿè®¡
        """
        logger.info(f"Starting skill indexing (force_rebuild={force_rebuild})")
        start_time = datetime.now()

        # 1. æ‰«æå¹¶è§£ææŠ€èƒ½æ–‡ä»?
        skills = self.skill_indexer.index_all_skills(force_rebuild=force_rebuild)

        if not skills:
            logger.warning("No skills found to index")
            return {"status": "no_skills", "count": 0}

        # 2. å‡†å¤‡æ•°æ®
        documents = []
        metadatas = []
        ids = []

        for skill in skills:
            # æ–‡æ¡£æ–‡æœ¬ï¼ˆç”¨äºæœç´¢ï¼‰
            documents.append(skill['search_text'])

            # æå–actionç±»å‹åˆ—è¡¨
            action_types = self._extract_action_types(skill)

            # å…ƒæ•°æ?
            metadata = {
                'skill_id': skill.get('skillId', ''),
                'skill_name': skill.get('skillName', ''),
                'file_name': skill.get('file_name', ''),
                'file_path': skill.get('file_path', ''),
                'file_hash': skill.get('file_hash', ''),
                'last_modified': skill.get('last_modified', ''),
                'total_duration': skill.get('totalDuration', 0),
                'frame_rate': skill.get('frameRate', 30),
                'num_tracks': len(skill.get('tracks', [])),
                'num_actions': sum(len(track.get('actions', [])) for track in skill.get('tracks', [])),
                'action_type_list': json.dumps(action_types)  # å­˜å‚¨ä¸ºJSONå­—ç¬¦ä¸²ï¼Œæ”¯æŒChromaè¿‡æ»¤
            }
            metadatas.append(metadata)

            # æ–‡æ¡£IDï¼ˆä½¿ç”¨æ–‡ä»¶è·¯å¾„çš„å“ˆå¸Œï¼?
            doc_id = hashlib.md5(skill['file_path'].encode('utf-8')).hexdigest()
            ids.append(doc_id)

        # 3. ç”ŸæˆåµŒå…¥å‘é‡
        logger.info(f"Generating embeddings for {len(documents)} skills...")
        embeddings = self.embedding_generator.encode_batch(
            documents,
            show_progress=True
        )

        # 4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        logger.info("Storing embeddings to vector database...")

        # å¦‚æœæ˜¯å¼ºåˆ¶é‡å»ºï¼Œå…ˆæ¸…ç©?
        if force_rebuild:
            self.vector_store.clear()

        # æ·»åŠ æ–‡æ¡£
        success = self.vector_store.add_documents(
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )

        # 5. ç»Ÿè®¡
        elapsed_time = (datetime.now() - start_time).total_seconds()

        if success:
            self._stats['total_indexed'] = len(skills)
            self._stats['last_index_time'] = datetime.now().isoformat()

            result = {
                "status": "success",
                "count": len(skills),
                "elapsed_time": elapsed_time,
                "vector_dimension": self.embedding_generator.get_embedding_dimension()
            }

            logger.info(f"Indexing completed: {len(skills)} skills in {elapsed_time:.2f}s")
        else:
            result = {
                "status": "error",
                "count": 0,
                "message": "Failed to add documents to vector store"
            }
            logger.error("Indexing failed")

        return result

    def search_skills(
        self,
        query: str,
        top_k: Optional[int] = None,
        filters: Optional[Dict[str, Any]] = None,
        return_details: bool = False
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼æŠ€èƒ?

        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            filters: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»?
            return_details: æ˜¯å¦è¿”å›è¯¦ç»†ä¿¡æ¯

        Returns:
            åŒ¹é…çš„æŠ€èƒ½åˆ—è¡?
        """
        self._stats['total_queries'] += 1

        # ä½¿ç”¨é»˜è®¤å€?
        top_k = top_k or self.top_k

        # æ£€æŸ¥ç¼“å­?
        cache_key = None
        if self._query_cache is not None:
            cache_key = self._get_cache_key(query, top_k, filters)
            if cache_key in self._query_cache:
                self._stats['cache_hits'] += 1
                logger.debug(f"Cache hit for query: {query[:50]}")
                return self._query_cache[cache_key]

        # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆä½¿ç”¨query promptä¼˜åŒ–Qwen3ç­‰æ¨¡å‹æ€§èƒ½ï¼?
        query_embedding = self.embedding_generator.encode(
            query,
            use_cache=True,
            prompt_name="query"
        )

        # 2. å‘é‡æ£€ç´?
        results = self.vector_store.query(
            query_embeddings=[query_embedding],
            top_k=top_k,
            where=filters
        )

        # 3. å¤„ç†ç»“æœ
        matched_skills = []

        if results and results['ids'] and results['ids'][0]:
            for i in range(len(results['ids'][0])):
                doc_id = results['ids'][0][i]
                distance = results['distances'][0][i]
                metadata = results['metadatas'][0][i]
                document = results['documents'][0][i]

                # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦ï¼ˆcosine distance -> cosine similarityï¼?
                similarity = 1.0 - distance

                # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
                if similarity < self.similarity_threshold:
                    continue

                skill_result = {
                    'skill_id': metadata.get('skill_id', ''),
                    'skill_name': metadata.get('skill_name', ''),
                    'file_name': metadata.get('file_name', ''),
                    'similarity': round(similarity, 4),
                    'distance': round(distance, 4)
                }

                # å¦‚æœéœ€è¦è¯¦ç»†ä¿¡æ?
                if return_details:
                    skill_result.update({
                        'file_path': metadata.get('file_path', ''),
                        'file_hash': metadata.get('file_hash', ''),
                        'total_duration': metadata.get('total_duration', 0),
                        'frame_rate': metadata.get('frame_rate', 30),
                        'num_tracks': metadata.get('num_tracks', 0),
                        'num_actions': metadata.get('num_actions', 0),
                        'action_type_list': metadata.get('action_type_list', '[]'),
                        'last_modified': metadata.get('last_modified', ''),
                        'search_text_preview': document[:200]
                    })

                matched_skills.append(skill_result)

        # 4. æ›´æ–°ç¼“å­˜
        if self._query_cache is not None and cache_key:
            self._query_cache[cache_key] = matched_skills

        logger.info(f"Search query '{query[:50]}' returned {len(matched_skills)} results")
        return matched_skills

    def get_skill_by_id(self, skill_id: str) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®skill_idè·å–æŠ€èƒ½è¯¦ç»†ä¿¡æ?

        Args:
            skill_id: æŠ€èƒ½ID

        Returns:
            æŠ€èƒ½è¯¦ç»†ä¿¡æ¯ï¼Œæœªæ‰¾åˆ°è¿”å›None
        """
        # é€šè¿‡å…ƒæ•°æ®æŸ¥è¯?
        results = self.vector_store.search_by_metadata(
            where={"skill_id": skill_id},
            limit=1
        )

        if results and results['ids']:
            metadata = results['metadatas'][0]
            file_path = metadata.get('file_path', '')

            # é‡æ–°è§£ææ–‡ä»¶ä»¥è·å–å®Œæ•´æ•°æ?
            if file_path:
                skill_data = self.skill_indexer.parse_skill_file(file_path)
                return skill_data

        return None

    def recommend_actions(
        self,
        context: str,
        top_k: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®ä¸Šä¸‹æ–‡æ¨èActionç±»å‹
        åŸºäºActionè„šæœ¬å®šä¹‰çš„çº¯è¯­ä¹‰åŒ¹é…

        Args:
            context: ä¸Šä¸‹æ–‡æè¿°ï¼ˆå¦?é€ æˆä¼¤å®³"ã€?ç§»åŠ¨è§’è‰²"ç­‰ï¼‰
            top_k: æ¨èæ•°é‡

        Returns:
            æ¨èçš„Actionåˆ—è¡¨ï¼ŒæŒ‰è¯­ä¹‰ç›¸ä¼¼åº¦æ’åº?
        """
        # åŸºäºActionå®šä¹‰çš„è¯­ä¹‰æœç´?
        semantic_matches = self.search_actions(
            query=context,
            top_k=top_k,
            return_details=False
        )

        # æ„å»ºç»“æœåˆ—è¡¨
        recommended_actions = []

        for match in semantic_matches:
            action_type = match.get('type_name', '')
            similarity = match.get('similarity', 0.0)

            # è·å–Actionè¯¦ç»†ä¿¡æ¯
            action_def = self.get_action_by_type(action_type)
            display_name = action_def.get('displayName', action_type) if action_def else action_type
            category = action_def.get('category', 'Other') if action_def else 'Other'
            description = action_def.get('description', '') if action_def else ''

            recommended_actions.append({
                'action_type': action_type,
                'display_name': display_name,
                'category': category,
                'description': description,
                'semantic_similarity': similarity
            })

        logger.info(
            f"Recommended {len(recommended_actions)} actions for context: {context} "
            f"(pure semantic matching)"
        )

        return recommended_actions

    def update_skill(self, file_path: str) -> bool:
        """
        æ›´æ–°å•ä¸ªæŠ€èƒ½çš„ç´¢å¼•

        Args:
            file_path: æŠ€èƒ½æ–‡ä»¶è·¯å¾?

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # è§£ææŠ€èƒ?
            skill_data = self.skill_indexer.parse_skill_file(file_path)
            if not skill_data:
                return False

            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = hashlib.md5(file_path.encode('utf-8')).hexdigest()

            # æ„å»ºæœç´¢æ–‡æœ¬
            search_text = self.skill_indexer.build_search_text(skill_data)

            # ç”ŸæˆåµŒå…¥
            embedding = self.embedding_generator.encode(search_text)

            # å‡†å¤‡å…ƒæ•°æ?
            metadata = {
                'skill_id': skill_data.get('skillId', ''),
                'skill_name': skill_data.get('skillName', ''),
                'file_name': skill_data.get('file_name', ''),
                'file_path': file_path,
                'file_hash': skill_data.get('file_hash', ''),
                'last_modified': skill_data.get('last_modified', ''),
                'total_duration': skill_data.get('totalDuration', 0),
                'frame_rate': skill_data.get('frameRate', 30),
                'num_tracks': len(skill_data.get('tracks', [])),
                'num_actions': sum(len(track.get('actions', [])) for track in skill_data.get('tracks', []))
            }

            # æ›´æ–°å‘é‡æ•°æ®åº?
            success = self.vector_store.update_document(
                document_id=doc_id,
                document=search_text,
                embedding=embedding,
                metadata=metadata
            )

            if success:
                logger.info(f"Updated skill: {file_path}")
                # æ¸…ç©ºç¼“å­˜
                if self._query_cache is not None:
                    self._query_cache.clear()

            return success

        except Exception as e:
            logger.error(f"Error updating skill {file_path}: {e}")
            return False

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–RAGå¼•æ“ç»Ÿè®¡ä¿¡æ¯"""
        vector_stats = self.vector_store.get_statistics()
        embedding_cache = self.embedding_generator.get_cache_info()
        action_stats = self.action_indexer.get_statistics()

        return {
            'engine_stats': self._stats,
            'vector_store': vector_stats,
            'embedding_cache': embedding_cache,
            'query_cache_size': len(self._query_cache) if self._query_cache else 0,
            'action_stats': action_stats
        }

    # ============ Actionç›¸å…³æ–¹æ³• ============

    def index_actions(self, force_rebuild: bool = False) -> Dict[str, Any]:
        """
        ç´¢å¼•æ‰€æœ‰Actionåˆ°å‘é‡æ•°æ®åº“

        Args:
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•

        Returns:
            ç´¢å¼•ç»“æœç»Ÿè®¡
        """
        logger.info(f"Starting action indexing (force_rebuild={force_rebuild})")
        start_time = datetime.now()

        # 1. å‡†å¤‡Actionæ•°æ®
        prepared_actions = self.action_indexer.prepare_actions_for_indexing()

        if not prepared_actions:
            logger.warning("No actions found to index")
            return {"status": "no_actions", "count": 0}

        # 2. æå–æ•°æ®
        documents = []
        metadatas = []
        ids = []

        for action in prepared_actions:
            documents.append(action['search_text'])
            metadatas.append(action['metadata'])
            ids.append(action['id'])

        # 3. ç”ŸæˆåµŒå…¥å‘é‡
        logger.info(f"Generating embeddings for {len(documents)} actions...")
        embeddings = self.embedding_generator.encode_batch(
            documents,
            show_progress=True
        )

        # 4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        logger.info("Storing action embeddings to vector database...")

        # å¦‚æœæ˜¯å¼ºåˆ¶é‡å»ºï¼Œå…ˆæ¸…ç©?
        if force_rebuild:
            self.action_vector_store.clear()

        # æ·»åŠ æ–‡æ¡£
        success = self.action_vector_store.add_documents(
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )

        # 5. ç»Ÿè®¡
        elapsed_time = (datetime.now() - start_time).total_seconds()

        if success:
            result = {
                "status": "success",
                "count": len(prepared_actions),
                "elapsed_time": elapsed_time
            }
            logger.info(f"Action indexing completed: {len(prepared_actions)} actions in {elapsed_time:.2f}s")
        else:
            result = {
                "status": "error",
                "count": 0,
                "message": "Failed to add action documents to vector store"
            }
            logger.error("Action indexing failed")

        return result

    def search_actions(
        self,
        query: str,
        top_k: Optional[int] = None,
        category_filter: Optional[str] = None,
        return_details: bool = False
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢Actionç±»å‹

        Args:
            query: æœç´¢æŸ¥è¯¢ï¼ˆå¦‚"é€ æˆä¼¤å®³"ã€?ç§»åŠ¨è§’è‰²"ï¼?
            top_k: è¿”å›ç»“æœæ•°é‡
            category_filter: æŒ‰åˆ†ç±»è¿‡æ»¤ï¼ˆå¦?Damage"ã€?Movement"ï¼?
            return_details: æ˜¯å¦è¿”å›è¯¦ç»†å‚æ•°ä¿¡æ¯

        Returns:
            åŒ¹é…çš„Actionåˆ—è¡¨
        """
        top_k = top_k or self.top_k

        # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedding_generator.encode(
            query,
            use_cache=True,
            prompt_name="query"
        )

        # 2. æ„å»ºè¿‡æ»¤æ¡ä»¶
        where_filter = None
        if category_filter:
            where_filter = {"category": category_filter}

        # 3. å‘é‡æ£€ç´?
        results = self.action_vector_store.query(
            query_embeddings=[query_embedding],
            top_k=top_k,
            where=where_filter
        )

        # 4. å¤„ç†ç»“æœ
        matched_actions = []

        if results and results['ids'] and results['ids'][0]:
            for i in range(len(results['ids'][0])):
                action_id = results['ids'][0][i]
                distance = results['distances'][0][i]
                metadata = results['metadatas'][0][i]

                # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦
                similarity = 1.0 - distance

                # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
                if similarity < self.similarity_threshold:
                    continue

                action_result = {
                    'type_name': metadata.get('type_name', ''),
                    'display_name': metadata.get('display_name', ''),
                    'category': metadata.get('category', ''),
                    'similarity': round(similarity, 4)
                }

                # å¦‚æœéœ€è¦è¯¦ç»†ä¿¡æ¯ï¼Œä»action_indexerè·å–å®Œæ•´å®šä¹‰
                if return_details:
                    type_name = metadata.get('type_name', '')
                    action_def = self.action_indexer.get_action_by_type(type_name)
                    if action_def:
                        action_result['parameters'] = action_def.get('parameters', [])
                        action_result['description'] = action_def.get('description', '')
                        action_result['full_type_name'] = action_def.get('fullTypeName', '')

                matched_actions.append(action_result)

        logger.info(f"Action search query '{query[:50]}' returned {len(matched_actions)} results")
        return matched_actions

    def get_action_by_type(self, type_name: str) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®ç±»å‹åè·å–Actionè¯¦ç»†ä¿¡æ¯

        Args:
            type_name: Actionç±»å‹åï¼ˆå¦?DamageAction"ï¼?

        Returns:
            Actionè¯¦ç»†ä¿¡æ¯ï¼Œæœªæ‰¾åˆ°è¿”å›None
        """
        return self.action_indexer.get_action_by_type(type_name)

    def get_actions_by_category(self, category: str) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šåˆ†ç±»çš„æ‰€æœ‰Action

        Args:
            category: Actionåˆ†ç±»

        Returns:
            Actionåˆ—è¡¨
        """
        return self.action_indexer.get_actions_by_category(category)

    def get_action_categories(self) -> List[str]:
        """
        è·å–æ‰€æœ‰Actionåˆ†ç±»

        Returns:
            åˆ†ç±»åˆ—è¡¨
        """
        actions = self.action_indexer.get_all_actions()
        categories = set(action.get('category', 'Other') for action in actions)
        return sorted(list(categories))

    # ============ REQ-03 ç»“æ„åŒ–æŸ¥è¯¢æ–¹æ³?============

    def query_skills_structured(
        self,
        query_str: str,
        limit: int = 100,
        include_context: bool = True
    ) -> Dict[str, Any]:
        """
        ç»“æ„åŒ–æŸ¥è¯¢æŠ€èƒ½Actionï¼ˆREQ-03ï¼?

        æ”¯æŒæŒ‰Actionç±»å‹ã€å‚æ•°æ¡ä»¶ç­›é€‰ã€?

        Args:
            query_str: æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå¦?"DamageAction where baseDamage > 200"
            limit: æœ€å¤§è¿”å›ç»“æœæ•°
            include_context: æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæŠ€èƒ½åã€è½¨é“åï¼?

        Returns:
            æŸ¥è¯¢ç»“æœ

        Examples:
            >>> engine.query_skills_structured("DamageAction where baseDamage > 200")
            >>> engine.query_skills_structured("baseDamage between 100 and 300")
            >>> engine.query_skills_structured("animationClipName contains Attack")
        """
        return self.structured_query_engine.query(
            query_str=query_str,
            limit=limit,
            include_context=include_context
        )

    def get_action_statistics_structured(
        self,
        query_str: Optional[str] = None,
        group_by: str = "action_type"
    ) -> Dict[str, Any]:
        """
        è·å–Actionå‚æ•°çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆREQ-03ï¼?

        å¯æŒ‰Actionç±»å‹ã€è½¨é“åˆ†ç»„ç»Ÿè®¡å‚æ•°çš„min/max/avgå€¼ã€?

        Args:
            query_str: è¿‡æ»¤æŸ¥è¯¢ï¼ˆå¯é€‰ï¼‰ï¼Œä¸æŒ‡å®šåˆ™ç»Ÿè®¡å…¨éƒ?
            group_by: åˆ†ç»„å­—æ®µï¼Œå¦‚ "action_type" æˆ?"track_name"

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        return self.structured_query_engine.get_statistics(
            query_str=query_str,
            group_by=group_by
        )

    def get_action_detail_structured(
        self,
        skill_file: str,
        json_path: str
    ) -> Optional[Dict[str, Any]]:
        """
        è·å–Actionçš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ï¼ˆREQ-03ï¼?

        åŒ…å«åŸå§‹JSONæ•°æ®ã€è¡Œå·ã€ä¸Šä¸‹æ–‡ç­‰ã€?

        Args:
            skill_file: æŠ€èƒ½æ–‡ä»¶åï¼Œå¦‚ "FlameShockwave.json"
            json_path: Actionçš„JSONPath

        Returns:
            å®Œæ•´çš„Actionæ•°æ®
        """
        return self.structured_query_engine.get_action_detail(
            skill_file=skill_file,
            json_path=json_path
        )

    def rebuild_structured_index(self, force: bool = False) -> Dict[str, Any]:
        """
        é‡å»ºç»†ç²’åº¦ç´¢å¼•ï¼ˆREQ-03ï¼?

        å½“æŠ€èƒ½æ–‡ä»¶ä¿®æ”¹åï¼Œéœ€è¦é‡å»ºç´¢å¼•ã€?

        Args:
            force: å¼ºåˆ¶é‡å»ºæ‰€æœ‰æ–‡ä»¶ï¼ˆé»˜è®¤åªæ›´æ–°ä¿®æ”¹çš„æ–‡ä»¶ï¼?

        Returns:
            ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        """
        return self.structured_query_engine.rebuild_index(force=force)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import yaml
    logging.basicConfig(level=logging.INFO)

    # åŠ è½½é…ç½®
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # åˆ›å»ºRAGå¼•æ“
    engine = RAGEngine(config)

    # ç´¢å¼•æŠ€èƒ?
    print("\n=== Indexing Skills ===")
    index_result = engine.index_skills(force_rebuild=True)
    print(f"Index result: {index_result}")

    # æœç´¢æŠ€èƒ?
    print("\n=== Searching Skills ===")
    results = engine.search_skills("ç«ç„°ä¼¤å®³æŠ€èƒ?, top_k=3, return_details=True)
    for i, skill in enumerate(results, 1):
        print(f"\n{i}. {skill['skill_name']} (ç›¸ä¼¼åº? {skill['similarity']:.3f})")
        print(f"   æ–‡ä»¶: {skill['file_name']}")

    # æ¨èAction
    print("\n=== Recommending Actions ===")
    actions = engine.recommend_actions("é€ æˆä¼¤å®³å¹¶å‡»é€€æ•Œäºº", top_k=3)
    for i, action in enumerate(actions, 1):
        print(f"\n{i}. {action['action_type']} (ç›¸ä¼¼åº? {action['semantic_similarity']:.3f})")
        print(f"   åˆ†ç±»: {action['category']}")

    # ç»Ÿè®¡ä¿¡æ¯
    print("\n=== Statistics ===")
    stats = engine.get_statistics()
    print(f"Total indexed: {stats['engine_stats']['total_indexed']}")
    print(f"Total queries: {stats['engine_stats']['total_queries']}")
