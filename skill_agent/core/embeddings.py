"""
æ–‡æœ¬åµŒå…¥ç”Ÿæˆæ¨¡å—
è´Ÿè´£å°†æŠ€èƒ½æè¿°ã€Actionä¿¡æ¯è½¬æ¢ä¸ºå‘é‡è¡¨ç¤?
ä½¿ç”¨æœ¬åœ° Qwen3-Embedding-0.6B æ¨¡å‹
"""

import os
import logging
from typing import List, Union, Optional
import torch
from sentence_transformers import SentenceTransformer
from cachetools import LRUCache
import hashlib

logger = logging.getLogger(__name__)


class EmbeddingGenerator:
    """
    Qwen3 Embeddingç”Ÿæˆå™?
    ä½¿ç”¨ SentenceTransformer åŠ è½½æœ¬åœ° Qwen3-Embedding-0.6B æ¨¡å‹
    """

    def __init__(self, config: dict):
        """
        åˆå§‹åŒ–åµŒå…¥ç”Ÿæˆå™¨

        Args:
            config: åµŒå…¥é…ç½®å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
                - model_name: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¦‚ "../Data/models/Qwen3-Embedding-0.6B"ï¼?
                - device: è®¾å¤‡ç±»å‹ï¼?cpu" æˆ?"cuda"ï¼?
                - batch_size: æ‰¹é‡å¤§å°
                - max_length: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆQwen3æ”¯æŒ32Kï¼Œå»ºè®?192ï¼?
                - cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆé€šå¸¸æ— éœ€è®¾ç½®ï¼?
                - use_flash_attention: æ˜¯å¦ä½¿ç”¨flash_attention_2åŠ é€Ÿï¼ˆéœ€è¦GPUï¼?
        """
        self.config = config
        self.model_name = config.get("model_name", "../Data/models/Qwen3-Embedding-0.6B")
        self.device = config.get("device", "cpu")
        self.batch_size = config.get("batch_size", 32)
        self.max_length = config.get("max_length", 8192)  # Qwen3æ”¯æŒ32Kï¼Œé»˜è®?192
        self.cache_dir = config.get("cache_dir", None)
        self.use_flash_attention = config.get("use_flash_attention", False)

        # åŠ è½½æœ¬åœ°æ¨¡å‹
        logger.info(f"Loading Qwen3 embedding model from: {self.model_name}")

        # åŠ è½½æœ¬åœ°æ¨¡å‹
        if self.use_flash_attention and self.device == "cuda":
            # ä½¿ç”¨ flash_attention_2 åŠ é€Ÿï¼ˆéœ€è¦GPUï¼?
            logger.info("Using flash_attention_2 for acceleration")
            self.model = SentenceTransformer(
                self.model_name,
                model_kwargs={
                    "attn_implementation": "flash_attention_2",
                    "device_map": "auto"
                },
                tokenizer_kwargs={"padding_side": "left"},
                cache_folder=self.cache_dir
            )
        else:
            # æ ‡å‡†åŠ è½½æ–¹å¼ï¼ˆä»æœ¬åœ°è·¯å¾„ï¼?
            self.model = SentenceTransformer(
                self.model_name,
                device=self.device,
                cache_folder=self.cache_dir,
                tokenizer_kwargs={"padding_side": "left"},
                trust_remote_code=True  # ä¿¡ä»»æœ¬åœ°æ¨¡å‹ä»£ç 
            )

        # å†…å­˜ç¼“å­˜ï¼ˆLRUï¼Œæœ€å¤šç¼“å­?000ä¸ªembeddingï¼?
        self._cache = LRUCache(maxsize=1000)

        logger.info(f"Qwen3 embedding model loaded on device: {self.device}")
        logger.info(f"Embedding dimension: {self.model.get_sentence_embedding_dimension()}")

    def get_embedding_dimension(self) -> int:
        """è·å–åµŒå…¥å‘é‡ç»´åº¦"""
        return self.model.get_sentence_embedding_dimension()

    def _get_cache_key(self, text: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”?""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def encode(
        self,
        texts: Union[str, List[str]],
        use_cache: bool = True,
        show_progress: bool = False,
        prompt_name: Optional[str] = None
    ) -> Union[List[float], List[List[float]]]:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡

        æŒ‰ç…§Qwen3å®˜æ–¹æ–‡æ¡£ï¼?
        - æŸ¥è¯¢æ–‡æœ¬å»ºè®®ä½¿ç”¨ prompt_name="query" ä»¥è·å¾—æ›´å¥½çš„æ£€ç´¢æ€§èƒ½
        - æ–‡æ¡£æ–‡æœ¬ä¸éœ€è¦ä½¿ç”?prompt

        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡?
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ?
            prompt_name: æç¤ºåç§°ï¼Œç”¨äºä¼˜åŒ–queryç¼–ç ï¼ˆå¦‚"query"ï¼?

        Returns:
            å•ä¸ªå‘é‡æˆ–å‘é‡åˆ—è¡?
        """
        # ç»Ÿä¸€å¤„ç†ä¸ºåˆ—è¡?
        is_single = isinstance(texts, str)
        if is_single:
            texts = [texts]

        # æ£€æŸ¥ç¼“å­˜ï¼ˆç¼“å­˜æ—¶éœ€è¦è€ƒè™‘prompt_nameï¼?
        cache_suffix = f"_{prompt_name}" if prompt_name else ""
        embeddings = []
        texts_to_encode = []
        text_indices = []

        for i, text in enumerate(texts):
            if use_cache:
                cache_key = self._get_cache_key(text) + cache_suffix
                if cache_key in self._cache:
                    embeddings.append(self._cache[cache_key])
                    continue

            texts_to_encode.append(text)
            text_indices.append(i)

        # å¦‚æœæœ‰æœªç¼“å­˜çš„æ–‡æœ¬ï¼Œè¿›è¡Œç¼–ç 
        if texts_to_encode:
            logger.debug(f"Encoding {len(texts_to_encode)} texts (cache hit: {len(embeddings)}/{len(texts)})")

            # æŒ‰ç…§Qwen3æ–‡æ¡£å‡†å¤‡encodeå‚æ•°
            encode_kwargs = {
                'batch_size': self.batch_size,
                'show_progress_bar': show_progress,
                'convert_to_numpy': True,
                'normalize_embeddings': True  # L2å½’ä¸€åŒ–ï¼Œé€‚åˆä½™å¼¦ç›¸ä¼¼åº?
            }

            # æŒ‰ç…§æ–‡æ¡£ï¼šæŸ¥è¯¢ä½¿ç”?prompt_name="query"ï¼Œæ–‡æ¡£ä¸éœ€è¦prompt
            if prompt_name:
                encode_kwargs['prompt_name'] = prompt_name

            new_embeddings = self.model.encode(
                texts_to_encode,
                **encode_kwargs
            )

            # æ›´æ–°ç¼“å­˜å’Œç»“æ?
            for i, embedding in enumerate(new_embeddings):
                text = texts_to_encode[i]
                embedding_list = embedding.tolist()

                if use_cache:
                    cache_key = self._get_cache_key(text) + cache_suffix
                    self._cache[cache_key] = embedding_list

                embeddings.append(embedding_list)

        # å¦‚æœæ˜¯å•ä¸ªæ–‡æœ¬ï¼Œè¿”å›å•ä¸ªå‘é‡
        if is_single:
            return embeddings[0]

        return embeddings

    def encode_batch(
        self,
        texts: List[str],
        batch_size: Optional[int] = None,
        show_progress: bool = True,
        prompt_name: Optional[str] = None
    ) -> List[List[float]]:
        """
        æ‰¹é‡ç¼–ç æ–‡æœ¬ï¼ˆé€‚ç”¨äºå¤§è§„æ¨¡åˆå§‹åŒ–ï¼‰

        æŒ‰ç…§Qwen3æ–‡æ¡£ï¼šæ–‡æ¡£ç¼–ç ä¸éœ€è¦ä½¿ç”¨prompt

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹é‡å¤§å°ï¼ŒNoneä½¿ç”¨é»˜è®¤å€?
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ?
            prompt_name: æç¤ºåç§°ï¼ˆé€šå¸¸æ–‡æ¡£ç¼–ç ä¸éœ€è¦ï¼ŒæŸ¥è¯¢æ—¶å¯ç”?query"ï¼?

        Returns:
            å‘é‡åˆ—è¡¨
        """
        batch_size = batch_size or self.batch_size

        logger.info(f"Batch encoding {len(texts)} texts with batch_size={batch_size}")

        # æŒ‰ç…§Qwen3æ–‡æ¡£å‡†å¤‡å‚æ•°
        encode_kwargs = {
            'batch_size': batch_size,
            'show_progress_bar': show_progress,
            'convert_to_numpy': True,
            'normalize_embeddings': True  # L2å½’ä¸€åŒ?
        }

        # å¦‚æœéœ€è¦ï¼ˆå¦‚æ‰¹é‡ç¼–ç æŸ¥è¯¢ï¼‰ï¼Œå¯ä»¥æ·»åŠ prompt_name
        if prompt_name:
            encode_kwargs['prompt_name'] = prompt_name

        embeddings = self.model.encode(texts, **encode_kwargs)

        return [emb.tolist() for emb in embeddings]

    def compute_similarity(
        self,
        text1: Union[str, List[float]],
        text2: Union[str, List[float]]
    ) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬æˆ–å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº?

        Args:
            text1: æ–‡æœ¬æˆ–å‘é‡?
            text2: æ–‡æœ¬æˆ–å‘é‡?

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼?
        """
        # è·å–å‘é‡
        if isinstance(text1, str):
            vec1 = torch.tensor(self.encode(text1))
        else:
            vec1 = torch.tensor(text1)

        if isinstance(text2, str):
            vec2 = torch.tensor(self.encode(text2))
        else:
            vec2 = torch.tensor(text2)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº?
        similarity = torch.nn.functional.cosine_similarity(
            vec1.unsqueeze(0),
            vec2.unsqueeze(0)
        )

        return similarity.item()

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        logger.info("Embedding cache cleared")

    def get_cache_info(self) -> dict:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        return {
            "size": len(self._cache),
            "max_size": self._cache.maxsize,
            "hit_rate": getattr(self._cache, 'hits', 0) / max(getattr(self._cache, 'hits', 0) + getattr(self._cache, 'misses', 0), 1)
        }


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç  - ä½¿ç”¨æœ¬åœ°æ¨¡å‹
    logging.basicConfig(level=logging.INFO)

    print("=" * 60)
    print("Testing Qwen3 Embedding Generator (Local Model)")
    print("=" * 60)

    config = {
        "model_name": "../Data/models/Qwen3-Embedding-0.6B",  # æœ¬åœ°æ¨¡å‹è·¯å¾„
        "device": "cpu",
        "batch_size": 32,
        "max_length": 8192,  # Qwen3æ”¯æŒ32Kï¼Œè¿™é‡Œè®¾ä¸?192
        "cache_dir": None,
        "use_flash_attention": False  # CPUä¸æ”¯æŒflash attention
    }

    generator = EmbeddingGenerator(config)
    print(f"\nModel: {config['model_name']}")
    print(f"Embedding dimension: {generator.get_embedding_dimension()}")

    # æµ‹è¯•æ–‡æ¡£ç¼–ç ï¼ˆæŒ‰ç…§æ–‡æ¡£ï¼šä¸éœ€è¦promptï¼?
    print("\n--- Testing Document Encoding ---")
    documents = [
        "é‡Šæ”¾ä¸€é“ç«ç„°å†²å‡»æ³¢ï¼Œå¯¹æ•Œäººé€ æˆé­”æ³•ä¼¤å®³",
        "å¬å”¤ä¸€ä¸ªç«å…ƒç´ ï¼ŒæŒç»­æ”»å‡»æ•Œäº?
    ]
    doc_embeddings = generator.encode(documents)
    print(f"Encoded {len(doc_embeddings)} documents")
    print(f"Document 1 dimension: {len(doc_embeddings[0])}")
    print(f"Document 1 first 5 values: {doc_embeddings[0][:5]}")

    # æµ‹è¯•æŸ¥è¯¢ç¼–ç ï¼ˆæŒ‰ç…§æ–‡æ¡£ï¼šä½¿ç”¨prompt_name="query"ï¼?
    print("\n--- Testing Query Encoding ---")
    queries = [
        "ç«ç„°ä¼¤å®³æŠ€èƒ?,
        "å¬å”¤ç«å…ƒç´?
    ]
    query_embeddings = generator.encode(queries, prompt_name="query")
    print(f"Encoded {len(query_embeddings)} queries with prompt_name='query'")
    print(f"Query 1 dimension: {len(query_embeddings[0])}")
    print(f"Query 1 first 5 values: {query_embeddings[0][:5]}")

    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæŒ‰ç…§æ–‡æ¡£ç¤ºä¾‹ï¼?
    print("\n--- Testing Similarity Calculation ---")
    from sentence_transformers import util
    similarity_matrix = util.cos_sim(query_embeddings, doc_embeddings)
    print("Similarity Matrix (Query x Document):")
    print(similarity_matrix)

    # ç¼“å­˜ä¿¡æ¯
    print("\n--- Cache Info ---")
    print(generator.get_cache_info())

    print("\n" + "=" * 60)
    print("âœ?All tests passed!")
