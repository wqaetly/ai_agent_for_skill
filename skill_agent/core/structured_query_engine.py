"""
ç»“æž„åŒ–æŸ¥è¯¢å¼•æ“?é›†æˆç»†ç²’åº¦ç´¢å¼•ã€æŸ¥è¯¢è§£æžå’Œç¼“å­˜æœºåˆ¶
"""

import time
from typing import Dict, List, Any, Optional
from pathlib import Path
from functools import lru_cache
from collections import OrderedDict

from .query_parser import QueryParser, QueryEvaluator, QueryExpression
from .fine_grained_indexer import FineGrainedIndexer
from .chunked_json_store import ChunkedJsonStore


class LRUCache:
    """ç®€å•çš„LRUç¼“å­˜å®žçŽ°"""

    def __init__(self, max_size: int = 100):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.hits = 0
        self.misses = 0

    def get(self, key: str) -> Optional[Any]:
        """èŽ·å–ç¼“å­˜å€?""
        if key in self.cache:
            # ç§»åˆ°æœ«å°¾ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
            self.cache.move_to_end(key)
            self.hits += 1
            return self.cache[key]
        else:
            self.misses += 1
            return None

    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜å€?""
        if key in self.cache:
            # æ›´æ–°å¹¶ç§»åˆ°æœ«å°?            self.cache.move_to_end(key)
        self.cache[key] = value

        # è¶…è¿‡æœ€å¤§å®¹é‡ï¼Œåˆ é™¤æœ€æ—©çš„
        if len(self.cache) > self.max_size:
            self.cache.popitem(last=False)

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.hits = 0
        self.misses = 0

    def get_stats(self) -> Dict[str, Any]:
        """èŽ·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0

        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": round(hit_rate, 4)
        }


class StructuredQueryEngine:
    """ç»“æž„åŒ–æŸ¥è¯¢å¼•æ“?""

    def __init__(
        self,
        skills_dir: str = "../../ai_agent_for_skill/Assets/Skills",
        cache_size: int = 100
    ):
        """
        Args:
            skills_dir: æŠ€èƒ½æ–‡ä»¶ç›®å½?            cache_size: ç¼“å­˜å¤§å°
        """
        self.indexer = FineGrainedIndexer(skills_dir)
        self.parser = QueryParser()
        self.evaluator = QueryEvaluator()
        self.json_store = ChunkedJsonStore()

        # æŸ¥è¯¢ç»“æžœç¼“å­˜
        self.query_cache = LRUCache(max_size=cache_size)

        # ç»Ÿè®¡æ‘˜è¦ç¼“å­˜ï¼ˆç”¨äºŽé¢‘ç¹è®¿é—®çš„ç»Ÿè®¡æ•°æ®ï¼?        self.stats_cache = LRUCache(max_size=20)

    def query(
        self,
        query_str: str,
        limit: int = 100,
        include_context: bool = True,
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œç»“æž„åŒ–æŸ¥è¯?
        Args:
            query_str: æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œå¦?"DamageAction where baseDamage > 200"
            limit: æœ€å¤§è¿”å›žç»“æžœæ•°
            include_context: æ˜¯å¦åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆè½¨é“ã€æŠ€èƒ½åç§°ç­‰ï¼?            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            {
                "results": [
                    {
                        "skill_name": "Flame Shockwave",
                        "skill_file": "FlameShockwave.json",
                        "track_name": "Damage Track",
                        "action_type": "DamageAction",
                        "action_index": 0,
                        "json_path": "tracks.$rcontent[2].actions.$rcontent[0]",
                        "line_number": 145,
                        "frame": 10,
                        "duration": 20,
                        "parameters": {...},
                        "summary": "..."
                    }
                ],
                "total_matches": 15,
                "query_time_ms": 123,
                "cache_hit": false
            }
        """
        start_time = time.time()

        # æ£€æŸ¥ç¼“å­?        cache_key = f"{query_str}|{limit}|{include_context}"
        if use_cache:
            cached_result = self.query_cache.get(cache_key)
            if cached_result is not None:
                cached_result["cache_hit"] = True
                cached_result["query_time_ms"] = round((time.time() - start_time) * 1000, 2)
                return cached_result

        # è§£æžæŸ¥è¯¢
        expression = self.parser.parse(query_str)

        # æ‰§è¡ŒæŸ¥è¯¢
        results = self._execute_query(expression, include_context)

        # é™åˆ¶ç»“æžœæ•°é‡
        limited_results = results[:limit]

        # æž„å»ºè¿”å›žæ•°æ®
        response = {
            "results": limited_results,
            "total_matches": len(results),
            "returned_count": len(limited_results),
            "query_time_ms": round((time.time() - start_time) * 1000, 2),
            "cache_hit": False
        }

        # ç¼“å­˜ç»“æžœ
        if use_cache:
            self.query_cache.set(cache_key, response)

        return response

    def _execute_query(
        self,
        expression: QueryExpression,
        include_context: bool
    ) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒæŸ¥è¯¢ï¼ŒéåŽ†ç´¢å¼?""
        results = []

        index_data = self.indexer.get_index()

        # éåŽ†æ‰€æœ‰æŠ€èƒ½æ–‡ä»?        for file_path, file_index in index_data["files"].items():
            skill_name = file_index.get("skill_name", "Unknown")

            # éåŽ†æ‰€æœ‰è½¨é?            for track in file_index.get("tracks", []):
                track_name = track.get("track_name")

                # éåŽ†æ‰€æœ‰Action
                for action in track.get("actions", []):
                    # è¯„ä¼°æ˜¯å¦æ»¡è¶³æ¡ä»¶
                    if self.evaluator.evaluate(expression, action, track_name):
                        result_item = {
                            "action_type": action["action_type"],
                            "action_index": action["action_index"],
                            "json_path": action["json_path"],
                            "line_number": action["line_number"],
                            "frame": action["frame"],
                            "duration": action["duration"],
                            "parameters": action["parameters"],
                            "summary": action["summary"]
                        }

                        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ?                        if include_context:
                            result_item["skill_name"] = skill_name
                            result_item["skill_file"] = Path(file_path).name
                            result_item["track_name"] = track_name
                            result_item["track_index"] = track["track_index"]

                        results.append(result_item)

        return results

    def get_action_detail(
        self,
        skill_file: str,
        json_path: str
    ) -> Optional[Dict[str, Any]]:
        """
        èŽ·å–Actionçš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ï¼ˆåŒ…å«åŽŸå§‹JSONï¼?
        Args:
            skill_file: æŠ€èƒ½æ–‡ä»¶å
            json_path: Actionçš„JSONè·¯å¾„

        Returns:
            å®Œæ•´çš„Actionæ•°æ®
        """
        skills_dir = Path(self.indexer.skills_dir)
        full_path = skills_dir / skill_file

        if not full_path.exists():
            return None

        # ä½¿ç”¨ChunkedJsonStoreåŠ è½½ç‰‡æ®µ
        chunk = self.json_store.load_by_path(
            str(full_path),
            json_path,
            include_context=True
        )

        return chunk

    def get_statistics(
        self,
        query_str: Optional[str] = None,
        group_by: str = "action_type",
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """
        èŽ·å–æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯

        Args:
            query_str: æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™ç»Ÿè®¡å…¨éƒ¨ï¼?            group_by: åˆ†ç»„å­—æ®µï¼Œå¦‚ "action_type", "track_name"
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            {
                "total_actions": 120,
                "groups": {
                    "DamageAction": {
                        "count": 45,
                        "avg_baseDamage": 175.3,
                        "min_baseDamage": 50,
                        "max_baseDamage": 500
                    },
                    ...
                }
            }
        """
        cache_key = f"stats|{query_str}|{group_by}"
        if use_cache:
            cached = self.stats_cache.get(cache_key)
            if cached:
                return cached

        # æ‰§è¡ŒæŸ¥è¯¢èŽ·å–ç»“æžœ
        if query_str:
            query_result = self.query(query_str, limit=10000, use_cache=False)
            actions = query_result["results"]
        else:
            # èŽ·å–æ‰€æœ‰Action
            actions = []
            index_data = self.indexer.get_index()
            for file_index in index_data["files"].values():
                for track in file_index.get("tracks", []):
                    for action in track.get("actions", []):
                        actions.append({
                            **action,
                            "track_name": track["track_name"]
                        })

        # åˆ†ç»„ç»Ÿè®¡
        groups = {}
        for action in actions:
            group_key = action.get(group_by, "Unknown")

            if group_key not in groups:
                groups[group_key] = {
                    "count": 0,
                    "actions": []
                }

            groups[group_key]["count"] += 1
            groups[group_key]["actions"].append(action)

        # è®¡ç®—æ¯ç»„çš„å‚æ•°ç»Ÿè®?        for group_key, group_data in groups.items():
            group_actions = group_data["actions"]

            # æ”¶é›†æ‰€æœ‰å‚æ•?            param_stats = {}
            for action in group_actions:
                for param_name, param_value in action.get("parameters", {}).items():
                    if param_name not in param_stats:
                        param_stats[param_name] = []

                    # åªç»Ÿè®¡æ•°å€¼å‚æ•?                    if isinstance(param_value, (int, float)):
                        param_stats[param_name].append(param_value)

            # è®¡ç®—å‚æ•°çš„min/max/avg
            for param_name, values in param_stats.items():
                if values:
                    group_data[f"avg_{param_name}"] = round(sum(values) / len(values), 2)
                    group_data[f"min_{param_name}"] = min(values)
                    group_data[f"max_{param_name}"] = max(values)

            # åˆ é™¤ä¸´æ—¶çš„actionsåˆ—è¡¨
            del group_data["actions"]

        stats = {
            "total_actions": len(actions),
            "groups": groups
        }

        # ç¼“å­˜ç»“æžœ
        if use_cache:
            self.stats_cache.set(cache_key, stats)

        return stats

    def rebuild_index(self, force: bool = False) -> Dict[str, Any]:
        """
        é‡å»ºç»†ç²’åº¦ç´¢å¼?
        Args:
            force: å¼ºåˆ¶é‡å»ºæ‰€æœ‰æ–‡ä»?
        Returns:
            ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        """
        # æ¸…ç©ºç¼“å­˜
        self.query_cache.clear()
        self.stats_cache.clear()

        # é‡å»ºç´¢å¼•
        return self.indexer.index_all_skills(force_rebuild=force)

    def get_cache_stats(self) -> Dict[str, Any]:
        """èŽ·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "query_cache": self.query_cache.get_stats(),
            "stats_cache": self.stats_cache.get_stats()
        }

    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­?""
        self.query_cache.clear()
        self.stats_cache.clear()


# ==================== ä¾¿æ·å‡½æ•° ====================

def query_skills(
    query_str: str,
    limit: int = 100,
    skills_dir: str = None
) -> Dict[str, Any]:
    """
    æ‰§è¡Œç»“æž„åŒ–æŸ¥è¯¢ï¼ˆä¾¿æ·å‡½æ•°ï¼?
    Args:
        query_str: æŸ¥è¯¢å­—ç¬¦ä¸?        limit: æœ€å¤§è¿”å›žç»“æžœæ•°
        skills_dir: æŠ€èƒ½ç›®å½•ï¼ˆå¯é€‰ï¼‰

    Returns:
        æŸ¥è¯¢ç»“æžœ

    Examples:
        >>> query_skills("DamageAction where baseDamage > 200")
        {
            "results": [...],
            "total_matches": 15,
            "query_time_ms": 45.2
        }

        >>> query_skills("baseDamage between 100 and 300")
        >>> query_skills("animationClipName contains 'Attack'")
    """
    if skills_dir:
        engine = StructuredQueryEngine(skills_dir)
    else:
        engine = StructuredQueryEngine()

    return engine.query(query_str, limit=limit)
