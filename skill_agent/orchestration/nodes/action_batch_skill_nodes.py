"""
Actionæ‰¹æ¬¡çº§æ¸è¿›å¼æŠ€èƒ½ç”ŸæˆèŠ‚ç‚¹å®ç°
å®ç°æ›´ç»†ç²’åº¦çš„ç”Ÿæˆï¼šéª¨æ¶ â†’ Trackæ‰¹æ¬¡è§„åˆ’ â†’ æ‰¹æ¬¡çº§Actionç”Ÿæˆ â†’ Trackç»„è£… â†’ æŠ€èƒ½ç»„è£…

ä¼˜åŠ¿:
1. Tokenæ¶ˆè€—é™ä½50%ï¼ˆæ¯æ‰¹æ¬¡3-5ä¸ªactions vs æ•´Track 15ä¸ªactionsï¼‰
2. é”™è¯¯éš”ç¦»æ€§ä¼˜ç§€ï¼ˆå•æ‰¹æ¬¡å¤±è´¥ä¸å½±å“å…¶ä»–æ‰¹æ¬¡ï¼‰
3. ç”Ÿæˆè´¨é‡æå‡ï¼ˆé¿å…é•¿è¾“å‡ºå¯¼è‡´çš„ååŠæ®µè´¨é‡ä¸‹é™ï¼‰
4. æµå¼è¾“å‡ºæ”¯æŒï¼ˆå®æ—¶è¿›åº¦åé¦ˆï¼‰
"""

import json
import logging
import math
import operator
import time
from functools import lru_cache
from typing import Any, Dict, List, Tuple, TypedDict, Annotated, Optional, Literal

from langchain_core.messages import AIMessage, AnyMessage
from langgraph.graph.message import add_messages
from langgraph.types import StreamWriter
from langgraph.config import get_stream_writer
from pydantic import ValidationError

from .skill_nodes import get_llm, _prepare_payload_text
from ..streaming import (
    ProgressEventType,
    ProgressCalculator,
    emit_progress,
)
from .progressive_skill_nodes import (
    format_similar_skills,
    skeleton_generator_node,  # å¤ç”¨éª¨æ¶ç”Ÿæˆ
    should_continue_to_track_generation,  # å¤ç”¨éª¨æ¶éªŒè¯
    skill_assembler_node,  # å¤ç”¨æŠ€èƒ½ç»„è£…
    finalize_progressive_node,  # å¤ç”¨æœ€ç»ˆåŒ–
    should_finalize_or_fail,  # å¤ç”¨æœ€ç»ˆåˆ¤æ–­
)
from ..schemas import (
    SkillSkeletonSchema,
    ActionBatchPlan,
    ActionBatch,
    SkillAction,
    SkillTrack,
    # æ–°å¢ï¼šæ‰¹æ¬¡ä¸Šä¸‹æ–‡ç›¸å…³
    BatchPhase,
    SemanticGroup,
    CompletedActionSummary,
    BatchContextState,
    SemanticRule,
)

# å‚æ•°æ·±åº¦éªŒè¯æ¨¡å—ï¼ˆå¯é€‰ä¾èµ–ï¼Œå¤±è´¥æ—¶é™çº§ï¼‰
try:
    from .parameter_validator import validate_batch_actions_deep
    HAS_DEEP_VALIDATOR = True
except ImportError:
    HAS_DEEP_VALIDATOR = False
    validate_batch_actions_deep = None  # type: ignore

logger = logging.getLogger(__name__)


# ==================== æµå¼è¾“å‡ºè¾…åŠ©å‡½æ•° ====================

def _get_writer_safe() -> Optional[Any]:
    """
    å®‰å…¨è·å–StreamWriter

    åœ¨éæµå¼æ‰§è¡Œç¯å¢ƒä¸­ä¸ä¼šæŠ¥é”™
    """
    try:
        return get_stream_writer()
    except Exception:
        return None


def _emit_progress(
    event_type: ProgressEventType,
    message: str,
    state: Optional[Dict[str, Any]] = None,
    **kwargs
):
    """
    å‘é€è¿›åº¦äº‹ä»¶çš„ä¾¿æ·å‡½æ•°

    è‡ªåŠ¨ä»stateä¸­æå–ä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    writer = _get_writer_safe()
    if writer is None:
        logger.debug(f"[{event_type.value}] {message}")
        return

    # ä»stateæå–è¿›åº¦ä¿¡æ¯
    extra_data = {}
    if state:
        track_plan = state.get("track_plan", [])
        current_track_idx = state.get("current_track_index", 0)
        batch_plan = state.get("current_track_batch_plan", [])
        current_batch_idx = state.get("current_batch_index", 0)

        extra_data["track_index"] = current_track_idx
        extra_data["total_tracks"] = len(track_plan)
        extra_data["batch_index"] = current_batch_idx
        extra_data["total_batches"] = len(batch_plan)

        # è®¡ç®—è¿›åº¦
        if track_plan:
            # éª¨æ¶ 10% + tracks 80% + ç»„è£… 10%
            skeleton_progress = 0.1
            track_progress = 0.0

            total_tracks = len(track_plan)
            if total_tracks > 0:
                completed_tracks = current_track_idx
                # å½“å‰trackå†…çš„æ‰¹æ¬¡è¿›åº¦
                if batch_plan:
                    current_track_batch_progress = current_batch_idx / len(batch_plan)
                else:
                    current_track_batch_progress = 0

                track_progress = (completed_tracks + current_track_batch_progress) / total_tracks
                track_progress *= 0.8  # 80% æƒé‡

            extra_data["progress"] = skeleton_progress + track_progress

        if current_track_idx < len(track_plan):
            extra_data["track_name"] = track_plan[current_track_idx].get("trackName", "")

    # åˆå¹¶é¢å¤–å‚æ•°
    extra_data.update(kwargs)

    emit_progress(writer, event_type, message, **extra_data)


# ==================== è¯­ä¹‰éªŒè¯è§„åˆ™å®šä¹‰ ====================

SEMANTIC_RULES: List[SemanticRule] = [
    # === ä¼¤å®³ç›¸å…³è§„åˆ™ ===
    {
        "name": "damage_requires_animation",
        "condition": "DamageAction",
        "requires_before": ["AnimationAction", "SpawnEffectAction"],
        "suggests_after": [],
        "suggests_with": ["SpawnEffectAction"],
        "severity": "warning"
    },
    {
        "name": "area_damage_needs_effect",
        "condition": "AreaOfEffectAction",
        "requires_before": [],
        "suggests_after": [],
        "suggests_with": ["SpawnEffectAction", "CameraAction"],
        "severity": "info"
    },
    {
        "name": "projectile_requires_spawn",
        "condition": "ProjectileAction",
        "requires_before": ["AnimationAction"],
        "suggests_after": ["DamageAction"],
        "suggests_with": ["SpawnEffectAction"],
        "severity": "warning"
    },

    # === Buff/Debuffç›¸å…³è§„åˆ™ ===
    {
        "name": "buff_needs_effect",
        "condition": "BuffAction",
        "requires_before": [],
        "suggests_after": [],
        "suggests_with": ["SpawnEffectAction"],
        "severity": "info"
    },
    {
        "name": "heal_with_effect",
        "condition": "HealAction",
        "requires_before": [],
        "suggests_after": [],
        "suggests_with": ["SpawnEffectAction", "AudioAction"],
        "severity": "info"
    },
    {
        "name": "shield_with_visual",
        "condition": "ShieldAction",
        "requires_before": [],
        "suggests_after": [],
        "suggests_with": ["SpawnEffectAction"],
        "severity": "info"
    },

    # === ç§»åŠ¨ç›¸å…³è§„åˆ™ ===
    {
        "name": "movement_followed_by_action",
        "condition": "MovementAction",
        "requires_before": [],
        "suggests_after": ["DamageAction", "SpawnEffectAction"],
        "suggests_with": [],
        "severity": "info"
    },
    {
        "name": "teleport_needs_effect",
        "condition": "TeleportAction",
        "requires_before": [],
        "suggests_after": [],
        "suggests_with": ["SpawnEffectAction", "AudioAction"],
        "severity": "warning"
    },
    {
        "name": "dash_with_trail",
        "condition": "DashAction",
        "requires_before": ["AnimationAction"],
        "suggests_after": ["DamageAction"],
        "suggests_with": ["SpawnEffectAction"],
        "severity": "info"
    },

    # === éŸ³æ•ˆç›¸å…³è§„åˆ™ ===
    {
        "name": "audio_with_animation",
        "condition": "AudioAction",
        "requires_before": [],
        "suggests_after": [],
        "suggests_with": ["AnimationAction"],
        "severity": "info"
    },
    {
        "name": "play_sound_with_action",
        "condition": "PlaySoundAction",
        "requires_before": [],
        "suggests_after": [],
        "suggests_with": ["AnimationAction", "SpawnEffectAction"],
        "severity": "info"
    },

    # === é•œå¤´ç›¸å…³è§„åˆ™ ===
    {
        "name": "camera_shake_with_impact",
        "condition": "CameraAction",
        "requires_before": [],
        "suggests_after": [],
        "suggests_with": ["DamageAction", "SpawnEffectAction"],
        "severity": "info"
    },
    {
        "name": "camera_focus_before_skill",
        "condition": "CameraFocusAction",
        "requires_before": [],
        "suggests_after": ["AnimationAction", "SpawnEffectAction"],
        "suggests_with": [],
        "severity": "info"
    },

    # === å¬å”¤ç›¸å…³è§„åˆ™ ===
    {
        "name": "summon_with_effect",
        "condition": "SummonAction",
        "requires_before": ["AnimationAction"],
        "suggests_after": [],
        "suggests_with": ["SpawnEffectAction", "AudioAction"],
        "severity": "warning"
    },

    # === æ§åˆ¶ç›¸å…³è§„åˆ™ ===
    {
        "name": "control_with_animation",
        "condition": "ControlAction",
        "requires_before": ["AnimationAction"],
        "suggests_after": [],
        "suggests_with": ["SpawnEffectAction"],
        "severity": "info"
    },
    {
        "name": "stun_with_effect",
        "condition": "StunAction",
        "requires_before": [],
        "suggests_after": [],
        "suggests_with": ["SpawnEffectAction", "AudioAction"],
        "severity": "info"
    },

    # === ç¢°æ’ç›¸å…³è§„åˆ™ ===
    {
        "name": "collision_after_projectile",
        "condition": "CollisionAction",
        "requires_before": ["ProjectileAction", "SpawnEffectAction"],
        "suggests_after": ["DamageAction"],
        "suggests_with": [],
        "severity": "info"
    },

    # === èµ„æºç›¸å…³è§„åˆ™ ===
    {
        "name": "resource_with_effect",
        "condition": "ResourceAction",
        "requires_before": [],
        "suggests_after": [],
        "suggests_with": ["SpawnEffectAction"],
        "severity": "info"
    },
]


# === é˜¶æ®µæ€§è§„åˆ™ï¼ˆæ ¹æ®æ‰¹æ¬¡é˜¶æ®µåº”ç”¨ä¸åŒè§„åˆ™ï¼‰===

PHASE_RULES: Dict[str, List[str]] = {
    "setup": [
        "åŠ¨ç”»Actionåº”æ”¾åœ¨èµ·æ‰‹é˜¶æ®µ",
        "å‡†å¤‡ç‰¹æ•ˆåº”åœ¨ä¼¤å®³å‰ç”Ÿæˆ",
        "é•œå¤´èšç„¦é€‚åˆæ”¾åœ¨æŠ€èƒ½å¼€å§‹",
    ],
    "main": [
        "æ ¸å¿ƒä¼¤å®³å’Œæ•ˆæœåº”åœ¨ä¸»ä½“é˜¶æ®µ",
        "Buff/Debuffåº”ç”¨é€šå¸¸åœ¨ä¸»ä½“é˜¶æ®µ",
        "AOEä¼¤å®³é€‚åˆæ”¾åœ¨ä¸»ä½“é˜¶æ®µä¸­æ®µ",
    ],
    "cleanup": [
        "åæ‘‡åŠ¨ç”»åº”åœ¨æ”¶å°¾é˜¶æ®µ",
        "æ¶ˆæ•£ç‰¹æ•ˆåº”åœ¨æ”¶å°¾é˜¶æ®µ",
        "æŠ€èƒ½ç»“æŸéŸ³æ•ˆæ”¾åœ¨æ”¶å°¾",
    ],
}


# === Trackç±»å‹ç‰¹å®šè§„åˆ™ ===

TRACK_TYPE_RULES: Dict[str, Dict[str, Any]] = {
    "animation": {
        "primary_actions": ["AnimationAction"],
        "forbidden_actions": ["DamageAction", "BuffAction"],  # ä¼¤å®³å’ŒBuffä¸åº”åœ¨åŠ¨ç”»è½¨é“
        "typical_count": (1, 5),  # å…¸å‹actionæ•°é‡èŒƒå›´
    },
    "effect": {
        "primary_actions": ["SpawnEffectAction", "DamageAction", "BuffAction", "HealAction"],
        "forbidden_actions": [],
        "typical_count": (2, 10),
    },
    "audio": {
        "primary_actions": ["AudioAction", "PlaySoundAction"],
        "forbidden_actions": ["DamageAction", "MovementAction"],
        "typical_count": (1, 5),
    },
    "movement": {
        "primary_actions": ["MovementAction", "TeleportAction", "DashAction"],
        "forbidden_actions": [],
        "typical_count": (1, 3),
    },
    "camera": {
        "primary_actions": ["CameraAction", "CameraFocusAction"],
        "forbidden_actions": ["DamageAction", "BuffAction"],
        "typical_count": (1, 3),
    },
}

# åŠŸèƒ½å…³é”®è¯åˆ°Actionç±»å‹çš„æ˜ å°„
SEMANTIC_KEYWORD_MAP: Dict[str, List[str]] = {
    # åŠ¨ç”»ç›¸å…³
    "åŠ¨ç”»": ["AnimationAction"],
    "æ’­æ”¾": ["AnimationAction", "PlaySoundAction"],
    "å‰æ‘‡": ["AnimationAction"],
    "åæ‘‡": ["AnimationAction"],
    "æ–½æ³•": ["AnimationAction"],
    # ä¼¤å®³ç›¸å…³
    "ä¼¤å®³": ["DamageAction"],
    "æ”»å‡»": ["DamageAction"],
    "é€ æˆ": ["DamageAction"],
    "æ‰“å‡»": ["DamageAction"],
    # ç‰¹æ•ˆç›¸å…³
    "ç‰¹æ•ˆ": ["SpawnEffectAction"],
    "æ•ˆæœ": ["SpawnEffectAction"],
    "ç”Ÿæˆ": ["SpawnEffectAction"],
    "ç²’å­": ["SpawnEffectAction"],
    # Buff/Debuffç›¸å…³
    "buff": ["BuffAction"],
    "å¢ç›Š": ["BuffAction"],
    "debuff": ["DebuffAction"],
    "å‡ç›Š": ["DebuffAction"],
    "çŠ¶æ€": ["BuffAction", "DebuffAction"],
    "ç‡ƒçƒ§": ["DebuffAction"],
    "å†»ç»“": ["DebuffAction"],
    # ç§»åŠ¨ç›¸å…³
    "ç§»åŠ¨": ["MovementAction"],
    "ä½ç§»": ["MovementAction"],
    "å†²åˆº": ["MovementAction"],
    "ä¼ é€": ["MovementAction"],
    # éŸ³æ•ˆç›¸å…³
    "éŸ³æ•ˆ": ["PlaySoundAction"],
    "å£°éŸ³": ["PlaySoundAction"],
}


# ==================== ä¸Šä¸‹æ–‡ç®¡ç†å‡½æ•° ====================

def create_initial_context(
    track_plan_item: Dict[str, Any],
    skeleton: Dict[str, Any],
    batch_plan: List[Dict[str, Any]]
) -> BatchContextState:
    """
    åˆ›å»ºåˆå§‹æ‰¹æ¬¡ä¸Šä¸‹æ–‡

    Args:
        track_plan_item: å½“å‰Trackçš„è®¡åˆ’ä¿¡æ¯
        skeleton: æŠ€èƒ½éª¨æ¶ä¿¡æ¯
        batch_plan: æ‰¹æ¬¡è®¡åˆ’åˆ—è¡¨

    Returns:
        åˆå§‹åŒ–çš„BatchContextState
    """
    purpose = track_plan_item.get("purpose", "")
    track_name = track_plan_item.get("trackName", "Unknown Track")

    # è§£æpurposeæå–å»ºè®®çš„Actionç±»å‹
    suggested_types = parse_purpose_to_action_types(purpose)

    # æ ¹æ®Trackåç§°æ¨æ–­åˆå§‹çº¦æŸ
    must_follow = []
    if "animation" in track_name.lower():
        must_follow.append("åŠ¨ç”»Actionåº”åœ¨å…¶ä»–Actionä¹‹å‰")
    elif "effect" in track_name.lower():
        must_follow.append("ç‰¹æ•ˆå’Œä¼¤å®³Actionåº”åœ¨åŠ¨ç”»ä¹‹å")

    return {
        "batch_id": 0,
        "total_batches": len(batch_plan),
        "phase": BatchPhase.SETUP.value,
        "design_intent": purpose,
        "current_goal": batch_plan[0].get("context", "") if batch_plan else "",
        "completed_actions": [],
        "used_action_types": [],
        "occupied_frames": [],
        "must_follow": must_follow,
        "suggested_types": suggested_types,
        "avoid_patterns": [],
        "prerequisites_met": [],
        "pending_effects": [],
        "violations": [],
    }


def update_context_after_batch(
    context: BatchContextState,
    batch_actions: List[Dict[str, Any]],
    batch_plan: List[Dict[str, Any]],
    next_batch_idx: int
) -> BatchContextState:
    """
    æ‰¹æ¬¡ç”Ÿæˆå®Œæˆåæ›´æ–°ä¸Šä¸‹æ–‡

    Args:
        context: å½“å‰ä¸Šä¸‹æ–‡
        batch_actions: æœ¬æ‰¹æ¬¡ç”Ÿæˆçš„actions
        batch_plan: æ‰¹æ¬¡è®¡åˆ’åˆ—è¡¨
        next_batch_idx: ä¸‹ä¸€ä¸ªæ‰¹æ¬¡ç´¢å¼•

    Returns:
        æ›´æ–°åçš„BatchContextState
    """
    # å¤åˆ¶ä¸Šä¸‹æ–‡ï¼ˆé¿å…ç›´æ¥ä¿®æ”¹ï¼‰
    new_context: BatchContextState = dict(context)  # type: ignore

    # æ›´æ–°å·²å®Œæˆactionsæ‘˜è¦
    completed = list(context.get("completed_actions", []))
    for action in batch_actions:
        params = action.get("parameters", {})
        odin_type = params.get("_odin_type", "")
        # æå–ç®€åŒ–ç±»å‹åï¼ˆå¦‚ "DamageAction"ï¼‰
        action_type = extract_action_type_name(odin_type)

        # æå–å…³é”®å‚æ•°
        key_params = extract_key_params(params)

        completed.append({
            "frame": action.get("frame", 0),
            "duration": action.get("duration", 0),
            "action_type": action_type,
            "key_params": key_params,
        })
    new_context["completed_actions"] = completed

    # æ›´æ–°å·²ä½¿ç”¨çš„Actionç±»å‹
    used_types = list(context.get("used_action_types", []))
    for action in batch_actions:
        action_type = extract_action_type_name(
            action.get("parameters", {}).get("_odin_type", "")
        )
        if action_type and action_type not in used_types:
            used_types.append(action_type)
    new_context["used_action_types"] = used_types

    # æ›´æ–°å·²å ç”¨å¸§åŒºé—´
    occupied = list(context.get("occupied_frames", []))
    for action in batch_actions:
        frame = action.get("frame", 0)
        duration = action.get("duration", 0)
        occupied.append((frame, frame + duration))
    # æ’åºå¹¶åˆå¹¶é‡å åŒºé—´
    new_context["occupied_frames"] = merge_frame_intervals(occupied)

    # æ›´æ–°æ‰¹æ¬¡ä¿¡æ¯
    new_context["batch_id"] = next_batch_idx

    # æ›´æ–°é˜¶æ®µ
    if next_batch_idx < len(batch_plan):
        total = len(batch_plan)
        if next_batch_idx < total * 0.3:
            new_context["phase"] = BatchPhase.SETUP.value
        elif next_batch_idx < total * 0.7:
            new_context["phase"] = BatchPhase.MAIN.value
        else:
            new_context["phase"] = BatchPhase.CLEANUP.value

        new_context["current_goal"] = batch_plan[next_batch_idx].get("context", "")

    # æ£€æŸ¥è¯­ä¹‰è§„åˆ™ï¼Œæ›´æ–°prerequisites_met
    prerequisites_met = list(context.get("prerequisites_met", []))
    if "AnimationAction" in used_types:
        prerequisites_met.append("animation_played")
    if "SpawnEffectAction" in used_types:
        prerequisites_met.append("effect_spawned")
    new_context["prerequisites_met"] = list(set(prerequisites_met))

    return new_context


def format_context_for_prompt(context: BatchContextState) -> str:
    """
    å°†ä¸Šä¸‹æ–‡æ ¼å¼åŒ–ä¸ºpromptæ–‡æœ¬

    Args:
        context: æ‰¹æ¬¡ä¸Šä¸‹æ–‡

    Returns:
        æ ¼å¼åŒ–çš„æ–‡æœ¬ï¼Œç”¨äºæ’å…¥prompt
    """
    lines = []

    # è®¾è®¡æ„å›¾
    if context.get("design_intent"):
        lines.append(f"**Trackè®¾è®¡æ„å›¾**: {context['design_intent']}")

    # å½“å‰æ‰¹æ¬¡ç›®æ ‡
    if context.get("current_goal"):
        lines.append(f"**å½“å‰æ‰¹æ¬¡ç›®æ ‡**: {context['current_goal']}")

    # é˜¶æ®µä¿¡æ¯
    phase = context.get("phase", "main")
    phase_desc = {
        "setup": "èµ·æ‰‹é˜¶æ®µï¼ˆåŠ¨ç”»å‰æ‘‡ã€å‡†å¤‡ç‰¹æ•ˆï¼‰",
        "main": "ä¸»ä½“é˜¶æ®µï¼ˆæ ¸å¿ƒä¼¤å®³ã€ä¸»è¦æ•ˆæœï¼‰",
        "cleanup": "æ”¶å°¾é˜¶æ®µï¼ˆåæ‘‡ã€æ¶ˆæ•£ç‰¹æ•ˆï¼‰"
    }
    lines.append(f"**å½“å‰é˜¶æ®µ**: {phase_desc.get(phase, phase)}")

    # å·²å®Œæˆactionsæ‘˜è¦
    completed = context.get("completed_actions", [])
    if completed:
        lines.append("**å·²ç”ŸæˆActions**:")
        for action in completed[-8:]:  # åªæ˜¾ç¤ºæœ€è¿‘8ä¸ª
            lines.append(
                f"  - å¸§{action['frame']}-{action['frame']+action['duration']}: "
                f"{action['action_type']}"
            )
    else:
        lines.append("**å·²ç”ŸæˆActions**: æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰")

    # å·²å ç”¨å¸§åŒºé—´
    occupied = context.get("occupied_frames", [])
    if occupied:
        intervals = ", ".join([f"{s}-{e}" for s, e in occupied[-5:]])
        lines.append(f"**å·²å ç”¨å¸§åŒºé—´**: {intervals}")

    # çº¦æŸå’Œå»ºè®®
    if context.get("must_follow"):
        lines.append(f"**å¿…é¡»éµå®ˆ**: {'; '.join(context['must_follow'])}")

    if context.get("suggested_types"):
        lines.append(f"**å»ºè®®Actionç±»å‹**: {', '.join(context['suggested_types'][:5])}")

    if context.get("avoid_patterns"):
        lines.append(f"**åº”é¿å…**: {'; '.join(context['avoid_patterns'][:3])}")

    return "\n".join(lines)


# ==================== è¾…åŠ©å‡½æ•° ====================

def extract_action_type_name(odin_type: str) -> str:
    """
    ä»_odin_typeå­—ç¬¦ä¸²æå–ç®€åŒ–çš„Actionç±»å‹å

    Args:
        odin_type: å¦‚ "6|SkillSystem.Actions.DamageAction, Assembly-CSharp"

    Returns:
        ç®€åŒ–åç§°å¦‚ "DamageAction"
    """
    if not odin_type:
        return "Unknown"

    # å»æ‰IDå‰ç¼€
    if "|" in odin_type:
        odin_type = odin_type.split("|", 1)[1]

    # æå–ç±»å
    if "." in odin_type:
        parts = odin_type.split(".")
        # æ‰¾åˆ°Actionsåé¢çš„ç±»å
        for i, part in enumerate(parts):
            if part == "Actions" and i + 1 < len(parts):
                # å»æ‰", Assembly-CSharp"åç¼€
                class_name = parts[i + 1].split(",")[0].strip()
                return class_name

    # å›é€€ï¼šè¿”å›æœ€åä¸€ä¸ªéƒ¨åˆ†
    return odin_type.split(".")[-1].split(",")[0].strip()


def extract_key_params(params: Dict[str, Any]) -> Dict[str, Any]:
    """
    æå–Actionå‚æ•°ä¸­çš„å…³é”®å‚æ•°ï¼ˆç”¨äºæ‘˜è¦ï¼‰

    Args:
        params: å®Œæ•´çš„parameterså­—å…¸

    Returns:
        åªåŒ…å«å…³é”®å‚æ•°çš„å­—å…¸
    """
    # å…³é”®å‚æ•°ç™½åå•
    key_param_names = {
        "damage", "damageAmount", "healAmount",
        "effectPrefab", "effectName",
        "animationClipName", "clipName",
        "buffId", "debuffId", "duration",
        "soundName", "audioClip",
        "moveDistance", "direction",
    }

    result = {}
    for key, value in params.items():
        if key == "_odin_type":
            continue
        if key in key_param_names:
            result[key] = value
        # ä¹Ÿæå–æ•°å€¼å‹å‚æ•°ï¼ˆå¯èƒ½æ˜¯ä¼¤å®³ã€æŒç»­æ—¶é—´ç­‰ï¼‰
        elif isinstance(value, (int, float)) and value != 0:
            result[key] = value

    # é™åˆ¶å‚æ•°æ•°é‡
    return dict(list(result.items())[:5])


def merge_frame_intervals(intervals: List[Tuple[int, int]]) -> List[Tuple[int, int]]:
    """
    åˆå¹¶é‡å çš„å¸§åŒºé—´

    Args:
        intervals: å¸§åŒºé—´åˆ—è¡¨ [(start, end), ...]

    Returns:
        åˆå¹¶åçš„åŒºé—´åˆ—è¡¨
    """
    if not intervals:
        return []

    # æ’åº
    sorted_intervals = sorted(intervals, key=lambda x: x[0])

    merged = [sorted_intervals[0]]
    for start, end in sorted_intervals[1:]:
        last_start, last_end = merged[-1]
        if start <= last_end:  # æœ‰é‡å 
            merged[-1] = (last_start, max(last_end, end))
        else:
            merged.append((start, end))

    return merged


def parse_purpose_to_action_types(purpose: str) -> List[str]:
    """
    ä»purposeæ–‡æœ¬è§£æå»ºè®®çš„Actionç±»å‹

    Args:
        purpose: Trackç”¨é€”æè¿°

    Returns:
        å»ºè®®ä½¿ç”¨çš„Actionç±»å‹åˆ—è¡¨
    """
    suggested = []
    purpose_lower = purpose.lower()

    for keyword, action_types in SEMANTIC_KEYWORD_MAP.items():
        if keyword in purpose_lower:
            for action_type in action_types:
                if action_type not in suggested:
                    suggested.append(action_type)

    return suggested


def validate_semantic_rules(
    actions: List[Dict[str, Any]],
    context: BatchContextState,
    track_type: Optional[str] = None
) -> List[str]:
    """
    éªŒè¯actionsæ˜¯å¦ç¬¦åˆè¯­ä¹‰è§„åˆ™ï¼ˆå¢å¼ºç‰ˆï¼‰

    Args:
        actions: å½“å‰æ‰¹æ¬¡çš„actions
        context: æ‰¹æ¬¡ä¸Šä¸‹æ–‡
        track_type: Trackç±»å‹ï¼ˆanimation/effect/audioç­‰ï¼‰

    Returns:
        è¿è§„ä¿¡æ¯åˆ—è¡¨
    """
    violations = []
    used_types = list(context.get("used_action_types", []))
    current_phase = context.get("phase", "main")

    # æ”¶é›†æœ¬æ‰¹æ¬¡çš„Actionç±»å‹
    batch_types = []
    for action in actions:
        action_type = extract_action_type_name(
            action.get("parameters", {}).get("_odin_type", "")
        )
        batch_types.append(action_type)

    all_types = used_types + batch_types

    # === 1. åŸºç¡€è¯­ä¹‰è§„åˆ™éªŒè¯ ===
    for rule in SEMANTIC_RULES:
        condition = rule["condition"]

        # æ£€æŸ¥æœ¬æ‰¹æ¬¡æ˜¯å¦æœ‰è§¦å‘æ¡ä»¶çš„Action
        if condition not in batch_types:
            continue

        # æ£€æŸ¥requires_before
        for required in rule.get("requires_before", []):
            if required not in all_types:
                severity = rule.get("severity", "warning")
                if severity == "error":
                    violations.append(
                        f"[é”™è¯¯] {condition} éœ€è¦ {required} å…ˆå‡ºç°"
                    )
                elif severity == "warning":
                    violations.append(
                        f"[è­¦å‘Š] å»ºè®®åœ¨ {condition} ä¹‹å‰æ·»åŠ  {required}"
                    )

        # æ£€æŸ¥suggests_with
        for suggested in rule.get("suggests_with", []):
            if suggested not in batch_types and suggested not in used_types:
                violations.append(
                    f"[å»ºè®®] {condition} é€šå¸¸ä¸ {suggested} é…åˆä½¿ç”¨"
                )

    # === 2. Trackç±»å‹ç‰¹å®šè§„åˆ™éªŒè¯ ===
    if track_type and track_type in TRACK_TYPE_RULES:
        track_rules = TRACK_TYPE_RULES[track_type]

        # æ£€æŸ¥ç¦æ­¢çš„Actionç±»å‹
        forbidden = track_rules.get("forbidden_actions", [])
        for action_type in batch_types:
            if action_type in forbidden:
                violations.append(
                    f"[è­¦å‘Š] {action_type} ä¸åº”å‡ºç°åœ¨ {track_type} è½¨é“ä¸­"
                )

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æ¨èçš„Actionç±»å‹
        primary = track_rules.get("primary_actions", [])
        has_primary = any(at in primary for at in batch_types)
        if not has_primary and batch_types:
            violations.append(
                f"[å»ºè®®] {track_type} è½¨é“å»ºè®®ä½¿ç”¨: {', '.join(primary[:3])}"
            )

    # === 3. é˜¶æ®µç‰¹å®šè§„åˆ™éªŒè¯ ===
    if current_phase in PHASE_RULES:
        phase_hints = PHASE_RULES[current_phase]

        # é˜¶æ®µæ€§æç¤ºï¼ˆåªåœ¨ç‰¹å®šæƒ…å†µä¸‹æ·»åŠ ï¼‰
        if current_phase == "setup":
            # æ£€æŸ¥èµ·æ‰‹é˜¶æ®µæ˜¯å¦ç¼ºå°‘åŠ¨ç”»
            if "AnimationAction" not in batch_types and "AnimationAction" not in used_types:
                if any(at in batch_types for at in ["DamageAction", "BuffAction"]):
                    violations.append(
                        f"[å»ºè®®] èµ·æ‰‹é˜¶æ®µåº”å…ˆæ’­æ”¾åŠ¨ç”»ï¼Œå†æ‰§è¡Œä¼¤å®³/Buff"
                    )

        elif current_phase == "cleanup":
            # æ£€æŸ¥æ”¶å°¾é˜¶æ®µæ˜¯å¦æœ‰ä¸åˆé€‚çš„Action
            cleanup_unfriendly = ["DamageAction", "BuffAction", "ProjectileAction"]
            for action_type in batch_types:
                if action_type in cleanup_unfriendly:
                    violations.append(
                        f"[å»ºè®®] {action_type} ä¸é€‚åˆæ”¾åœ¨æ”¶å°¾é˜¶æ®µ"
                    )

    # === 4. æ—¶é—´è½´å†²çªæ£€æµ‹ ===
    occupied_frames = context.get("occupied_frames", [])
    for action in actions:
        frame = action.get("frame", 0)
        duration = action.get("duration", 0)
        action_end = frame + duration

        # æ£€æŸ¥æ˜¯å¦ä¸å·²å ç”¨å¸§ä¸¥é‡é‡å ï¼ˆå…è®¸å°‘é‡é‡å ï¼‰
        for start, end in occupied_frames:
            overlap_start = max(frame, start)
            overlap_end = min(action_end, end)
            overlap = overlap_end - overlap_start

            if overlap > duration * 0.5:  # è¶…è¿‡50%é‡å 
                action_type = extract_action_type_name(
                    action.get("parameters", {}).get("_odin_type", "")
                )
                violations.append(
                    f"[å»ºè®®] {action_type}(å¸§{frame}-{action_end}) ä¸å·²æœ‰Actioné‡å è¾ƒå¤š"
                )
                break

    return violations


def validate_track_type_compliance(
    actions: List[Dict[str, Any]],
    track_type: str
) -> Tuple[List[str], List[str]]:
    """
    éªŒè¯Trackå†…actionsæ˜¯å¦ç¬¦åˆTrackç±»å‹è¦æ±‚

    Args:
        actions: Trackå†…çš„æ‰€æœ‰actions
        track_type: Trackç±»å‹

    Returns:
        (errors, warnings) å…ƒç»„
    """
    errors = []
    warnings = []

    if track_type not in TRACK_TYPE_RULES:
        return errors, warnings

    rules = TRACK_TYPE_RULES[track_type]
    primary_actions = rules.get("primary_actions", [])
    forbidden_actions = rules.get("forbidden_actions", [])
    typical_count = rules.get("typical_count", (1, 20))

    # æ”¶é›†æ‰€æœ‰actionç±»å‹
    action_types = []
    for action in actions:
        action_type = extract_action_type_name(
            action.get("parameters", {}).get("_odin_type", "")
        )
        action_types.append(action_type)

    # æ£€æŸ¥ç¦æ­¢çš„Action
    for action_type in action_types:
        if action_type in forbidden_actions:
            errors.append(
                f"Trackç±»å‹'{track_type}'ä¸å…è®¸åŒ…å«'{action_type}'"
            )

    # æ£€æŸ¥æ•°é‡èŒƒå›´
    min_count, max_count = typical_count
    actual_count = len(actions)
    if actual_count < min_count:
        warnings.append(
            f"Track '{track_type}' actionæ•°é‡({actual_count})ä½äºå»ºè®®å€¼({min_count})"
        )
    elif actual_count > max_count:
        warnings.append(
            f"Track '{track_type}' actionæ•°é‡({actual_count})è¶…è¿‡å»ºè®®å€¼({max_count})"
        )

    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸»è¦Actionç±»å‹
    has_primary = any(at in primary_actions for at in action_types)
    if not has_primary and action_types:
        warnings.append(
            f"Track '{track_type}' ç¼ºå°‘ä¸»è¦Actionç±»å‹: {', '.join(primary_actions[:2])}"
        )

    return errors, warnings


# ==================== è¯­ä¹‰æ‰¹æ¬¡è§„åˆ’å‡½æ•° ====================

def parse_purpose_to_semantic_groups(purpose: str) -> List[SemanticGroup]:
    """
    è§£æpurposeæ–‡æœ¬ä¸ºè¯­ä¹‰åŠŸèƒ½ç»„

    Args:
        purpose: Trackç”¨é€”æè¿°ï¼Œå¦‚ "æ’­æ”¾æ–½æ³•åŠ¨ç”»ã€ç”Ÿæˆç«ç„°ç‰¹æ•ˆã€é€ æˆèŒƒå›´ä¼¤å®³"

    Returns:
        è¯­ä¹‰åŠŸèƒ½ç»„åˆ—è¡¨
    """
    groups: List[SemanticGroup] = []

    # åˆ†å‰²purposeï¼ˆæ”¯æŒé€—å·ã€é¡¿å·ã€å’Œï¼‰
    import re
    segments = re.split(r'[,ï¼Œã€;ï¼›å’Œ]', purpose)

    for i, segment in enumerate(segments):
        segment = segment.strip()
        if not segment:
            continue

        # è§£æå…³é”®è¯
        keywords = []
        suggested_types = []
        for keyword, action_types in SEMANTIC_KEYWORD_MAP.items():
            if keyword in segment.lower():
                keywords.append(keyword)
                for at in action_types:
                    if at not in suggested_types:
                        suggested_types.append(at)

        if not keywords:
            # æ— æ³•è¯†åˆ«çš„åŠŸèƒ½ï¼Œä½¿ç”¨é»˜è®¤
            keywords = [segment[:10]]
            suggested_types = []

        # ç¡®å®šé˜¶æ®µ
        total_segments = len(segments)
        if i < total_segments * 0.3:
            phase = BatchPhase.SETUP.value
        elif i < total_segments * 0.7:
            phase = BatchPhase.MAIN.value
        else:
            phase = BatchPhase.CLEANUP.value

        groups.append({
            "name": segment[:20],
            "keywords": keywords,
            "suggested_action_types": suggested_types,
            "estimated_count": max(1, len(suggested_types)),
            "phase": phase,
        })

    return groups


def calculate_semantic_batch_plan(
    track_name: str,
    estimated_actions: int,
    total_duration: int,
    purpose: str
) -> Tuple[List[Dict[str, Any]], BatchContextState]:
    """
    è¯­ä¹‰åŒ–æ‰¹æ¬¡è§„åˆ’ï¼ˆæ›¿ä»£åŸæœ‰çš„çº¯æ•°é‡é©±åŠ¨åˆ’åˆ†ï¼‰

    ç­–ç•¥:
    1. è§£æpurposeæå–è¯­ä¹‰åŠŸèƒ½ç»„
    2. åŸºäºåŠŸèƒ½ç»„åˆ’åˆ†æ‰¹æ¬¡ï¼ˆä¿æŒè¯­ä¹‰å…³è”ï¼‰
    3. æ•°é‡ä¸Šé™ä½œä¸ºå…œåº•

    Args:
        track_name: Trackåç§°
        estimated_actions: é¢„ä¼°actionæ•°é‡
        total_duration: æŠ€èƒ½æ€»æ—¶é•¿ï¼ˆå¸§æ•°ï¼‰
        purpose: Trackç”¨é€”æè¿°

    Returns:
        (æ‰¹æ¬¡è®¡åˆ’åˆ—è¡¨, åˆå§‹ä¸Šä¸‹æ–‡)
    """
    # è§£æè¯­ä¹‰åŠŸèƒ½ç»„
    semantic_groups = parse_purpose_to_semantic_groups(purpose)

    if not semantic_groups:
        # å›é€€åˆ°æ•°é‡é©±åŠ¨ç­–ç•¥
        logger.warning(f"âš ï¸ æ— æ³•è§£æpurposeè¯­ä¹‰ï¼Œå›é€€åˆ°æ•°é‡é©±åŠ¨ç­–ç•¥")
        batch_plan = calculate_batch_plan(
            track_name, estimated_actions, total_duration, purpose
        )
        # åˆ›å»ºåŸºç¡€ä¸Šä¸‹æ–‡
        context = create_initial_context(
            {"trackName": track_name, "purpose": purpose},
            {"totalDuration": total_duration},
            batch_plan
        )
        return batch_plan, context

    # åŸºäºè¯­ä¹‰ç»„ç”Ÿæˆæ‰¹æ¬¡è®¡åˆ’
    batch_plan = []
    remaining_actions = estimated_actions
    frame_per_group = total_duration // len(semantic_groups) if semantic_groups else total_duration

    for i, group in enumerate(semantic_groups):
        # è®¡ç®—æœ¬ç»„çš„actionæ•°é‡
        group_action_count = min(
            group["estimated_count"] + 1,  # è¯­ä¹‰ç»„ä¼°è®¡ + 1çš„buffer
            remaining_actions,
            5  # ä¸Šé™
        )

        if group_action_count <= 0:
            continue

        # è®¡ç®—å¸§èŒƒå›´
        start_frame = i * frame_per_group
        end_frame = min((i + 1) * frame_per_group, total_duration)

        # ç”Ÿæˆæ‰¹æ¬¡ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è¯­ä¹‰ä¿¡æ¯ï¼‰
        context_desc = f"{group['name']}"
        if group["suggested_action_types"]:
            context_desc += f"ï¼ˆå»ºè®®: {', '.join(group['suggested_action_types'][:2])}ï¼‰"

        batch_plan.append({
            "batch_index": len(batch_plan),
            "action_count": group_action_count,
            "start_frame_hint": start_frame,
            "end_frame_hint": end_frame,
            "context": context_desc,
            "semantic_group": group,  # é™„åŠ è¯­ä¹‰ä¿¡æ¯
        })

        remaining_actions -= group_action_count

    # å¦‚æœè¿˜æœ‰å‰©ä½™actionsï¼Œè¿½åŠ åˆ°æœ€åä¸€ä¸ªæ‰¹æ¬¡
    if remaining_actions > 0 and batch_plan:
        batch_plan[-1]["action_count"] += remaining_actions

    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ‰¹æ¬¡
    if not batch_plan:
        batch_plan.append({
            "batch_index": 0,
            "action_count": estimated_actions,
            "start_frame_hint": 0,
            "end_frame_hint": total_duration,
            "context": purpose[:50],
            "semantic_group": None,
        })

    logger.info(
        f"ğŸ“Š è¯­ä¹‰æ‰¹æ¬¡è§„åˆ’å®Œæˆ: {track_name}\n"
        f"   - è¯†åˆ« {len(semantic_groups)} ä¸ªåŠŸèƒ½ç»„ â†’ {len(batch_plan)} ä¸ªæ‰¹æ¬¡"
    )

    # åˆ›å»ºåˆå§‹ä¸Šä¸‹æ–‡
    context = create_initial_context(
        {"trackName": track_name, "purpose": purpose},
        {"totalDuration": total_duration},
        batch_plan
    )

    # å°†è¯­ä¹‰ç»„ä¿¡æ¯æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
    if semantic_groups:
        all_suggested = []
        for group in semantic_groups:
            all_suggested.extend(group["suggested_action_types"])
        context["suggested_types"] = list(set(all_suggested))

    return batch_plan, context


# ==================== State å®šä¹‰ ====================

class ActionBatchProgressiveState(TypedDict):
    """
    Actionæ‰¹æ¬¡çº§æ¸è¿›å¼ç”ŸæˆState

    æ‰©å±•è‡ªProgressiveSkillGenerationState,å¢åŠ æ‰¹æ¬¡çº§å­—æ®µå’Œè¯­ä¹‰ä¸Šä¸‹æ–‡
    """
    # === è¾“å…¥ ===
    requirement: str
    similar_skills: List[Dict[str, Any]]

    # === é˜¶æ®µ1: éª¨æ¶ç”Ÿæˆï¼ˆå¤ç”¨ï¼‰ ===
    skill_skeleton: Dict[str, Any]
    skeleton_validation_errors: List[str]
    track_plan: List[Dict[str, Any]]

    # === é˜¶æ®µ2: Trackæ‰¹æ¬¡è§„åˆ’ï¼ˆæ–°å¢ï¼‰ ===
    current_track_index: int  # å½“å‰Trackç´¢å¼•
    current_track_batch_plan: List[Dict[str, Any]]  # å½“å‰Trackçš„æ‰¹æ¬¡è®¡åˆ’

    # === é˜¶æ®µ3: æ‰¹æ¬¡çº§Actionç”Ÿæˆï¼ˆæ–°å¢ï¼‰ ===
    current_batch_index: int  # å½“å‰æ‰¹æ¬¡ç´¢å¼•
    current_batch_actions: List[Dict[str, Any]]  # å½“å‰æ‰¹æ¬¡ç”Ÿæˆçš„actions
    current_batch_errors: List[str]  # å½“å‰æ‰¹æ¬¡éªŒè¯é”™è¯¯
    batch_retry_count: int  # å½“å‰æ‰¹æ¬¡é‡è¯•æ¬¡æ•°
    max_batch_retries: int  # å•æ‰¹æ¬¡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤2,å¿«é€Ÿå¤±è´¥ï¼‰

    # === è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼ˆæ–°å¢ï¼‰ ===
    batch_context: BatchContextState  # æ‰¹æ¬¡ä¸Šä¸‹æ–‡çŠ¶æ€ï¼Œç”¨äºè·¨æ‰¹æ¬¡ä¼ é€’è®¾è®¡æ„å›¾å’Œçº¦æŸ

    # === Trackå†…actionsç´¯ç§¯ï¼ˆæ–°å¢ï¼‰ ===
    accumulated_track_actions: List[Dict[str, Any]]  # å½“å‰Trackå·²å®Œæˆçš„æ‰€æœ‰æ‰¹æ¬¡actions

    # === é˜¶æ®µ4: Trackç»„è£…ï¼ˆå¤ç”¨ä½†ä¿®æ”¹ï¼‰ ===
    generated_tracks: List[Dict[str, Any]]  # å·²å®Œæˆçš„Tracks

    # === é˜¶æ®µ5: æŠ€èƒ½ç»„è£…ï¼ˆå¤ç”¨ï¼‰ ===
    assembled_skill: Dict[str, Any]
    final_validation_errors: List[str]

    # === å…¼å®¹å­—æ®µ ===
    final_result: Dict[str, Any]
    is_valid: bool

    # === Tokenç›‘æ§å­—æ®µï¼ˆæ–°å¢ï¼‰ ===
    total_tokens_used: int  # ç´¯è®¡ä½¿ç”¨çš„tokenæ•°
    batch_token_history: List[Dict[str, Any]]  # æ¯æ‰¹æ¬¡tokenä½¿ç”¨è®°å½• [{batch_idx, input_tokens, output_tokens}]
    token_budget: int  # Tokené¢„ç®—ä¸Šé™ï¼ˆé»˜è®¤100000ï¼‰
    adaptive_batch_size: int  # è‡ªé€‚åº”æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®tokenä½¿ç”¨åŠ¨æ€è°ƒæ•´ï¼‰

    # === æµå¼è¾“å‡ºæ”¯æŒï¼ˆæ–°å¢ï¼‰ ===
    progress_calculator: Optional[Dict[str, Any]]  # è¿›åº¦è®¡ç®—å™¨çŠ¶æ€

    # === é€šç”¨ ===
    # ä½¿ç”¨add_messages reducerç¡®ä¿æ¶ˆæ¯æ­£ç¡®ç´¯ç§¯
    messages: Annotated[List[AnyMessage], add_messages]
    thread_id: str


# ==================== Tokenç›‘æ§è¾…åŠ©å‡½æ•° ====================
# P1-2: Tokené…ç½®ä»é…ç½®æ¨¡å—è¯»å–
from ..config import get_skill_gen_config as _get_config

def _get_batch_config():
    """è·å–æ‰¹æ¬¡é…ç½®"""
    return _get_config().batch

DEFAULT_TOKEN_BUDGET = _get_config().batch.token_budget
MIN_BATCH_SIZE = _get_config().batch.min_batch_size
MAX_BATCH_SIZE = _get_config().batch.max_batch_size


def estimate_tokens_for_batch(batch_size: int, track_purpose: str) -> int:
    """
    ä¼°ç®—æ‰¹æ¬¡ç”Ÿæˆæ‰€éœ€çš„tokenæ•°

    åŸºäºç»éªŒå€¼ä¼°ç®—ï¼š
    - è¾“å…¥tokensçº¦ = åŸºç¡€prompt(~1000) + æ¯action schema(~200) + ä¸Šä¸‹æ–‡(~500)
    - è¾“å‡ºtokensçº¦ = æ¯action(~150)

    Args:
        batch_size: æ‰¹æ¬¡actionæ•°é‡
        track_purpose: Trackç”¨é€”ï¼ˆå¤æ‚ç”¨é€”éœ€è¦æ›´å¤štokenï¼‰

    Returns:
        é¢„ä¼°tokenæ•°
    """
    base_input = 1500  # åŸºç¡€prompt + ç³»ç»ŸæŒ‡ä»¤
    schema_tokens = batch_size * 250  # æ¯ä¸ªactionçš„schema
    context_tokens = 500  # è¯­ä¹‰ä¸Šä¸‹æ–‡
    output_tokens = batch_size * 180  # é¢„ä¼°è¾“å‡º

    # å¤æ‚purposeå¢åŠ 10%
    complexity_factor = 1.1 if len(track_purpose) > 50 else 1.0

    return int((base_input + schema_tokens + context_tokens + output_tokens) * complexity_factor)


def calculate_adaptive_batch_size(
    remaining_budget: int,
    default_batch_size: int,
    recent_token_usage: List[Dict[str, Any]],
    remaining_batches: int
) -> int:
    """
    æ ¹æ®tokenä½¿ç”¨æƒ…å†µåŠ¨æ€è°ƒæ•´batch_size

    ç­–ç•¥ï¼š
    1. å¦‚æœå‰©ä½™é¢„ç®—ç´§å¼ ï¼Œå‡å°batch_size
    2. å¦‚æœå‰å‡ æ‰¹æ•ˆç‡é«˜ï¼ˆtoken/actionä½ï¼‰ï¼Œå¯ä»¥é€‚å½“å¢å¤§batch_size
    3. ä¿æŒåœ¨[MIN_BATCH_SIZE, MAX_BATCH_SIZE]èŒƒå›´å†…

    Args:
        remaining_budget: å‰©ä½™tokené¢„ç®—
        default_batch_size: é»˜è®¤æ‰¹æ¬¡å¤§å°
        recent_token_usage: æœ€è¿‘å‡ æ‰¹çš„tokenä½¿ç”¨è®°å½•
        remaining_batches: å‰©ä½™æ‰¹æ¬¡æ•°

    Returns:
        è°ƒæ•´åçš„batch_size
    """
    if remaining_batches <= 0:
        return default_batch_size

    # è®¡ç®—å¹³å‡æ¯æ‰¹æ¬¡tokenä½¿ç”¨
    if recent_token_usage:
        total_tokens = sum(r.get("total_tokens", 0) for r in recent_token_usage[-3:])
        avg_tokens_per_batch = total_tokens / len(recent_token_usage[-3:])
    else:
        # æ— å†å²æ•°æ®ï¼Œä½¿ç”¨ä¼°ç®—å€¼
        avg_tokens_per_batch = estimate_tokens_for_batch(default_batch_size, "")

    # é¢„ä¼°å‰©ä½™æ‰€éœ€token
    estimated_remaining_tokens = avg_tokens_per_batch * remaining_batches

    # å¦‚æœé¢„ç®—ç´§å¼ ï¼ˆå‰©ä½™é¢„ç®— < é¢„ä¼°æ‰€éœ€çš„1.5å€ï¼‰ï¼Œå‡å°batch_size
    if remaining_budget < estimated_remaining_tokens * 1.5:
        # æŒ‰æ¯”ä¾‹ç¼©å‡
        reduction_ratio = remaining_budget / (estimated_remaining_tokens * 1.5)
        adjusted_size = max(MIN_BATCH_SIZE, int(default_batch_size * reduction_ratio))
        logger.info(
            f"âš ï¸ Tokené¢„ç®—ç´§å¼ ï¼Œè°ƒæ•´batch_size: {default_batch_size} -> {adjusted_size}"
        )
        return adjusted_size

    # å¦‚æœé¢„ç®—å……è£•ä¸”å†å²æ•ˆç‡é«˜ï¼Œå¯ä»¥è€ƒè™‘å¢å¤§ï¼ˆä½†ä¿å®ˆå¢åŠ ï¼‰
    if remaining_budget > estimated_remaining_tokens * 3 and recent_token_usage:
        avg_tokens_per_action = sum(
            r.get("total_tokens", 0) / max(1, r.get("action_count", 1))
            for r in recent_token_usage[-3:]
        ) / len(recent_token_usage[-3:])

        # æ•ˆç‡é«˜ï¼ˆæ¯action token < 400ï¼‰åˆ™å°è¯•å¢å¤§
        if avg_tokens_per_action < 400 and default_batch_size < MAX_BATCH_SIZE:
            adjusted_size = min(MAX_BATCH_SIZE, default_batch_size + 1)
            logger.info(
                f"ğŸ“ˆ Tokenæ•ˆç‡è‰¯å¥½ï¼Œè°ƒæ•´batch_size: {default_batch_size} -> {adjusted_size}"
            )
            return adjusted_size

    return default_batch_size


def update_token_tracking(
    state: ActionBatchProgressiveState,
    input_tokens: int,
    output_tokens: int,
    batch_idx: int,
    action_count: int
) -> Dict[str, Any]:
    """
    æ›´æ–°tokenè¿½è¸ªä¿¡æ¯

    Args:
        state: å½“å‰State
        input_tokens: æœ¬æ¬¡è°ƒç”¨çš„è¾“å…¥tokenæ•°
        output_tokens: æœ¬æ¬¡è°ƒç”¨çš„è¾“å‡ºtokenæ•°
        batch_idx: å½“å‰æ‰¹æ¬¡ç´¢å¼•
        action_count: ç”Ÿæˆçš„actionæ•°é‡

    Returns:
        æ›´æ–°å­—æ®µçš„å­—å…¸
    """
    total_used = state.get("total_tokens_used", 0) + input_tokens + output_tokens
    history = list(state.get("batch_token_history", []))
    budget = state.get("token_budget", DEFAULT_TOKEN_BUDGET)

    # è®°å½•æœ¬æ‰¹æ¬¡tokenä½¿ç”¨
    history.append({
        "batch_idx": batch_idx,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "total_tokens": input_tokens + output_tokens,
        "action_count": action_count,
    })

    # è®¡ç®—å‰©ä½™é¢„ç®—å’Œä½¿ç”¨ç‡
    remaining = budget - total_used
    usage_rate = total_used / budget if budget > 0 else 1.0

    if usage_rate > 0.8:
        logger.warning(f"âš ï¸ Tokenä½¿ç”¨å·²è¾¾ {usage_rate*100:.1f}%ï¼Œå‰©ä½™ {remaining:,}")

    return {
        "total_tokens_used": total_used,
        "batch_token_history": history,
    }


def get_token_usage_summary(state: ActionBatchProgressiveState) -> str:
    """
    è·å–tokenä½¿ç”¨æ‘˜è¦

    Args:
        state: å½“å‰State

    Returns:
        æ ¼å¼åŒ–çš„tokenä½¿ç”¨æ‘˜è¦å­—ç¬¦ä¸²
    """
    total_used = state.get("total_tokens_used", 0)
    budget = state.get("token_budget", DEFAULT_TOKEN_BUDGET)
    history = state.get("batch_token_history", [])

    if not history:
        return "æ— tokenä½¿ç”¨è®°å½•"

    avg_per_batch = total_used / len(history) if history else 0
    total_actions = sum(r.get("action_count", 0) for r in history)
    avg_per_action = total_used / total_actions if total_actions > 0 else 0

    return (
        f"Tokenä½¿ç”¨: {total_used:,}/{budget:,} ({total_used/budget*100:.1f}%)\n"
        f"æ‰¹æ¬¡æ•°: {len(history)}, å¹³å‡æ¯æ‰¹æ¬¡: {avg_per_batch:,.0f}\n"
        f"Actionsæ•°: {total_actions}, å¹³å‡æ¯Action: {avg_per_action:,.0f}"
    )


# ==================== æ‰¹æ¬¡è§„åˆ’è¾…åŠ©å‡½æ•° ====================

def calculate_batch_plan(
    track_name: str,
    estimated_actions: int,
    total_duration: int,
    purpose: str
) -> List[Dict[str, Any]]:
    """
    åŠ¨æ€è®¡ç®—Trackçš„æ‰¹æ¬¡åˆ’åˆ†æ–¹æ¡ˆ

    ç­–ç•¥:
    - ç®€å•Track (â‰¤5 actions): ä¸åˆ†æ‰¹
    - ä¸­ç­‰Track (6-10 actions): åˆ†2æ‰¹
    - å¤æ‚Track (11-15 actions): åˆ†3æ‰¹
    - è¶…çº§å¤æ‚Track (>15 actions): æ¯æ‰¹3-5ä¸ªactions

    Args:
        track_name: Trackåç§°
        estimated_actions: é¢„ä¼°actionæ•°é‡
        total_duration: æŠ€èƒ½æ€»æ—¶é•¿ï¼ˆå¸§æ•°ï¼‰
        purpose: Trackç”¨é€”æè¿°

    Returns:
        æ‰¹æ¬¡è®¡åˆ’åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«batch_index, action_count, start_frame_hint, end_frame_hint, context
    """
    # ç¡®å®šæ‰¹æ¬¡ç­–ç•¥
    if estimated_actions <= 5:
        # ç®€å•Track: ä¸åˆ†æ‰¹
        batch_size = estimated_actions
        num_batches = 1
    elif estimated_actions <= 10:
        # ä¸­ç­‰Track: åˆ†2æ‰¹
        batch_size = math.ceil(estimated_actions / 2)
        num_batches = 2
    elif estimated_actions <= 15:
        # å¤æ‚Track: åˆ†3æ‰¹
        batch_size = math.ceil(estimated_actions / 3)
        num_batches = 3
    else:
        # è¶…çº§å¤æ‚Track: æ¯æ‰¹æœ€å¤š5ä¸ª
        batch_size = 5
        num_batches = math.ceil(estimated_actions / batch_size)

    logger.info(
        f"ğŸ“Š Track '{track_name}' æ‰¹æ¬¡è§„åˆ’: "
        f"{estimated_actions} actions â†’ {num_batches} æ‰¹æ¬¡, æ¯æ‰¹çº¦ {batch_size} actions"
    )

    # ç”Ÿæˆæ‰¹æ¬¡è®¡åˆ’
    batch_plan = []
    frame_per_batch = total_duration // num_batches if num_batches > 0 else total_duration

    for i in range(num_batches):
        # è®¡ç®—æœ¬æ‰¹æ¬¡çš„actionæ•°é‡ï¼ˆæœ€åä¸€æ‰¹å¯èƒ½æ›´å°‘ï¼‰
        if i == num_batches - 1:
            # æœ€åä¸€æ‰¹: å‰©ä½™æ‰€æœ‰actions
            batch_action_count = estimated_actions - (batch_size * i)
        else:
            batch_action_count = min(batch_size, estimated_actions - (batch_size * i))

        # è®¡ç®—å¸§èŒƒå›´æç¤º
        start_frame = i * frame_per_batch
        end_frame = min((i + 1) * frame_per_batch, total_duration)

        # ç”Ÿæˆæ‰¹æ¬¡ä¸Šä¸‹æ–‡æè¿°ï¼ˆæ ¹æ®æ‰¹æ¬¡åœ¨Trackä¸­çš„ä½ç½®ï¼‰
        if num_batches == 1:
            context = f"{purpose}"
        elif i == 0:
            context = f"{track_name}çš„å‰æœŸé˜¶æ®µ: {purpose[:40]}"
        elif i == num_batches - 1:
            context = f"{track_name}çš„æ”¶å°¾é˜¶æ®µ"
        else:
            context = f"{track_name}çš„ä¸­æœŸé˜¶æ®µï¼ˆæ‰¹æ¬¡{i+1}/{num_batches}ï¼‰"

        batch_plan.append({
            "batch_index": i,
            "action_count": batch_action_count,
            "start_frame_hint": start_frame,
            "end_frame_hint": end_frame,
            "context": context
        })

    return batch_plan


# ==================== é˜¶æ®µ2: Trackæ‰¹æ¬¡è§„åˆ’èŠ‚ç‚¹ ====================

def plan_track_batches_node(state: ActionBatchProgressiveState) -> Dict[str, Any]:
    """
    Trackæ‰¹æ¬¡è§„åˆ’èŠ‚ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼šè¯­ä¹‰åŒ–æ‰¹æ¬¡è§„åˆ’ + æµå¼è¾“å‡ºï¼‰

    èŒè´£:
    1. è·å–å½“å‰Trackçš„ä¿¡æ¯ï¼ˆtrackName, purpose, estimatedActionsï¼‰
    2. ä½¿ç”¨è¯­ä¹‰æ‰¹æ¬¡è§„åˆ’ç®—æ³•ï¼ˆè§£æpurposeæå–åŠŸèƒ½ç»„ï¼‰
    3. åˆå§‹åŒ–æ‰¹æ¬¡ä¸Šä¸‹æ–‡ï¼Œç”¨äºè·¨æ‰¹æ¬¡ä¼ é€’è®¾è®¡æ„å›¾
    4. å‘é€è¿›åº¦äº‹ä»¶

    è¾“å‡º:
    - current_track_batch_plan: æ‰¹æ¬¡è®¡åˆ’åˆ—è¡¨ï¼ˆåŒ…å«è¯­ä¹‰ä¿¡æ¯ï¼‰
    - current_batch_index: åˆå§‹åŒ–ä¸º0
    - accumulated_track_actions: åˆå§‹åŒ–ä¸ºç©ºæ•°ç»„
    - batch_retry_count: åˆå§‹åŒ–ä¸º0
    - batch_context: æ‰¹æ¬¡ä¸Šä¸‹æ–‡çŠ¶æ€
    """
    skeleton = state.get("skill_skeleton", {})
    track_plan = state.get("track_plan", [])
    current_track_idx = state.get("current_track_index", 0)

    # æ–°ä»»åŠ¡å¼€å§‹æ—¶ï¼ˆç¬¬ä¸€ä¸ªTrackï¼‰æ¸…ç†ç¼“å­˜
    if current_track_idx == 0:
        clear_action_schema_cache()
        logger.debug("ğŸ“‹ æ–°ä»»åŠ¡å¼€å§‹ï¼Œå·²æ¸…ç†Action Schemaç¼“å­˜")
        # å‘é€ç”Ÿæˆå¼€å§‹äº‹ä»¶
        _emit_progress(
            ProgressEventType.GENERATION_STARTED,
            f"å¼€å§‹ç”ŸæˆæŠ€èƒ½: {skeleton.get('skillName', 'Unknown')}",
            state,
            phase="skeleton",
            data={"skill_name": skeleton.get("skillName"), "total_tracks": len(track_plan)}
        )

    if current_track_idx >= len(track_plan):
        logger.error(f"âŒ current_track_index ({current_track_idx}) è¶…å‡ºèŒƒå›´")
        return {
            "current_track_batch_plan": [],
            "current_batch_index": 0,
            "accumulated_track_actions": [],
            "batch_retry_count": 0,
            "batch_context": {},
            "messages": [AIMessage(content="âŒ Trackç´¢å¼•é”™è¯¯")]
        }

    current_track = track_plan[current_track_idx]
    track_name = current_track.get("trackName", "Unknown Track")
    purpose = current_track.get("purpose", "")
    estimated_actions = current_track.get("estimatedActions", 5)
    total_duration = skeleton.get("totalDuration", 150)

    logger.info(
        f"ğŸ“‹ è§„åˆ’ Track [{current_track_idx + 1}/{len(track_plan)}]: {track_name} "
        f"({estimated_actions} actions)"
    )

    # å‘é€Trackå¼€å§‹äº‹ä»¶
    _emit_progress(
        ProgressEventType.TRACK_STARTED,
        f"å¼€å§‹ç”Ÿæˆ Track: {track_name}",
        state,
        phase="track",
        data={"track_name": track_name, "purpose": purpose[:50], "estimated_actions": estimated_actions}
    )

    # ä½¿ç”¨è¯­ä¹‰æ‰¹æ¬¡è§„åˆ’ï¼ˆæ›¿ä»£åŸæœ‰çš„çº¯æ•°é‡é©±åŠ¨ï¼‰
    batch_plan, batch_context = calculate_semantic_batch_plan(
        track_name=track_name,
        estimated_actions=estimated_actions,
        total_duration=total_duration,
        purpose=purpose
    )

    # å‘é€æ‰¹æ¬¡è§„åˆ’å®Œæˆäº‹ä»¶
    _emit_progress(
        ProgressEventType.BATCH_PLANNING,
        f"æ‰¹æ¬¡è§„åˆ’å®Œæˆ: {len(batch_plan)} ä¸ªæ‰¹æ¬¡",
        state,
        data={"batch_count": len(batch_plan)}
    )

    # å‡†å¤‡æ¶ˆæ¯
    messages = []
    batch_summary = "\n".join([
        f"  æ‰¹æ¬¡ {b['batch_index'] + 1}: {b['action_count']} actions "
        f"(å¸§ {b['start_frame_hint']}-{b['end_frame_hint']}) - {b.get('context', '')[:30]}"
        for b in batch_plan
    ])

    # æ·»åŠ è¯­ä¹‰ä¿¡æ¯åˆ°æ¶ˆæ¯
    suggested_types = batch_context.get("suggested_types", [])
    type_info = f"\nå»ºè®®Actionç±»å‹: {', '.join(suggested_types[:4])}" if suggested_types else ""

    messages.append(AIMessage(
        content=f"ğŸ“‹ **Trackè¯­ä¹‰æ‰¹æ¬¡è§„åˆ’å®Œæˆ**: {track_name}\n"
                f"è®¾è®¡æ„å›¾: {purpose[:50]}...\n"
                f"å…± {len(batch_plan)} ä¸ªæ‰¹æ¬¡:\n{batch_summary}{type_info}"
    ))

    return {
        "current_track_batch_plan": batch_plan,
        "current_batch_index": 0,
        "accumulated_track_actions": [],
        "batch_retry_count": 0,
        "batch_context": batch_context,
        "messages": messages
    }


# ==================== é˜¶æ®µ3: æ‰¹æ¬¡Actionç”ŸæˆèŠ‚ç‚¹ ====================

def format_previous_actions_summary(actions: List[Dict[str, Any]]) -> str:
    """
    æ ¼å¼åŒ–å·²ç”Ÿæˆactionsçš„æ‘˜è¦ï¼ˆä»…åŒ…å«å…³é”®ä¿¡æ¯ï¼Œé¿å…promptè†¨èƒ€ï¼‰

    Args:
        actions: å·²ç”Ÿæˆçš„actionsåˆ—è¡¨

    Returns:
        æ‘˜è¦æ–‡æœ¬
    """
    if not actions:
        return "æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ï¼‰"

    summary_items = []
    for action in actions[-10:]:  # åªæ˜¾ç¤ºæœ€è¿‘10ä¸ª
        frame = action.get("frame", 0)
        duration = action.get("duration", 0)
        params = action.get("parameters", {})
        action_type = params.get("_odin_type", "Unknown").split(".")[-1].replace(", Assembly-CSharp", "")

        summary_items.append(
            f"  - å¸§{frame}-{frame+duration}: {action_type}"
        )

    return "\n".join(summary_items)


def batch_action_generator_node(state: ActionBatchProgressiveState) -> Dict[str, Any]:
    """
    æ‰¹æ¬¡Actionç”ŸæˆèŠ‚ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼šä½¿ç”¨è¯­ä¹‰ä¸Šä¸‹æ–‡ + æµå¼è¾“å‡ºï¼‰

    èŒè´£:
    1. æå–å½“å‰æ‰¹æ¬¡çš„çº¦æŸæ¡ä»¶ï¼ˆå¸§èŒƒå›´ã€actionæ•°é‡ã€è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼‰
    2. ä½¿ç”¨BatchContextStateä¼ é€’è®¾è®¡æ„å›¾å’Œçº¦æŸ
    3. æ„å»ºè¯­ä¹‰å¢å¼ºçš„prompt:
       - æŠ€èƒ½éª¨æ¶ä¿¡æ¯
       - Trackä¿¡æ¯å’Œè®¾è®¡æ„å›¾
       - ç»“æ„åŒ–çš„å·²ç”Ÿæˆactionsæ‘˜è¦
       - è¯­ä¹‰çº¦æŸå’Œå»ºè®®
       - RAGæ£€ç´¢çš„Action schemas
    4. è°ƒç”¨LLMç”Ÿæˆactions
    5. ä½¿ç”¨structured outputç¡®ä¿æ ¼å¼
    6. å‘é€è¿›åº¦äº‹ä»¶

    è¾“å‡º:
    - current_batch_actions: å½“å‰æ‰¹æ¬¡ç”Ÿæˆçš„actionsåˆ—è¡¨
    """
    from ..prompts.prompt_manager import get_prompt_manager
    from .json_utils import extract_json_from_markdown
    from .progressive_skill_nodes import (
        search_actions_by_track_type,
        infer_track_type,
        format_action_schemas_for_prompt,
    )

    skeleton = state["skill_skeleton"]
    track_plan = state["track_plan"]
    current_track_idx = state["current_track_index"]
    batch_plan = state["current_track_batch_plan"]
    current_batch_idx = state["current_batch_index"]
    accumulated_actions = state.get("accumulated_track_actions", [])
    # è·å–è¯­ä¹‰ä¸Šä¸‹æ–‡ï¼ˆæ–°å¢ï¼‰
    batch_context_state = state.get("batch_context", {})

    if current_batch_idx >= len(batch_plan):
        logger.error(f"âŒ current_batch_index ({current_batch_idx}) è¶…å‡ºèŒƒå›´")
        return {
            "current_batch_actions": [],
            "messages": [AIMessage(content="âŒ æ‰¹æ¬¡ç´¢å¼•é”™è¯¯")]
        }

    current_track = track_plan[current_track_idx]
    current_batch = batch_plan[current_batch_idx]

    track_name = current_track.get("trackName", "Unknown Track")
    purpose = current_track.get("purpose", "")
    batch_action_count = current_batch["action_count"]
    start_frame_hint = current_batch["start_frame_hint"]
    end_frame_hint = current_batch["end_frame_hint"]
    batch_context_desc = current_batch["context"]

    # è·å–å½“å‰é˜¶æ®µä¿¡æ¯
    current_phase = batch_context_state.get("phase", "main")
    phase_names = {"setup": "èµ·æ‰‹é˜¶æ®µ", "main": "ä¸»ä½“é˜¶æ®µ", "cleanup": "æ”¶å°¾é˜¶æ®µ"}

    logger.info(
        f"ğŸ¯ ç”Ÿæˆæ‰¹æ¬¡ [{current_batch_idx + 1}/{len(batch_plan)}] ({phase_names.get(current_phase, current_phase)}): "
        f"{track_name}, {batch_action_count} actions, å¸§ {start_frame_hint}-{end_frame_hint}"
    )

    # å‘é€æ‰¹æ¬¡å¼€å§‹äº‹ä»¶
    _emit_progress(
        ProgressEventType.BATCH_STARTED,
        f"ç”Ÿæˆæ‰¹æ¬¡ {current_batch_idx + 1}/{len(batch_plan)}: {batch_context_desc[:30]}",
        state,
        phase="batch",
        data={
            "batch_action_count": batch_action_count,
            "frame_range": f"{start_frame_hint}-{end_frame_hint}",
            "phase": current_phase
        }
    )

    # å‡†å¤‡æ¶ˆæ¯
    messages = []
    messages.append(AIMessage(
        content=f"ğŸ¯ **æ‰¹æ¬¡ [{current_batch_idx + 1}/{len(batch_plan)}]** ({phase_names.get(current_phase, current_phase)})\n"
                f"ç›®æ ‡: {batch_context_desc}\n"
                f"ç”Ÿæˆ {batch_action_count} ä¸ªactionsï¼ˆå¸§ {start_frame_hint}-{end_frame_hint}ï¼‰"
    ))

    # å‘é€RAGæ£€ç´¢äº‹ä»¶
    _emit_progress(
        ProgressEventType.RAG_SEARCHING,
        f"æ£€ç´¢ç›¸å…³Actionå®šä¹‰...",
        state
    )

    # RAGæ£€ç´¢ç›¸å…³Actionsï¼ˆå¢å¼ºç‰ˆï¼šç»“åˆè¯­ä¹‰ä¸Šä¸‹æ–‡ç²¾å‡†æ£€ç´¢ï¼‰
    track_type = infer_track_type(track_name)
    suggested_types = batch_context_state.get("suggested_types", [])
    used_types = batch_context_state.get("used_action_types", [])

    relevant_actions = search_actions_by_track_type(
        track_type=track_type,
        purpose=purpose,
        top_k=6,
        suggested_types=suggested_types,
        used_types=used_types,
        batch_context=batch_context_desc
    )

    # å‘é€RAGæ£€ç´¢å®Œæˆäº‹ä»¶
    _emit_progress(
        ProgressEventType.RAG_COMPLETED,
        f"æ£€ç´¢åˆ° {len(relevant_actions)} ä¸ªç›¸å…³Actionå®šä¹‰",
        state,
        data={"action_count": len(relevant_actions)}
    )

    # RAG æ£€ç´¢å®¹é”™ï¼šæ— ç»“æœæ—¶ä½¿ç”¨é»˜è®¤æ¨¡æ¿ï¼ˆä¸ progressive_skill_nodes ä¿æŒä¸€è‡´ï¼‰
    if not relevant_actions:
        from .progressive_skill_nodes import get_default_actions_for_track_type
        logger.warning(f"âš ï¸ RAG æ£€ç´¢æ— ç»“æœï¼Œä½¿ç”¨ {track_type} ç±»å‹é»˜è®¤æ¨¡æ¿")
        relevant_actions = get_default_actions_for_track_type(track_type)
        messages.append(AIMessage(
            content=f"âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³ Actionï¼Œä½¿ç”¨ {track_type} ç±»å‹é»˜è®¤æ¨¡æ¿ç”Ÿæˆ"
        ))
    else:
        messages.append(AIMessage(
            content=f"ğŸ“‹ æ£€ç´¢åˆ° {len(relevant_actions)} ä¸ªç›¸å…³Actionå®šä¹‰"
        ))

    # æ ¼å¼åŒ–promptè¾“å…¥ï¼ˆä½¿ç”¨å¢å¼ºçš„ä¸Šä¸‹æ–‡æ ¼å¼åŒ–ï¼‰
    action_schemas_text = format_action_schemas_for_prompt(relevant_actions)

    # ä½¿ç”¨æ–°çš„ä¸Šä¸‹æ–‡æ ¼å¼åŒ–å‡½æ•°ï¼ˆæ›¿ä»£åŸæœ‰çš„ç®€å•æ‘˜è¦ï¼‰
    if batch_context_state:
        context_text = format_context_for_prompt(batch_context_state)
    else:
        # å›é€€åˆ°æ—§çš„æ‘˜è¦æ–¹å¼
        context_text = format_previous_actions_summary(accumulated_actions)

    # è·å–Prompt
    prompt_mgr = get_prompt_manager()
    prompt = prompt_mgr.get_prompt("batch_action_generation")

    # å‘é€LLMè°ƒç”¨äº‹ä»¶
    _emit_progress(
        ProgressEventType.LLM_CALLING,
        f"è°ƒç”¨LLMç”ŸæˆActions...",
        state
    )

    llm_start_time = time.time()
    logger.info(f"â³ å¼€å§‹è°ƒç”¨ DeepSeek APIï¼ˆLangChain streamingï¼‰(batch {current_batch_idx + 1}/{len(batch_plan)})...")

    try:
        # ğŸ”¥ ä½¿ç”¨ LangChain LLMï¼ˆstreaming=Trueï¼‰
        # LangGraph Studio é€šè¿‡ stream_mode="messages" è‡ªåŠ¨æ•è· token æµ
        llm = get_llm(streaming=True)
        
        # åˆ›å»º chain
        chain = prompt | llm
        
        # è°ƒç”¨ LLMï¼ˆLangGraph ä¼šè‡ªåŠ¨è¿½è¸ªè¿™ä¸ªè°ƒç”¨å¹¶æµå¼è¾“å‡º tokenï¼‰
        response = chain.invoke({
            "skill_name": skeleton.get("skillName", "Unknown"),
            "total_duration": skeleton.get("totalDuration", 150),
            "track_name": track_name,
            "track_purpose": purpose,
            "batch_action_count": batch_action_count,
            "start_frame_hint": start_frame_hint,
            "end_frame_hint": end_frame_hint,
            "batch_context": batch_context_desc,
            "current_batch_index": current_batch_idx,
            "previous_actions_summary": context_text,
            "relevant_actions": action_schemas_text or "æ— ç‰¹å®šActionå‚è€ƒ"
        })

        llm_elapsed = time.time() - llm_start_time
        logger.info(f"â±ï¸ DeepSeek API å“åº”è€—æ—¶: {llm_elapsed:.2f}s")

        # æå–å“åº”å†…å®¹
        full_content = _prepare_payload_text(response)
        logger.info(f"ğŸ“ LLM å“åº”é•¿åº¦: {len(full_content)} å­—ç¬¦")

        # è§£æ JSON å“åº”
        json_content = extract_json_from_markdown(full_content)
        batch_dict = json.loads(json_content)

        # ä½¿ç”¨ Pydantic éªŒè¯
        validated = ActionBatch.model_validate(batch_dict)
        batch_actions = [action.model_dump() for action in validated.actions]
        logger.info(f"âœ… æ‰¹æ¬¡ç”ŸæˆæˆåŠŸï¼ˆæµå¼ï¼‰: {len(batch_actions)} actions")

        # å‘é€LLMå®Œæˆäº‹ä»¶
        _emit_progress(
            ProgressEventType.LLM_COMPLETED,
            f"ç”Ÿæˆ {len(batch_actions)} ä¸ªActions",
            state,
            data={"action_count": len(batch_actions)}
        )

        messages.append(AIMessage(
            content=f"âœ… æ‰¹æ¬¡ç”Ÿæˆå®Œæˆ: {len(batch_actions)} ä¸ªactions"
        ))

        return {
            "current_batch_actions": batch_actions,
            "messages": messages
        }

    except ValidationError as e:
        logger.error(f"âŒ æ‰¹æ¬¡SchemaéªŒè¯å¤±è´¥: {e}")
        if full_content:
            logger.error(f"åŸå§‹LLMè¾“å‡º: {full_content[:500]}...")

        error_details = "\n".join([f"â€¢ {err['loc']}: {err['msg']}" for err in e.errors()])
        messages.append(AIMessage(
            content=f"âŒ æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥ï¼ˆæ ¼å¼é”™è¯¯ï¼‰:\n{error_details}\n"
                    f"æç¤º: æ¯ä¸ªactionå¿…é¡»åŒ…å«frame, duration, enabled, parameterså››ä¸ªå­—æ®µ"
        ))

        # å‘é€é”™è¯¯äº‹ä»¶
        _emit_progress(
            ProgressEventType.BATCH_FAILED,
            f"æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥: SchemaéªŒè¯é”™è¯¯",
            state,
            data={"error": str(e)[:100]}
        )

        # è¿”å›ç©ºåˆ—è¡¨è§¦å‘ä¿®å¤æµç¨‹
        return {
            "current_batch_actions": [],
            "current_batch_errors": [f"SchemaéªŒè¯å¤±è´¥: {error_details}"],
            "messages": messages
        }

    except Exception as e:
        logger.error(f"âŒ æ‰¹æ¬¡ç”Ÿæˆå¼‚å¸¸: {e}", exc_info=True)
        messages.append(AIMessage(content=f"âŒ æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥: {str(e)}"))

        # å‘é€é”™è¯¯äº‹ä»¶
        _emit_progress(
            ProgressEventType.BATCH_FAILED,
            f"æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥: {str(e)[:50]}",
            state,
            data={"error": str(e)[:100]}
        )

        return {
            "current_batch_actions": [],
            "current_batch_errors": [f"ç”Ÿæˆå¼‚å¸¸: {str(e)}"],
            "messages": messages
        }


# ==================== é˜¶æ®µ3: æ‰¹æ¬¡éªŒè¯å’Œä¿®å¤èŠ‚ç‚¹ ====================

def validate_batch_actions(
    batch_actions: List[Dict[str, Any]],
    batch_plan_item: Dict[str, Any],
    total_duration: int
) -> List[str]:
    """
    éªŒè¯æ‰¹æ¬¡actionsçš„åˆæ³•æ€§

    éªŒè¯è§„åˆ™:
    1. actionsæ•°é‡åœ¨åˆç†èŒƒå›´å†…
    2. æ¯ä¸ªactionçš„frameåœ¨å»ºè®®èŒƒå›´å†…ï¼ˆå®½æ¾æ£€æŸ¥ï¼‰
    3. frame + duration <= total_duration
    4. parametersåŒ…å«_odin_type

    Args:
        batch_actions: æ‰¹æ¬¡actionsåˆ—è¡¨
        batch_plan_item: æ‰¹æ¬¡è®¡åˆ’é¡¹
        total_duration: æŠ€èƒ½æ€»æ—¶é•¿

    Returns:
        é”™è¯¯åˆ—è¡¨
    """
    errors = []

    if not batch_actions:
        errors.append("æ‰¹æ¬¡actionsä¸ºç©º")
        return errors

    expected_count = batch_plan_item["action_count"]
    actual_count = len(batch_actions)

    # å®½æ¾æ£€æŸ¥æ•°é‡ï¼ˆå…è®¸Â±2ä¸ªï¼‰
    if abs(actual_count - expected_count) > 2:
        errors.append(
            f"æ‰¹æ¬¡actionæ•°é‡å¼‚å¸¸: æœŸæœ›{expected_count}ä¸ª, å®é™…{actual_count}ä¸ª"
        )

    start_hint = batch_plan_item["start_frame_hint"]
    end_hint = batch_plan_item["end_frame_hint"]

    for idx, action in enumerate(batch_actions):
        frame = action.get("frame")
        duration = action.get("duration")

        if not isinstance(frame, int) or frame < 0:
            errors.append(f"action[{idx}].frame æ— æ•ˆ: {frame}")
            continue

        if not isinstance(duration, int) or duration < 1:
            errors.append(f"action[{idx}].duration æ— æ•ˆ: {duration}")
            continue

        # æ£€æŸ¥æ˜¯å¦è¶…å‡ºæŠ€èƒ½æ€»æ—¶é•¿
        if frame + duration > total_duration:
            errors.append(
                f"action[{idx}] ç»“æŸå¸§({frame + duration}) è¶…å‡ºæ€»æ—¶é•¿({total_duration})"
            )

        # å®½æ¾æ£€æŸ¥å¸§èŒƒå›´ï¼ˆå…è®¸Â±30å¸§çš„åå·®ï¼‰
        if frame < start_hint - 30 or frame > end_hint + 30:
            logger.warning(
                f"âš ï¸ action[{idx}].frame({frame}) ä¸åœ¨å»ºè®®èŒƒå›´({start_hint}-{end_hint}), ä½†å¯æ¥å—"
            )

        # æ£€æŸ¥parameters
        parameters = action.get("parameters")
        if not parameters or not isinstance(parameters, dict):
            errors.append(f"action[{idx}].parameters ç¼ºå¤±")
        elif "_odin_type" not in parameters:
            errors.append(f"action[{idx}].parameters ç¼ºå°‘ _odin_type")

    return errors


def batch_action_validator_node(state: ActionBatchProgressiveState) -> Dict[str, Any]:
    """
    æ‰¹æ¬¡ActionéªŒè¯èŠ‚ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼šæ”¯æŒå‚æ•°æ·±åº¦éªŒè¯ + æµå¼è¾“å‡ºï¼‰

    èŒè´£:
    1. éªŒè¯å½“å‰æ‰¹æ¬¡actionsçš„åŸºç¡€åˆæ³•æ€§ï¼ˆframe/duration/parametersï¼‰
    2. å¯¹ç…§RAGæ£€ç´¢çš„Action Schemaè¿›è¡Œå‚æ•°æ·±åº¦éªŒè¯ï¼ˆç±»å‹/æšä¸¾/èŒƒå›´ï¼‰
    3. å‘é€éªŒè¯è¿›åº¦äº‹ä»¶

    è¾“å‡º:
    - current_batch_errors: é”™è¯¯åˆ—è¡¨
    """
    batch_actions = state.get("current_batch_actions", [])
    batch_plan = state["current_track_batch_plan"]
    current_batch_idx = state["current_batch_index"]
    total_duration = state["skill_skeleton"].get("totalDuration", 150)
    track_plan = state.get("track_plan", [])
    current_track_idx = state.get("current_track_index", 0)

    current_batch_plan = batch_plan[current_batch_idx]

    logger.info("ğŸ” éªŒè¯æ‰¹æ¬¡actionsï¼ˆå«å‚æ•°æ·±åº¦éªŒè¯ï¼‰...")

    # å‘é€éªŒè¯å¼€å§‹äº‹ä»¶
    _emit_progress(
        ProgressEventType.BATCH_VALIDATING,
        f"éªŒè¯æ‰¹æ¬¡ {current_batch_idx + 1}/{len(batch_plan)}...",
        state
    )

    # åŸºç¡€ç»“æ„éªŒè¯
    errors = validate_batch_actions(
        batch_actions=batch_actions,
        batch_plan_item=current_batch_plan,
        total_duration=total_duration
    )

    # å‚æ•°æ·±åº¦éªŒè¯ï¼ˆè·å–ç›¸å…³Action Schemaï¼‰- ä»…åœ¨æ¨¡å—å¯ç”¨æ—¶æ‰§è¡Œ
    warnings = []
    if batch_actions and not errors and HAS_DEEP_VALIDATOR:
        # è·å–å½“å‰Trackçš„purposeç”¨äºæ£€ç´¢
        track_purpose = ""
        if current_track_idx < len(track_plan):
            track_purpose = track_plan[current_track_idx].get("purpose", "")

        # æ£€ç´¢ç›¸å…³Action Schema
        relevant_schemas = _get_relevant_action_schemas_for_validation(
            batch_actions, track_purpose
        )

        if relevant_schemas and validate_batch_actions_deep is not None:
            # æ‰§è¡Œå‚æ•°æ·±åº¦éªŒè¯
            deep_errors, deep_warnings = validate_batch_actions_deep(
                batch_actions=batch_actions,
                relevant_action_schemas=relevant_schemas,
                total_duration=total_duration
            )
            errors.extend(deep_errors)
            warnings.extend(deep_warnings)
            logger.info(f"ğŸ“‹ å‚æ•°æ·±åº¦éªŒè¯å®Œæˆ: {len(deep_errors)} é”™è¯¯, {len(deep_warnings)} è­¦å‘Š")
    elif not HAS_DEEP_VALIDATOR:
        logger.debug("âš ï¸ å‚æ•°æ·±åº¦éªŒè¯æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡æ·±åº¦éªŒè¯")

    messages = []
    if errors:
        logger.warning(f"âš ï¸ æ‰¹æ¬¡éªŒè¯å‘ç° {len(errors)} ä¸ªé”™è¯¯")
        errors_list = "\n".join([f"â€¢ {err}" for err in errors[:10]])  # é™åˆ¶æ˜¾ç¤ºå‰10ä¸ª
        messages.append(AIMessage(
            content=f"âš ï¸ æ‰¹æ¬¡éªŒè¯å¤±è´¥ ({len(errors)} ä¸ªé”™è¯¯):\n{errors_list}"
        ))
    else:
        logger.info("âœ… æ‰¹æ¬¡éªŒè¯é€šè¿‡")
        msg = "âœ… æ‰¹æ¬¡éªŒè¯é€šè¿‡"
        if warnings:
            msg += f"\nâš ï¸ {len(warnings)} ä¸ªè­¦å‘Š:\n" + "\n".join([f"â€¢ {w}" for w in warnings[:5]])
        messages.append(AIMessage(content=msg))

    return {
        "current_batch_errors": errors,
        "messages": messages
    }


# RAGæ£€ç´¢ç»“æœç¼“å­˜ï¼ˆä½¿ç”¨lru_cacheéœ€è¦hashableå‚æ•°ï¼Œæ‰€ä»¥å°è£…ä¸€å±‚ï¼‰
_action_schema_cache: Dict[str, List[Dict[str, Any]]] = {}


def _cached_search_actions(type_name: str, top_k: int = 3) -> List[Dict[str, Any]]:
    """
    å¸¦ç¼“å­˜çš„Action Schemaæ£€ç´¢

    Args:
        type_name: Actionç±»å‹å
        top_k: è¿”å›æ•°é‡

    Returns:
        æ£€ç´¢ç»“æœåˆ—è¡¨
    """
    cache_key = f"{type_name}:{top_k}"

    if cache_key in _action_schema_cache:
        return _action_schema_cache[cache_key]

    from ..tools.rag_tools import search_actions

    try:
        results = search_actions.invoke({"query": type_name, "top_k": top_k})
        if isinstance(results, list):
            _action_schema_cache[cache_key] = results
            return results
    except Exception as e:
        logger.warning(f"âš ï¸ æ£€ç´¢Action Schemaå¤±è´¥ ({type_name}): {e}")

    return []


def clear_action_schema_cache():
    """æ¸…é™¤Action Schemaç¼“å­˜ï¼ˆåœ¨æ–°ä»»åŠ¡å¼€å§‹æ—¶è°ƒç”¨ï¼‰"""
    global _action_schema_cache
    _action_schema_cache = {}
    logger.debug("å·²æ¸…é™¤Action Schemaç¼“å­˜")


def _get_relevant_action_schemas_for_validation(
    batch_actions: List[Dict[str, Any]],
    track_purpose: str
) -> List[Dict[str, Any]]:
    """
    è·å–æ‰¹æ¬¡ä¸­actionså¯¹åº”çš„Schemaå®šä¹‰ï¼ˆå¸¦ç¼“å­˜ï¼‰

    Args:
        batch_actions: æ‰¹æ¬¡actionsåˆ—è¡¨
        track_purpose: Trackç”¨é€”ï¼ˆç”¨äºæ£€ç´¢ï¼‰

    Returns:
        Action Schemaåˆ—è¡¨
    """
    schemas = []

    # æ”¶é›†æ‰€æœ‰actionç±»å‹
    action_types = set()
    for action in batch_actions:
        params = action.get("parameters", {})
        odin_type = params.get("_odin_type", "")
        if odin_type:
            # æå–ç±»å‹å
            type_name = extract_action_type_name(odin_type)
            if type_name:
                action_types.add(type_name)

    # ä¸ºæ¯ç§ç±»å‹æ£€ç´¢Schemaï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    for type_name in action_types:
        results = _cached_search_actions(type_name, top_k=3)
        for result in results:
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…
            result_type = result.get("typeName", "")
            if result_type == type_name or type_name in result_type:
                schemas.append(result)
                break

    return schemas


def batch_action_fixer_node(state: ActionBatchProgressiveState) -> Dict[str, Any]:
    """
    æ‰¹æ¬¡Actionä¿®å¤èŠ‚ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼šæµå¼è¾“å‡ºï¼‰

    èŒè´£: æ ¹æ®éªŒè¯é”™è¯¯ä¿®å¤æ‰¹æ¬¡actions

    è¾“å‡º:
    - current_batch_actions: ä¿®å¤åçš„actions
    - batch_retry_count: +1
    """
    from ..prompts.prompt_manager import get_prompt_manager
    from .json_utils import extract_json_from_markdown

    batch_actions = state["current_batch_actions"]
    errors = state["current_batch_errors"]
    batch_plan = state["current_track_batch_plan"]
    current_batch_idx = state["current_batch_index"]
    total_duration = state["skill_skeleton"].get("totalDuration", 150)

    current_batch_plan = batch_plan[current_batch_idx]

    logger.info(f"ğŸ”§ ä¿®å¤æ‰¹æ¬¡actions, é”™è¯¯æ•°: {len(errors)}")

    # å‘é€ä¿®å¤å¼€å§‹äº‹ä»¶
    _emit_progress(
        ProgressEventType.BATCH_FIXING,
        f"ä¿®å¤æ‰¹æ¬¡ {current_batch_idx + 1}/{len(batch_plan)} ({len(errors)} ä¸ªé”™è¯¯)",
        state,
        data={"error_count": len(errors)}
    )

    # æ ¼å¼åŒ–é”™è¯¯
    errors_text = "\n".join([f"{i+1}. {err}" for i, err in enumerate(errors)])

    messages = []
    messages.append(AIMessage(
        content=f"ğŸ”§ å‘ç° {len(errors)} ä¸ªé”™è¯¯,æ­£åœ¨ä¿®å¤..."
    ))

    # è·å–Prompt
    prompt_mgr = get_prompt_manager()
    prompt = prompt_mgr.get_prompt("batch_action_fix")

    llm = get_llm(temperature=0.3)

    try:
        fixer_llm = llm.with_structured_output(
            ActionBatch,
            method="json_mode",
            include_raw=False
        )
    except:
        fixer_llm = llm

    chain = prompt | fixer_llm

    try:
        response = chain.invoke({
            "errors": errors_text,
            "batch_actions_json": json.dumps(batch_actions, ensure_ascii=False, indent=2),
            "batch_index": current_batch_idx,  # æ·»åŠ æ‰¹æ¬¡ç´¢å¼•å‚æ•°
            "total_duration": total_duration,
            "start_frame_hint": current_batch_plan["start_frame_hint"],
            "end_frame_hint": current_batch_plan["end_frame_hint"]
        })

        if isinstance(response, ActionBatch):
            fixed_actions = [action.model_dump() for action in response.actions]
        else:
            payload_text = _prepare_payload_text(response)
            json_content = extract_json_from_markdown(payload_text)
            batch_dict = json.loads(json_content)
            validated = ActionBatch.model_validate(batch_dict)
            fixed_actions = [action.model_dump() for action in validated.actions]

        logger.info("âœ… æ‰¹æ¬¡ä¿®å¤æˆåŠŸ")
        messages.append(AIMessage(content="âœ… æ‰¹æ¬¡å·²ä¿®å¤,é‡æ–°éªŒè¯..."))

        return {
            "current_batch_actions": fixed_actions,
            "batch_retry_count": state.get("batch_retry_count", 0) + 1,
            "messages": messages
        }

    except Exception as e:
        logger.error(f"âŒ æ‰¹æ¬¡ä¿®å¤å¤±è´¥: {e}", exc_info=True)
        messages.append(AIMessage(content=f"âŒ æ‰¹æ¬¡ä¿®å¤å¤±è´¥: {str(e)}"))

        return {
            "batch_retry_count": state.get("batch_retry_count", 0) + 1,
            "messages": messages
        }


def batch_action_saver_node(state: ActionBatchProgressiveState) -> Dict[str, Any]:
    """
    æ‰¹æ¬¡Actionä¿å­˜èŠ‚ç‚¹ï¼ˆå¢å¼ºç‰ˆï¼šæ›´æ–°è¯­ä¹‰ä¸Šä¸‹æ–‡ + æµå¼è¾“å‡ºï¼‰

    èŒè´£:
    1. ä¿å­˜éªŒè¯é€šè¿‡çš„æ‰¹æ¬¡actions
    2. æ›´æ–°BatchContextStateï¼ˆå·²ç”Ÿæˆæ‘˜è¦ã€å·²ç”¨ç±»å‹ã€å ç”¨å¸§åŒºé—´ï¼‰
    3. æ‰§è¡Œè¯­ä¹‰éªŒè¯å¹¶è®°å½•è­¦å‘Š
    4. ç§»åŠ¨åˆ°ä¸‹ä¸€æ‰¹æ¬¡
    5. å‘é€è¿›åº¦äº‹ä»¶

    è¾“å‡º:
    - accumulated_track_actions: è¿½åŠ å½“å‰æ‰¹æ¬¡actions
    - current_batch_index: +1
    - batch_retry_count: é‡ç½®ä¸º0
    - batch_context: æ›´æ–°åçš„ä¸Šä¸‹æ–‡
    """
    batch_actions = state.get("current_batch_actions", [])
    accumulated = list(state.get("accumulated_track_actions", []))
    current_batch_idx = state.get("current_batch_index", 0)
    batch_plan = state["current_track_batch_plan"]
    batch_context = state.get("batch_context", {})

    # å¤„ç†ç©ºæ‰¹æ¬¡ï¼ˆè·³è¿‡åœºæ™¯ï¼‰
    if not batch_actions:
        logger.warning(f"âš ï¸ æ‰¹æ¬¡ [{current_batch_idx + 1}/{len(batch_plan)}] ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
        return {
            "accumulated_track_actions": accumulated,  # ä¿æŒä¸å˜
            "current_batch_index": current_batch_idx + 1,
            "batch_retry_count": 0,
            "batch_context": batch_context,  # ä¸æ›´æ–°ä¸Šä¸‹æ–‡
            "messages": [AIMessage(
                content=f"âš ï¸ æ‰¹æ¬¡ [{current_batch_idx + 1}/{len(batch_plan)}] è·³è¿‡ï¼ˆç”Ÿæˆå¤±è´¥æˆ–ä¸ºç©ºï¼‰"
            )]
        }

    logger.info(f"ğŸ’¾ ä¿å­˜æ‰¹æ¬¡ [{current_batch_idx + 1}/{len(batch_plan)}]: {len(batch_actions)} actions")

    # è¿½åŠ åˆ°ç´¯ç§¯åˆ—è¡¨
    accumulated.extend(batch_actions)

    messages = []
    progress = f"[{current_batch_idx + 1}/{len(batch_plan)}]"

    # è·å–å½“å‰Trackç±»å‹ï¼ˆç”¨äºè¯­ä¹‰éªŒè¯ï¼‰
    track_plan_list = state.get("track_plan", [])
    current_track_idx = state.get("current_track_index", 0)
    track_type = None
    if current_track_idx < len(track_plan_list):
        track_name = track_plan_list[current_track_idx].get("trackName", "")
        from .progressive_skill_nodes import infer_track_type
        track_type = infer_track_type(track_name)

    # æ‰§è¡Œè¯­ä¹‰éªŒè¯ï¼ˆå¢å¼ºç‰ˆï¼šæ·»åŠ track_typeå‚æ•°ï¼‰
    if batch_context:
        violations = validate_semantic_rules(batch_actions, batch_context, track_type=track_type)
        if violations:
            logger.warning(f"âš ï¸ è¯­ä¹‰éªŒè¯å‘ç° {len(violations)} ä¸ªé—®é¢˜")
            violations_text = "\n".join([f"  â€¢ {v}" for v in violations[:3]])
            messages.append(AIMessage(
                content=f"âš ï¸ è¯­ä¹‰éªŒè¯æç¤º:\n{violations_text}"
            ))
            # å°†violationsæ·»åŠ åˆ°ä¸Šä¸‹æ–‡çš„avoid_patterns
            avoid = list(batch_context.get("avoid_patterns", []))
            for v in violations:
                if "[è­¦å‘Š]" in v or "[å»ºè®®]" in v:
                    avoid.append(v.split("]", 1)[1].strip()[:50])
            batch_context["avoid_patterns"] = avoid[-5:]  # åªä¿ç•™æœ€è¿‘5ä¸ª

    # æ›´æ–°ä¸Šä¸‹æ–‡ï¼ˆæ–°å¢ï¼‰
    next_batch_idx = current_batch_idx + 1
    if batch_context:
        updated_context = update_context_after_batch(
            context=batch_context,
            batch_actions=batch_actions,
            batch_plan=batch_plan,
            next_batch_idx=next_batch_idx
        )
    else:
        updated_context = {}

    # ç”Ÿæˆä¿å­˜æ¶ˆæ¯
    action_types = []
    for action in batch_actions:
        t = extract_action_type_name(action.get("parameters", {}).get("_odin_type", ""))
        if t and t not in action_types:
            action_types.append(t)

    type_info = f" ({', '.join(action_types[:3])})" if action_types else ""

    # å‘é€æ‰¹æ¬¡å®Œæˆäº‹ä»¶
    _emit_progress(
        ProgressEventType.BATCH_COMPLETED,
        f"æ‰¹æ¬¡ {progress} å·²ä¿å­˜: {len(batch_actions)} actions{type_info}",
        state,
        data={
            "action_count": len(batch_actions),
            "action_types": action_types[:3],
            "accumulated_total": len(accumulated)
        }
    )

    messages.append(AIMessage(
        content=f"ğŸ’¾ æ‰¹æ¬¡ {progress} å·²ä¿å­˜ ({len(batch_actions)} actions{type_info})"
    ))

    return {
        "accumulated_track_actions": accumulated,
        "current_batch_index": next_batch_idx,
        "batch_retry_count": 0,
        "batch_context": updated_context,
        "messages": messages
    }


# ==================== é˜¶æ®µ4: Trackç»„è£…èŠ‚ç‚¹ ====================

def track_assembler_node_batch(state: ActionBatchProgressiveState) -> Dict[str, Any]:
    """
    Trackç»„è£…èŠ‚ç‚¹ï¼ˆæ‰¹æ¬¡çº§ç‰ˆæœ¬ + æµå¼è¾“å‡ºï¼‰

    èŒè´£:
    1. å°†accumulated_track_actionsç»„è£…ä¸ºå®Œæ•´Track
    2. éªŒè¯Trackæ•´ä½“çš„æ—¶é—´è½´è¿è´¯æ€§
    3. æ·»åŠ åˆ°generated_tracks
    4. å‘é€è¿›åº¦äº‹ä»¶

    è¾“å‡º:
    - generated_tracks: è¿½åŠ å½“å‰Track
    - current_track_index: +1
    - accumulated_track_actions: æ¸…ç©º
    """
    from .progressive_skill_nodes import validate_track

    skeleton = state["skill_skeleton"]
    track_plan = state["track_plan"]
    current_track_idx = state["current_track_index"]
    accumulated_actions = state.get("accumulated_track_actions", [])
    generated_tracks = list(state.get("generated_tracks", []))

    current_track = track_plan[current_track_idx]
    track_name = current_track.get("trackName", "Unknown Track")
    total_duration = skeleton.get("totalDuration", 150)

    logger.info(
        f"ğŸ”§ ç»„è£… Track '{track_name}': {len(accumulated_actions)} actions"
    )

    # å‘é€Trackç»„è£…äº‹ä»¶
    _emit_progress(
        ProgressEventType.ASSEMBLING_TRACK,
        f"ç»„è£… Track: {track_name}",
        state,
        data={"track_name": track_name, "action_count": len(accumulated_actions)}
    )

    # ç»„è£…Track
    track_data = {
        "trackName": track_name,
        "enabled": True,
        "actions": accumulated_actions
    }

    # éªŒè¯Trackæ•´ä½“
    errors = validate_track(track_data, total_duration)

    messages = []
    if errors:
        logger.warning(f"âš ï¸ Trackç»„è£…åéªŒè¯å‘ç° {len(errors)} ä¸ªé—®é¢˜")
        errors_list = "\n".join([f"â€¢ {err}" for err in errors])
        messages.append(AIMessage(
            content=f"âš ï¸ Trackç»„è£…éªŒè¯å‘ç°é—®é¢˜:\n{errors_list}\nç»§ç»­ä¿å­˜..."
        ))

    # ä¿å­˜Track
    generated_tracks.append(track_data)

    progress = f"[{len(generated_tracks)}/{len(track_plan)}]"

    # å‘é€Trackå®Œæˆäº‹ä»¶
    _emit_progress(
        ProgressEventType.TRACK_COMPLETED,
        f"Track '{track_name}' ç»„è£…å®Œæˆ {progress}",
        state,
        data={
            "track_name": track_name,
            "action_count": len(accumulated_actions),
            "completed_tracks": len(generated_tracks),
            "total_tracks": len(track_plan)
        }
    )

    messages.append(AIMessage(
        content=f"âœ… Track '{track_name}' ç»„è£…å®Œæˆ {progress}"
    ))

    return {
        "generated_tracks": generated_tracks,
        "current_track_index": current_track_idx + 1,
        "accumulated_track_actions": [],  # æ¸…ç©º,å‡†å¤‡ä¸‹ä¸€ä¸ªTrack
        "messages": messages
    }


# ==================== æ¡ä»¶åˆ¤æ–­å‡½æ•° ====================

def should_fix_batch(state: ActionBatchProgressiveState) -> Literal["save", "fix", "skip"]:
    """
    åˆ¤æ–­æ‰¹æ¬¡æ˜¯å¦éœ€è¦ä¿®å¤

    è¿”å›:
    - "save": æ— é”™è¯¯ â†’ ä¿å­˜æ‰¹æ¬¡
    - "fix": æœ‰é”™è¯¯ä¸”æœªè¾¾é‡è¯•ä¸Šé™ â†’ ä¿®å¤æ‰¹æ¬¡
    - "skip": æœ‰é”™è¯¯ä½†è¾¾é‡è¯•ä¸Šé™ â†’ è·³è¿‡æ‰¹æ¬¡
    """
    errors = state.get("current_batch_errors", [])
    retry_count = state.get("batch_retry_count", 0)
    max_retries = state.get("max_batch_retries", 2)

    if not errors:
        return "save"

    if retry_count < max_retries:
        return "fix"
    else:
        logger.warning(f"æ‰¹æ¬¡è¾¾åˆ°é‡è¯•ä¸Šé™({max_retries}),è·³è¿‡")
        return "skip"


def should_continue_batches(state: ActionBatchProgressiveState) -> Literal["continue", "assemble_track"]:
    """
    åˆ¤æ–­æ˜¯å¦ç»§ç»­ç”Ÿæˆä¸‹ä¸€æ‰¹æ¬¡

    è¿”å›:
    - "continue": è¿˜æœ‰æ‰¹æ¬¡ â†’ ç”Ÿæˆä¸‹ä¸€æ‰¹æ¬¡
    - "assemble_track": æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ â†’ ç»„è£…Track
    """
    current_batch_idx = state.get("current_batch_index", 0)
    batch_plan = state.get("current_track_batch_plan", [])

    if current_batch_idx < len(batch_plan):
        return "continue"
    else:
        return "assemble_track"


def should_continue_tracks_batch(state: ActionBatchProgressiveState) -> Literal["continue", "assemble_skill"]:
    """
    åˆ¤æ–­æ˜¯å¦ç»§ç»­ç”Ÿæˆä¸‹ä¸€ä¸ªTrack

    è¿”å›:
    - "continue": è¿˜æœ‰Track â†’ è§„åˆ’ä¸‹ä¸€Trackçš„æ‰¹æ¬¡
    - "assemble_skill": æ‰€æœ‰Trackå®Œæˆ â†’ ç»„è£…æŠ€èƒ½
    """
    current_track_idx = state.get("current_track_index", 0)
    track_plan = state.get("track_plan", [])

    if current_track_idx < len(track_plan):
        return "continue"
    else:
        return "assemble_skill"
