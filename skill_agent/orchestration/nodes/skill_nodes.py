"""
LangGraph èŠ‚ç‚¹å®ç°
å®šä¹‰ Graph ä¸­çš„å„ä¸ªèŠ‚ç‚¹ï¼ˆgeneratorã€validatorã€fixer ç­‰ï¼‰
"""

import json
import logging
import time
from typing import Any, Dict, List, TypedDict, Annotated
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

from .json_utils import extract_json_from_markdown

logger = logging.getLogger(__name__)


# ==================== State å®šä¹‰ ====================

class SkillGenerationState(TypedDict):
    """æŠ€èƒ½ç”Ÿæˆæµç¨‹çš„çŠ¶æ€"""
    requirement: str  # ç”¨æˆ·éœ€æ±‚æè¿°
    similar_skills: List[Dict[str, Any]]  # æ£€ç´¢åˆ°çš„ç›¸ä¼¼æŠ€èƒ½
    action_schemas: List[Dict[str, Any]]  # ğŸ”¥ æ£€ç´¢åˆ°çš„Actionå®šä¹‰schema
    generated_json: str  # ç”Ÿæˆçš„ JSON
    validation_errors: List[str]  # éªŒè¯é”™è¯¯åˆ—è¡¨
    retry_count: int  # é‡è¯•æ¬¡æ•°
    max_retries: int  # æœ€å¤§é‡è¯•æ¬¡æ•°
    final_result: Dict[str, Any]  # æœ€ç»ˆç»“æœ
    messages: Annotated[List, "append"]  # å¯¹è¯å†å²
    thread_id: str  # ğŸ”¥ çº¿ç¨‹ID (ç”¨äºæµå¼è¾“å‡º)


# ==================== LLM åˆå§‹åŒ– ====================

def get_llm(model: str = "deepseek-reasoner", temperature: float = 1.0):
    """
    è·å– LLM å®ä¾‹ï¼ˆä½¿ç”¨ LangChain ChatOpenAI å…¼å®¹ DeepSeekï¼‰

    Args:
        model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨ deepseek-reasoner æ€è€ƒæ¨¡å‹ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼ˆdeepseek-reasoner æ¨èä½¿ç”¨ 1.0ï¼‰

    Returns:
        ChatOpenAI å®ä¾‹
    """
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        raise ValueError("ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY æœªè®¾ç½®")

    # ä»ç¯å¢ƒå˜é‡è¯»å–è¶…æ—¶é…ç½®ï¼ˆé»˜è®¤ 120 ç§’ï¼Œå› ä¸º reasoner æ¨¡å‹æ¨ç†æ—¶é—´é•¿ï¼‰
    timeout = int(os.getenv("DEEPSEEK_TIMEOUT", "120"))
    max_retries = int(os.getenv("DEEPSEEK_MAX_RETRIES", "2"))

    logger.info(f"åˆå§‹åŒ– LLM: model={model}, timeout={timeout}s, max_retries={max_retries}")

    return ChatOpenAI(
        model=model,
        temperature=temperature,
        api_key=api_key,
        base_url="https://api.deepseek.com/v1",
        timeout=timeout,  # è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
        max_retries=max_retries,  # æœ€å¤§é‡è¯•æ¬¡æ•°
    )


# ==================== èŠ‚ç‚¹å‡½æ•° ====================

def retriever_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    æ£€ç´¢ç›¸ä¼¼æŠ€èƒ½èŠ‚ç‚¹

    æ ¹æ®éœ€æ±‚æè¿°ï¼Œä» RAG Core æ£€ç´¢ç›¸ä¼¼æŠ€èƒ½å’Œç›¸å…³ Action å®šä¹‰ä½œä¸ºå‚è€ƒã€‚
    """
    from ..tools.rag_tools import search_skills_semantic, search_actions

    requirement = state["requirement"]
    logger.info(f"æ£€ç´¢ç›¸ä¼¼æŠ€èƒ½: {requirement}")

    # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
    messages = []

    # æ·»åŠ å¼€å§‹æ£€ç´¢çš„æ¶ˆæ¯
    messages.append(AIMessage(content=f"ğŸ” æ­£åœ¨ä»æŠ€èƒ½åº“ä¸­æ£€ç´¢ä¸ã€Œ{requirement}ã€ç›¸å…³çš„æŠ€èƒ½å’ŒActionå®šä¹‰..."))

    # ğŸ”¥ P0æ”¹è¿›ï¼šæ·»åŠ é”™è¯¯è¾¹ç•Œ
    try:
        # è°ƒç”¨ RAG å·¥å…·æ£€ç´¢æŠ€èƒ½ï¼ˆæ·»åŠ æ€§èƒ½æ—¥å¿—ï¼‰
        # top_k=2 ä¼˜åŒ–ï¼šå‡å°‘æ£€ç´¢æ•°é‡ä»¥æå‡é€Ÿåº¦ï¼Œ2ä¸ªé«˜è´¨é‡å‚è€ƒå·²è¶³å¤Ÿ
        start_time = time.time()
        results = search_skills_semantic.invoke({"query": requirement, "top_k": 2})
        rag_elapsed = time.time() - start_time
        logger.info(f"â±ï¸ RAG æŠ€èƒ½æ£€ç´¢è€—æ—¶: {rag_elapsed:.2f}s")

        # ğŸ”¥ æ–°å¢ï¼šæ£€ç´¢ç›¸å…³çš„ Action å®šä¹‰
        action_start = time.time()
        action_results = search_actions.invoke({"query": requirement, "top_k": 5})
        action_elapsed = time.time() - action_start
        logger.info(f"â±ï¸ RAG Actionæ£€ç´¢è€—æ—¶: {action_elapsed:.2f}s")
        logger.info(f"ğŸ“‹ æ£€ç´¢åˆ° {len(action_results) if isinstance(action_results, list) else 0} ä¸ªç›¸å…³Action")
    except Exception as e:
        # RAGæ£€ç´¢å¤±è´¥æ—¶è¿”å›ç©ºç»“æœï¼Œå…è®¸ç»§ç»­æ‰§è¡Œ
        logger.error(f"âŒ RAGæ£€ç´¢å¤±è´¥: {e}", exc_info=True)
        results = []
        action_results = []
        messages.append(AIMessage(content=f"âš ï¸ æ£€ç´¢å¤±è´¥ï¼Œå°†ç›´æ¥åŸºäºéœ€æ±‚ç”Ÿæˆ"))

    # æ„å»ºè¯¦ç»†çš„æ£€ç´¢ç»“æœæ¶ˆæ¯
    if results:
        skills_summary = "\n".join([
            f"â€¢ **{skill.get('skill_name', 'Unknown')}** (ç›¸ä¼¼åº¦: {skill.get('similarity', 0):.2%})"
            for skill in results[:3]
        ])
        message = f"ğŸ“š **æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸ä¼¼æŠ€èƒ½ï¼š**\n\n{skills_summary}\n\nè¿™äº›æŠ€èƒ½å°†ä½œä¸ºç”Ÿæˆå‚è€ƒã€‚"
    else:
        message = "âš ï¸ æœªæ£€ç´¢åˆ°ç›¸ä¼¼æŠ€èƒ½ï¼Œå°†åŸºäºéœ€æ±‚ç›´æ¥ç”Ÿæˆã€‚"

    # æ·»åŠ Actionæ£€ç´¢ç»“æœä¿¡æ¯
    if isinstance(action_results, list) and action_results:
        action_summary = "\n".join([
            f"â€¢ **{action.get('action_name', 'Unknown')}** ({action.get('category', 'N/A')})"
            for action in action_results[:3]
        ])
        message += f"\n\nğŸ¯ **æ£€ç´¢åˆ° {len(action_results)} ä¸ªç›¸å…³Actionï¼š**\n\n{action_summary}"

    messages.append(AIMessage(content=message))

    return {
        "similar_skills": results,
        "action_schemas": action_results if isinstance(action_results, list) else [],
        "messages": messages
    }


def generator_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    ç”ŸæˆæŠ€èƒ½ JSON èŠ‚ç‚¹

    æ ¹æ®éœ€æ±‚å’Œå‚è€ƒæŠ€èƒ½ï¼Œä½¿ç”¨ LLM ç”ŸæˆæŠ€èƒ½é…ç½® JSONã€‚

    Args:
        state: æŠ€èƒ½ç”ŸæˆçŠ¶æ€
    """
    from ..prompts.prompt_manager import get_prompt_manager
    from langgraph.config import get_stream_writer

    requirement = state["requirement"]
    similar_skills = state.get("similar_skills", [])
    action_schemas = state.get("action_schemas", [])  # ğŸ”¥ æ–°å¢ï¼šè·å–action schemas

    # ğŸ”¥ ä½¿ç”¨ LangGraph æ ‡å‡†çš„ stream_writer æœºåˆ¶
    try:
        writer = get_stream_writer()
        logger.info(f"âœ… Got stream writer")
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to get stream writer: {e}")
        writer = None

    logger.info(f"ç”ŸæˆæŠ€èƒ½ JSON: {requirement}")

    # æ ¼å¼åŒ–ç›¸ä¼¼æŠ€èƒ½
    similar_skills_text = "\n\n".join([
        f"æŠ€èƒ½ {i+1}: {skill.get('skill_name', 'Unknown')}\n{json.dumps(skill.get('skill_data', {}), indent=2, ensure_ascii=False)}"
        for i, skill in enumerate(similar_skills[:2])  # åªå–å‰2ä¸ª
    ])

    # ğŸ”¥ æ–°å¢ï¼šæ ¼å¼åŒ– Action Schemaï¼ˆåŒ…å«å‚æ•°å®šä¹‰å’Œçº¦æŸï¼‰
    action_schemas_text = ""
    if action_schemas:
        formatted_actions = []
        for action in action_schemas[:5]:  # åªå–å‰5ä¸ªç›¸å…³action
            action_name = action.get('action_name', 'Unknown')
            action_type = action.get('action_type', 'N/A')
            category = action.get('category', 'N/A')
            description = action.get('description', '')[:200]  # é™åˆ¶æè¿°é•¿åº¦
            parameters = action.get('parameters', [])

            # æ ¼å¼åŒ–å‚æ•°ä¿¡æ¯
            params_info = []
            for param in parameters:
                param_name = param.get('name', 'unknown')
                param_type = param.get('type', 'unknown')
                default_val = param.get('defaultValue', '')
                constraints = param.get('constraints', {})
                is_enum = param.get('isEnum', False)
                enum_values = param.get('enumValues', [])

                # æ„å»ºå‚æ•°çº¦æŸæè¿°
                constraint_desc = []
                if constraints.get('min'):
                    constraint_desc.append(f"min={constraints['min']}")
                if constraints.get('max'):
                    constraint_desc.append(f"max={constraints['max']}")
                if constraints.get('minValue'):
                    constraint_desc.append(f"minValue={constraints['minValue']}")
                if constraints.get('maxValue'):
                    constraint_desc.append(f"maxValue={constraints['maxValue']}")

                param_info = f"  - {param_name}: {param_type}"
                if default_val:
                    param_info += f" = {default_val}"
                if is_enum and enum_values:
                    param_info += f" (æšä¸¾å€¼: {', '.join(enum_values)})"
                elif constraint_desc:
                    param_info += f" ({', '.join(constraint_desc)})"

                params_info.append(param_info)

            params_text = "\n".join(params_info) if params_info else "  æ— å‚æ•°"

            formatted_action = f"""Action: {action_name} ({action_type})
åˆ†ç±»: {category}
æè¿°: {description}
å‚æ•°å®šä¹‰:
{params_text}"""
            formatted_actions.append(formatted_action)

        action_schemas_text = "\n\n".join(formatted_actions)
        logger.info(f"ğŸ“‹ å·²æ ¼å¼åŒ– {len(formatted_actions)} ä¸ªAction schemaç”¨äºprompt")

    # è·å– Prompt
    prompt_mgr = get_prompt_manager()
    prompt = prompt_mgr.get_prompt("skill_generation")

    # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
    messages = []

    # æ·»åŠ å¼€å§‹ç”Ÿæˆçš„æ¶ˆæ¯
    messages.append(AIMessage(content="ğŸ¤– æ­£åœ¨è°ƒç”¨ DeepSeek AI ç”ŸæˆæŠ€èƒ½é…ç½®..."))

    # è°ƒç”¨ LLM (ä½¿ç”¨æµå¼è¾“å‡º)
    llm = get_llm()
    chain = prompt | llm

    logger.info(f"â³ æ­£åœ¨è°ƒç”¨ DeepSeek API (æµå¼è¾“å‡º)...")
    api_start_time = time.time()
    first_chunk_time = None

    # æ”¶é›†æµå¼è¾“å‡ºï¼ˆåˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆè¾“å‡ºï¼‰
    full_reasoning = ""  # æ€è€ƒè¿‡ç¨‹
    full_content = ""    # æœ€ç»ˆè¾“å‡º

    # ğŸ”¥ ç”Ÿæˆå”¯ä¸€çš„ message_id ç”¨äºè·Ÿè¸ªæµå¼æ¶ˆæ¯
    thinking_message_id = f"thinking_{api_start_time}"
    content_message_id = f"content_{api_start_time}"

    # ğŸ”¥ P0æ”¹è¿›ï¼šæ·»åŠ é”™è¯¯è¾¹ç•Œ
    try:
        # æµå¼è°ƒç”¨
        for chunk in chain.stream({
            "requirement": requirement,
            "similar_skills": similar_skills_text or "æ— å‚è€ƒæŠ€èƒ½",
            "action_schemas": action_schemas_text or "æ— Actionå‚è€ƒ"  # ğŸ”¥ æ–°å¢ï¼šä¼ é€’action schemas
        }):
            # è®°å½•é¦–å­—èŠ‚æ—¶é—´ï¼ˆTTFBï¼‰
            if first_chunk_time is None:
                first_chunk_time = time.time()
                ttfb = first_chunk_time - api_start_time
                logger.info(f"âš¡ é¦–å­—èŠ‚å»¶è¿Ÿ (TTFB): {ttfb:.2f}s")

            # å°è¯•æå– reasoning_content (DeepSeek Reasoner ç‰¹æœ‰)
            # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ä½ç½®
            reasoning_chunk = None

            # æ–¹æ³•1: æ£€æŸ¥ response_metadata
            if hasattr(chunk, 'response_metadata') and isinstance(chunk.response_metadata, dict):
                reasoning_chunk = chunk.response_metadata.get('reasoning_content')

            # æ–¹æ³•2: æ£€æŸ¥ additional_kwargs
            if not reasoning_chunk and hasattr(chunk, 'additional_kwargs') and isinstance(chunk.additional_kwargs, dict):
                reasoning_chunk = chunk.additional_kwargs.get('reasoning_content')

            # æ–¹æ³•3: ç›´æ¥æ£€æŸ¥å±æ€§
            if not reasoning_chunk and hasattr(chunk, 'reasoning_content'):
                reasoning_chunk = chunk.reasoning_content

            # ç´¯ç§¯æ€è€ƒå†…å®¹
            if reasoning_chunk:
                full_reasoning += reasoning_chunk
                logger.info(f"ğŸ“ Reasoning chunk received: {len(reasoning_chunk)} chars")

                # ğŸ”¥ ä½¿ç”¨ LangGraph æ ‡å‡† writer å®æ—¶æ¨é€ thinking chunk
                if writer:
                    try:
                        writer({
                            "type": "thinking_chunk",
                            "message_id": thinking_message_id,
                            "chunk": reasoning_chunk
                        })
                        logger.info(f"âœ… Sent thinking chunk via writer")
                    except Exception as e:
                        logger.error(f"âŒ Failed to send thinking chunk: {e}")

            # ç´¯ç§¯æœ€ç»ˆå†…å®¹
            if hasattr(chunk, 'content') and chunk.content:
                full_content += chunk.content
                logger.info(f"ğŸ“ Content chunk received: {len(chunk.content)} chars, total: {len(full_content)}")

                # ğŸ”¥ ä½¿ç”¨ LangGraph æ ‡å‡† writer å®æ—¶æ¨é€ content chunk
                if writer:
                    try:
                        writer({
                            "type": "content_chunk",
                            "message_id": content_message_id,
                            "chunk": chunk.content
                        })
                        logger.info(f"âœ… Sent content chunk via writer")
                    except Exception as e:
                        logger.error(f"âŒ Failed to send content chunk: {e}")

        # è®°å½•å®Œæ•´å“åº”å’Œæ€§èƒ½æŒ‡æ ‡
        api_total_time = time.time() - api_start_time
        logger.info(f"âœ… DeepSeek API å“åº”å®Œæˆ")
        logger.info(f"â±ï¸ DeepSeek API æ€»è€—æ—¶: {api_total_time:.2f}s")
        logger.info(f"ğŸ§  æ€è€ƒå†…å®¹é•¿åº¦: {len(full_reasoning)} å­—ç¬¦")
        logger.info(f"ğŸ“ è¾“å‡ºå†…å®¹é•¿åº¦: {len(full_content)} å­—ç¬¦")

        if full_reasoning:
            logger.info(f"ğŸ’­ æ€è€ƒè¿‡ç¨‹é¢„è§ˆ:\n{full_reasoning[:300]}...")
        logger.info(f"ğŸ“„ DeepSeek å®Œæ•´è¾“å‡º:\n{full_content}")

        generated_json = full_content

        # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ï¼Œä½œä¸ºå•ç‹¬çš„æ¶ˆæ¯æ·»åŠ ï¼ˆæ ‡è®°ä¸º thinkingï¼‰
        # ğŸ”¥ ä½¿ç”¨ä¸æµå¼chunkç›¸åŒçš„ message_idï¼Œç¡®ä¿å‰ç«¯å¯ä»¥æ­£ç¡®æ›´æ–°æ¶ˆæ¯
        if full_reasoning:
            messages.append(AIMessage(
                content=full_reasoning,
                additional_kwargs={"thinking": True},
                id=thinking_message_id  # ğŸ”¥ ä½¿ç”¨ç›¸åŒçš„ ID
            ))

        # æ·»åŠ  DeepSeek çš„æœ€ç»ˆè¾“å‡º
        # ğŸ”¥ ä½¿ç”¨ä¸æµå¼chunkç›¸åŒçš„ message_id
        messages.append(AIMessage(
            content=full_content,
            id=content_message_id  # ğŸ”¥ ä½¿ç”¨ç›¸åŒçš„ ID
        ))

    except TimeoutError as e:
        # LLMè°ƒç”¨è¶…æ—¶
        logger.error(f"âŒ DeepSeek API è¶…æ—¶: {e}")
        generated_json = ""
        messages.append(AIMessage(content=f"â±ï¸ ç”Ÿæˆè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•"))
        return {
            "generated_json": generated_json,
            "messages": messages,
            "validation_errors": ["timeout"]  # æ ‡è®°ä¸ºè¶…æ—¶é”™è¯¯
        }
    except Exception as e:
        # å…¶ä»–é”™è¯¯ï¼ˆç½‘ç»œé”™è¯¯ã€APIé”™è¯¯ç­‰ï¼‰
        logger.error(f"âŒ DeepSeek API è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
        generated_json = ""
        messages.append(AIMessage(content=f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"))
        return {
            "generated_json": generated_json,
            "messages": messages,
            "validation_errors": [f"api_error: {str(e)}"]
        }

    return {
        "generated_json": generated_json,
        "messages": messages
    }


def validator_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    éªŒè¯ JSON èŠ‚ç‚¹

    éªŒè¯ç”Ÿæˆçš„ JSON æ˜¯å¦ç¬¦åˆ Schema å’Œä¸šåŠ¡è§„åˆ™ã€‚
    """
    generated_json = state["generated_json"]
    logger.info("éªŒè¯ç”Ÿæˆçš„ JSON")

    # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
    messages = []

    # æ·»åŠ å¼€å§‹éªŒè¯çš„æ¶ˆæ¯
    messages.append(AIMessage(content="ğŸ” æ­£åœ¨éªŒè¯æŠ€èƒ½é…ç½®çš„åˆæ³•æ€§..."))

    errors = []

    try:
        # ä» Markdown ä¸­æå– JSON
        json_content = extract_json_from_markdown(generated_json)
        logger.info(f"æå–çš„ JSON é•¿åº¦: {len(json_content)}")
        logger.debug(f"æå–çš„ JSON å†…å®¹é¢„è§ˆ: {json_content[:500]}...")  # åªè®°å½•å‰500å­—ç¬¦

        # è§£æ JSON
        skill_data = json.loads(json_content)

        # åŸºç¡€éªŒè¯
        required_fields = ["skillName", "skillId", "actions"]
        for field in required_fields:
            if field not in skill_data:
                errors.append(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")

        # Actions éªŒè¯
        if "actions" in skill_data:
            actions = skill_data["actions"]
            if not isinstance(actions, list):
                errors.append("actions å­—æ®µå¿…é¡»æ˜¯æ•°ç»„")
            elif len(actions) == 0:
                errors.append("actions æ•°ç»„ä¸èƒ½ä¸ºç©º")
            else:
                # éªŒè¯æ¯ä¸ª Action
                for i, action in enumerate(actions):
                    if not isinstance(action, dict):
                        errors.append(f"Action[{i}] å¿…é¡»æ˜¯å¯¹è±¡")
                    elif "actionType" not in action:
                        errors.append(f"Action[{i}] ç¼ºå°‘ actionType å­—æ®µ")

        # æ•°å€¼èŒƒå›´éªŒè¯
        if "cooldown" in skill_data:
            cooldown = skill_data["cooldown"]
            if not isinstance(cooldown, (int, float)) or cooldown < 0:
                errors.append(f"cooldown å¿…é¡»æ˜¯éè´Ÿæ•°ï¼Œå½“å‰å€¼: {cooldown}")

    except json.JSONDecodeError as e:
        errors.append(f"JSON è§£æå¤±è´¥: {str(e)}")
    except Exception as e:
        errors.append(f"éªŒè¯å¼‚å¸¸: {str(e)}")

    if errors:
        logger.warning(f"éªŒè¯å¤±è´¥ï¼Œå‘ç° {len(errors)} ä¸ªé”™è¯¯")
        errors_list = "\n".join([f"â€¢ {err}" for err in errors])
        message = f"âš ï¸ **éªŒè¯å¤±è´¥**ï¼Œå‘ç° {len(errors)} ä¸ªé”™è¯¯ï¼š\n\n{errors_list}"
    else:
        logger.info("éªŒè¯é€šè¿‡")
        message = "âœ… **éªŒè¯é€šè¿‡ï¼** æŠ€èƒ½é…ç½®ç¬¦åˆè§„èŒƒã€‚"

    messages.append(AIMessage(content=message))

    return {
        "validation_errors": errors,
        "messages": messages
    }


def fixer_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    ä¿®å¤ JSON èŠ‚ç‚¹

    æ ¹æ®éªŒè¯é”™è¯¯ï¼Œä½¿ç”¨ LLM ä¿®å¤ JSONã€‚
    """
    from ..prompts.prompt_manager import get_prompt_manager

    generated_json = state["generated_json"]
    errors = state["validation_errors"]

    logger.info(f"ä¿®å¤ JSONï¼Œé”™è¯¯æ•°: {len(errors)}")

    # æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯
    errors_text = "\n".join([f"{i+1}. {err}" for i, err in enumerate(errors)])

    # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
    messages = []

    # æ·»åŠ å¼€å§‹ä¿®å¤çš„æ¶ˆæ¯
    messages.append(AIMessage(content=f"ğŸ” å‘ç° {len(errors)} ä¸ªé”™è¯¯ï¼Œæ­£åœ¨è°ƒç”¨ DeepSeek AI è¿›è¡Œä¿®å¤...\n\né”™è¯¯åˆ—è¡¨ï¼š\n{errors_text}"))

    # è·å– Prompt
    prompt_mgr = get_prompt_manager()
    prompt = prompt_mgr.get_prompt("validation_fix")

    # è°ƒç”¨ LLM
    llm = get_llm(temperature=0.3)  # ä¿®å¤æ—¶ä½¿ç”¨æ›´ä½æ¸©åº¦
    chain = prompt | llm

    response = chain.invoke({
        "errors": errors_text,
        "json": generated_json
    })

    fixed_json = response.content

    # æ·»åŠ  DeepSeek ä¿®å¤å›åº”
    messages.append(AIMessage(content=f"ğŸ’¬ **DeepSeek å›åº”ï¼š**\n\nå·²é’ˆå¯¹ {len(errors)} ä¸ªé”™è¯¯è¿›è¡Œä¿®å¤ (å°è¯• {state['retry_count'] + 1}/{state['max_retries']})ã€‚"))

    # æ˜¾ç¤ºä¿®å¤åçš„JSON
    display_message = f"ğŸ”§ **å·²ä¿®å¤æŠ€èƒ½é…ç½®ï¼š**\n\n```json\n{fixed_json}\n```"
    messages.append(AIMessage(content=display_message))

    return {
        "generated_json": fixed_json,
        "retry_count": state["retry_count"] + 1,
        "messages": messages
    }


def finalize_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    æœ€ç»ˆåŒ–èŠ‚ç‚¹

    å°†ç”Ÿæˆçš„ JSON è§£æä¸ºæœ€ç»ˆç»“æœã€‚
    """
    generated_json = state["generated_json"]

    try:
        # ä» Markdown ä¸­æå– JSON
        json_content = extract_json_from_markdown(generated_json)
        logger.info(f"æœ€ç»ˆåŒ–ï¼šæå–çš„ JSON é•¿åº¦: {len(json_content)}")

        final_result = json.loads(json_content)
        logger.info("æŠ€èƒ½ç”ŸæˆæˆåŠŸ")
    except json.JSONDecodeError as e:
        final_result = {
            "error": f"JSON è§£æå¤±è´¥: {str(e)}",
            "raw_json": generated_json
        }
        logger.error(f"æœ€ç»ˆ JSON è§£æå¤±è´¥: {e}")

    return {
        "final_result": final_result,
        "messages": [HumanMessage(content="æŠ€èƒ½ç”Ÿæˆå®Œæˆ")]
    }


# ==================== æ¡ä»¶åˆ¤æ–­å‡½æ•° ====================

def should_continue(state: SkillGenerationState) -> str:
    """
    åˆ¤æ–­æ˜¯å¦ç»§ç»­ä¿®å¤å¾ªç¯

    Returns:
        "fix" - ç»§ç»­ä¿®å¤
        "finalize" - ç»“æŸï¼Œè¿”å›ç»“æœ
    """
    errors = state.get("validation_errors", [])
    retry_count = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 3)

    # å¦‚æœæ²¡æœ‰é”™è¯¯ï¼Œç»“æŸ
    if not errors:
        return "finalize"

    # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»“æŸ
    if retry_count >= max_retries:
        logger.warning(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œåœæ­¢ä¿®å¤")
        return "finalize"

    # ç»§ç»­ä¿®å¤
    return "fix"
