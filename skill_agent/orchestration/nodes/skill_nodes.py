"""
LangGraph èŠ‚ç‚¹å®ç°
å®šä¹‰ Graph ä¸­çš„å„ä¸ªèŠ‚ç‚¹ï¼ˆgeneratorã€validatorã€fixer ç­‰ï¼‰
"""

import json
import logging
import time
from typing import Any, Dict, List, TypedDict, Annotated
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

from .json_utils import extract_json_from_markdown

logger = logging.getLogger(__name__)


# ==================== State å®šä¹‰ ====================

class SkillGenerationState(TypedDict):
    """æŠ€èƒ½ç”Ÿæˆæµç¨‹çš„çŠ¶æ€"""
    requirement: str  # ç”¨æˆ·éœ€æ±‚æè¿°
    similar_skills: List[Dict[str, Any]]  # æ£€ç´¢åˆ°çš„ç›¸ä¼¼æŠ€èƒ½
    generated_json: str  # ç”Ÿæˆçš„ JSON
    validation_errors: List[str]  # éªŒè¯é”™è¯¯åˆ—è¡¨
    retry_count: int  # é‡è¯•æ¬¡æ•°
    max_retries: int  # æœ€å¤§é‡è¯•æ¬¡æ•°
    final_result: Dict[str, Any]  # æœ€ç»ˆç»“æœ
    messages: Annotated[List, "append"]  # å¯¹è¯å†å²


# ==================== LLM åˆå§‹åŒ– ====================

def get_llm(model: str = "deepseek-reasoner", temperature: float = 1.0):
    """
    è·å– LLM å®ä¾‹ï¼ˆä½¿ç”¨ LangChain ChatOpenAI å…¼å®¹ DeepSeekï¼‰

    Args:
        model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨ deepseek-reasoner æ€è€ƒæ¨¡å‹ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼ˆdeepseek-reasoner æ¨èä½¿ç”¨ 1.0ï¼‰

    Returns:
        ChatOpenAI å®ä¾‹
    """
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        raise ValueError("ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY æœªè®¾ç½®")

    # ä»ç¯å¢ƒå˜é‡è¯»å–è¶…æ—¶é…ç½®ï¼ˆé»˜è®¤ 120 ç§’ï¼Œå› ä¸º reasoner æ¨¡å‹æ¨ç†æ—¶é—´é•¿ï¼‰
    timeout = int(os.getenv("DEEPSEEK_TIMEOUT", "120"))
    max_retries = int(os.getenv("DEEPSEEK_MAX_RETRIES", "2"))

    logger.info(f"åˆå§‹åŒ– LLM: model={model}, timeout={timeout}s, max_retries={max_retries}")

    return ChatOpenAI(
        model=model,
        temperature=temperature,
        api_key=api_key,
        base_url="https://api.deepseek.com/v1",
        timeout=timeout,  # è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
        max_retries=max_retries,  # æœ€å¤§é‡è¯•æ¬¡æ•°
    )


# ==================== èŠ‚ç‚¹å‡½æ•° ====================

def retriever_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    æ£€ç´¢ç›¸ä¼¼æŠ€èƒ½èŠ‚ç‚¹

    æ ¹æ®éœ€æ±‚æè¿°ï¼Œä» RAG Core æ£€ç´¢ç›¸ä¼¼æŠ€èƒ½ä½œä¸ºå‚è€ƒã€‚
    """
    from ..tools.rag_tools import search_skills_semantic

    requirement = state["requirement"]
    logger.info(f"æ£€ç´¢ç›¸ä¼¼æŠ€èƒ½: {requirement}")

    # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
    messages = []

    # æ·»åŠ å¼€å§‹æ£€ç´¢çš„æ¶ˆæ¯
    messages.append(AIMessage(content=f"ğŸ” æ­£åœ¨ä»æŠ€èƒ½åº“ä¸­æ£€ç´¢ä¸ã€Œ{requirement}ã€ç›¸å…³çš„æŠ€èƒ½..."))

    # è°ƒç”¨ RAG å·¥å…·æ£€ç´¢ï¼ˆæ·»åŠ æ€§èƒ½æ—¥å¿—ï¼‰
    # top_k=2 ä¼˜åŒ–ï¼šå‡å°‘æ£€ç´¢æ•°é‡ä»¥æå‡é€Ÿåº¦ï¼Œ2ä¸ªé«˜è´¨é‡å‚è€ƒå·²è¶³å¤Ÿ
    start_time = time.time()
    results = search_skills_semantic.invoke({"query": requirement, "top_k": 2})
    rag_elapsed = time.time() - start_time
    logger.info(f"â±ï¸ RAG æ£€ç´¢è€—æ—¶: {rag_elapsed:.2f}s")

    # æ„å»ºè¯¦ç»†çš„æ£€ç´¢ç»“æœæ¶ˆæ¯
    if results:
        skills_summary = "\n".join([
            f"â€¢ **{skill.get('skill_name', 'Unknown')}** (ç›¸ä¼¼åº¦: {skill.get('similarity', 0):.2%})"
            for skill in results[:3]
        ])
        message = f"ğŸ“š **æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸ä¼¼æŠ€èƒ½ï¼š**\n\n{skills_summary}\n\nè¿™äº›æŠ€èƒ½å°†ä½œä¸ºç”Ÿæˆå‚è€ƒã€‚"
    else:
        message = "âš ï¸ æœªæ£€ç´¢åˆ°ç›¸ä¼¼æŠ€èƒ½ï¼Œå°†åŸºäºéœ€æ±‚ç›´æ¥ç”Ÿæˆã€‚"

    messages.append(AIMessage(content=message))

    return {
        "similar_skills": results,
        "messages": messages
    }


def generator_node(state: SkillGenerationState, config: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    ç”ŸæˆæŠ€èƒ½ JSON èŠ‚ç‚¹

    æ ¹æ®éœ€æ±‚å’Œå‚è€ƒæŠ€èƒ½ï¼Œä½¿ç”¨ LLM ç”ŸæˆæŠ€èƒ½é…ç½® JSONã€‚

    Args:
        state: æŠ€èƒ½ç”ŸæˆçŠ¶æ€
        config: LangGraph é…ç½®ï¼ˆåŒ…å« thread_idï¼‰
    """
    from ..prompts.prompt_manager import get_prompt_manager

    requirement = state["requirement"]
    similar_skills = state.get("similar_skills", [])

    # ğŸ”¥ è·å– thread_id å’Œ chunk é˜Ÿåˆ—
    thread_id = None
    chunk_queue = None
    if config and "configurable" in config:
        thread_id = config["configurable"].get("thread_id")
        if thread_id:
            # å¯¼å…¥å…¨å±€é˜Ÿåˆ—
            import sys
            langgraph_server = sys.modules.get('langgraph_server')
            if langgraph_server and hasattr(langgraph_server, 'chunk_queues'):
                chunk_queue = langgraph_server.chunk_queues.get(thread_id)
                logger.info(f"âœ… Got chunk queue for thread {thread_id}")

    logger.info(f"ç”ŸæˆæŠ€èƒ½ JSON: {requirement}")

    # æ ¼å¼åŒ–ç›¸ä¼¼æŠ€èƒ½
    similar_skills_text = "\n\n".join([
        f"æŠ€èƒ½ {i+1}: {skill.get('skill_name', 'Unknown')}\n{json.dumps(skill.get('skill_data', {}), indent=2, ensure_ascii=False)}"
        for i, skill in enumerate(similar_skills[:2])  # åªå–å‰2ä¸ª
    ])

    # è·å– Prompt
    prompt_mgr = get_prompt_manager()
    prompt = prompt_mgr.get_prompt("skill_generation")

    # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
    messages = []

    # æ·»åŠ å¼€å§‹ç”Ÿæˆçš„æ¶ˆæ¯
    messages.append(AIMessage(content="ğŸ¤– æ­£åœ¨è°ƒç”¨ DeepSeek AI ç”ŸæˆæŠ€èƒ½é…ç½®..."))

    # è°ƒç”¨ LLM (ä½¿ç”¨æµå¼è¾“å‡º)
    llm = get_llm()
    chain = prompt | llm

    logger.info(f"â³ æ­£åœ¨è°ƒç”¨ DeepSeek API (æµå¼è¾“å‡º)...")
    api_start_time = time.time()
    first_chunk_time = None

    # æ”¶é›†æµå¼è¾“å‡ºï¼ˆåˆ†ç¦»æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆè¾“å‡ºï¼‰
    full_reasoning = ""  # æ€è€ƒè¿‡ç¨‹
    full_content = ""    # æœ€ç»ˆè¾“å‡º

    # ğŸ”¥ ç”Ÿæˆå”¯ä¸€çš„ message_id ç”¨äºè·Ÿè¸ªæµå¼æ¶ˆæ¯
    thinking_message_id = f"thinking_{thread_id}_{api_start_time}" if thread_id else None
    content_message_id = f"content_{thread_id}_{api_start_time}" if thread_id else None

    # æµå¼è°ƒç”¨
    for chunk in chain.stream({
        "requirement": requirement,
        "similar_skills": similar_skills_text or "æ— å‚è€ƒæŠ€èƒ½"
    }):
        # è®°å½•é¦–å­—èŠ‚æ—¶é—´ï¼ˆTTFBï¼‰
        if first_chunk_time is None:
            first_chunk_time = time.time()
            ttfb = first_chunk_time - api_start_time
            logger.info(f"âš¡ é¦–å­—èŠ‚å»¶è¿Ÿ (TTFB): {ttfb:.2f}s")

        # å°è¯•æå– reasoning_content (DeepSeek Reasoner ç‰¹æœ‰)
        # æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ä½ç½®
        reasoning_chunk = None

        # æ–¹æ³•1: æ£€æŸ¥ response_metadata
        if hasattr(chunk, 'response_metadata') and isinstance(chunk.response_metadata, dict):
            reasoning_chunk = chunk.response_metadata.get('reasoning_content')

        # æ–¹æ³•2: æ£€æŸ¥ additional_kwargs
        if not reasoning_chunk and hasattr(chunk, 'additional_kwargs') and isinstance(chunk.additional_kwargs, dict):
            reasoning_chunk = chunk.additional_kwargs.get('reasoning_content')

        # æ–¹æ³•3: ç›´æ¥æ£€æŸ¥å±æ€§
        if not reasoning_chunk and hasattr(chunk, 'reasoning_content'):
            reasoning_chunk = chunk.reasoning_content

        # ç´¯ç§¯æ€è€ƒå†…å®¹
        if reasoning_chunk:
            full_reasoning += reasoning_chunk

            # ğŸ”¥ å®æ—¶æ¨é€ thinking chunk åˆ°é˜Ÿåˆ—
            if chunk_queue and thinking_message_id:
                try:
                    chunk_queue.put_nowait({
                        "type": "thinking_chunk",
                        "message_id": thinking_message_id,
                        "chunk": reasoning_chunk
                    })
                except Exception as e:
                    logger.error(f"âŒ Failed to push thinking chunk: {e}")

        # ç´¯ç§¯æœ€ç»ˆå†…å®¹
        if hasattr(chunk, 'content') and chunk.content:
            full_content += chunk.content

            # ğŸ”¥ å®æ—¶æ¨é€ content chunk åˆ°é˜Ÿåˆ—
            if chunk_queue and content_message_id:
                try:
                    chunk_queue.put_nowait({
                        "type": "content_chunk",
                        "message_id": content_message_id,
                        "chunk": chunk.content
                    })
                except Exception as e:
                    logger.error(f"âŒ Failed to push content chunk: {e}")

    # è®°å½•å®Œæ•´å“åº”å’Œæ€§èƒ½æŒ‡æ ‡
    api_total_time = time.time() - api_start_time
    logger.info(f"âœ… DeepSeek API å“åº”å®Œæˆ")
    logger.info(f"â±ï¸ DeepSeek API æ€»è€—æ—¶: {api_total_time:.2f}s")
    logger.info(f"ğŸ§  æ€è€ƒå†…å®¹é•¿åº¦: {len(full_reasoning)} å­—ç¬¦")
    logger.info(f"ğŸ“ è¾“å‡ºå†…å®¹é•¿åº¦: {len(full_content)} å­—ç¬¦")

    if full_reasoning:
        logger.info(f"ğŸ’­ æ€è€ƒè¿‡ç¨‹é¢„è§ˆ:\n{full_reasoning[:300]}...")
    logger.info(f"ğŸ“„ DeepSeek å®Œæ•´è¾“å‡º:\n{full_content}")

    generated_json = full_content

    # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ï¼Œä½œä¸ºå•ç‹¬çš„æ¶ˆæ¯æ·»åŠ ï¼ˆæ ‡è®°ä¸º thinkingï¼‰
    if full_reasoning:
        messages.append(AIMessage(
            content=full_reasoning,
            additional_kwargs={"thinking": True}
        ))

    # æ·»åŠ  DeepSeek çš„æœ€ç»ˆè¾“å‡º
    messages.append(AIMessage(content=full_content))

    return {
        "generated_json": generated_json,
        "messages": messages
    }


def validator_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    éªŒè¯ JSON èŠ‚ç‚¹

    éªŒè¯ç”Ÿæˆçš„ JSON æ˜¯å¦ç¬¦åˆ Schema å’Œä¸šåŠ¡è§„åˆ™ã€‚
    """
    generated_json = state["generated_json"]
    logger.info("éªŒè¯ç”Ÿæˆçš„ JSON")

    # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
    messages = []

    # æ·»åŠ å¼€å§‹éªŒè¯çš„æ¶ˆæ¯
    messages.append(AIMessage(content="ğŸ” æ­£åœ¨éªŒè¯æŠ€èƒ½é…ç½®çš„åˆæ³•æ€§..."))

    errors = []

    try:
        # ä» Markdown ä¸­æå– JSON
        json_content = extract_json_from_markdown(generated_json)
        logger.info(f"æå–çš„ JSON é•¿åº¦: {len(json_content)}")
        logger.debug(f"æå–çš„ JSON å†…å®¹é¢„è§ˆ: {json_content[:500]}...")  # åªè®°å½•å‰500å­—ç¬¦

        # è§£æ JSON
        skill_data = json.loads(json_content)

        # åŸºç¡€éªŒè¯
        required_fields = ["skillName", "skillId", "actions"]
        for field in required_fields:
            if field not in skill_data:
                errors.append(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")

        # Actions éªŒè¯
        if "actions" in skill_data:
            actions = skill_data["actions"]
            if not isinstance(actions, list):
                errors.append("actions å­—æ®µå¿…é¡»æ˜¯æ•°ç»„")
            elif len(actions) == 0:
                errors.append("actions æ•°ç»„ä¸èƒ½ä¸ºç©º")
            else:
                # éªŒè¯æ¯ä¸ª Action
                for i, action in enumerate(actions):
                    if not isinstance(action, dict):
                        errors.append(f"Action[{i}] å¿…é¡»æ˜¯å¯¹è±¡")
                    elif "actionType" not in action:
                        errors.append(f"Action[{i}] ç¼ºå°‘ actionType å­—æ®µ")

        # æ•°å€¼èŒƒå›´éªŒè¯
        if "cooldown" in skill_data:
            cooldown = skill_data["cooldown"]
            if not isinstance(cooldown, (int, float)) or cooldown < 0:
                errors.append(f"cooldown å¿…é¡»æ˜¯éè´Ÿæ•°ï¼Œå½“å‰å€¼: {cooldown}")

    except json.JSONDecodeError as e:
        errors.append(f"JSON è§£æå¤±è´¥: {str(e)}")
    except Exception as e:
        errors.append(f"éªŒè¯å¼‚å¸¸: {str(e)}")

    if errors:
        logger.warning(f"éªŒè¯å¤±è´¥ï¼Œå‘ç° {len(errors)} ä¸ªé”™è¯¯")
        errors_list = "\n".join([f"â€¢ {err}" for err in errors])
        message = f"âš ï¸ **éªŒè¯å¤±è´¥**ï¼Œå‘ç° {len(errors)} ä¸ªé”™è¯¯ï¼š\n\n{errors_list}"
    else:
        logger.info("éªŒè¯é€šè¿‡")
        message = "âœ… **éªŒè¯é€šè¿‡ï¼** æŠ€èƒ½é…ç½®ç¬¦åˆè§„èŒƒã€‚"

    messages.append(AIMessage(content=message))

    return {
        "validation_errors": errors,
        "messages": messages
    }


def fixer_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    ä¿®å¤ JSON èŠ‚ç‚¹

    æ ¹æ®éªŒè¯é”™è¯¯ï¼Œä½¿ç”¨ LLM ä¿®å¤ JSONã€‚
    """
    from ..prompts.prompt_manager import get_prompt_manager

    generated_json = state["generated_json"]
    errors = state["validation_errors"]

    logger.info(f"ä¿®å¤ JSONï¼Œé”™è¯¯æ•°: {len(errors)}")

    # æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯
    errors_text = "\n".join([f"{i+1}. {err}" for i, err in enumerate(errors)])

    # å‡†å¤‡æ¶ˆæ¯åˆ—è¡¨
    messages = []

    # æ·»åŠ å¼€å§‹ä¿®å¤çš„æ¶ˆæ¯
    messages.append(AIMessage(content=f"ğŸ” å‘ç° {len(errors)} ä¸ªé”™è¯¯ï¼Œæ­£åœ¨è°ƒç”¨ DeepSeek AI è¿›è¡Œä¿®å¤...\n\né”™è¯¯åˆ—è¡¨ï¼š\n{errors_text}"))

    # è·å– Prompt
    prompt_mgr = get_prompt_manager()
    prompt = prompt_mgr.get_prompt("validation_fix")

    # è°ƒç”¨ LLM
    llm = get_llm(temperature=0.3)  # ä¿®å¤æ—¶ä½¿ç”¨æ›´ä½æ¸©åº¦
    chain = prompt | llm

    response = chain.invoke({
        "errors": errors_text,
        "json": generated_json
    })

    fixed_json = response.content

    # æ·»åŠ  DeepSeek ä¿®å¤å›åº”
    messages.append(AIMessage(content=f"ğŸ’¬ **DeepSeek å›åº”ï¼š**\n\nå·²é’ˆå¯¹ {len(errors)} ä¸ªé”™è¯¯è¿›è¡Œä¿®å¤ (å°è¯• {state['retry_count'] + 1}/{state['max_retries']})ã€‚"))

    # æ˜¾ç¤ºä¿®å¤åçš„JSON
    display_message = f"ğŸ”§ **å·²ä¿®å¤æŠ€èƒ½é…ç½®ï¼š**\n\n```json\n{fixed_json}\n```"
    messages.append(AIMessage(content=display_message))

    return {
        "generated_json": fixed_json,
        "retry_count": state["retry_count"] + 1,
        "messages": messages
    }


def finalize_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    æœ€ç»ˆåŒ–èŠ‚ç‚¹

    å°†ç”Ÿæˆçš„ JSON è§£æä¸ºæœ€ç»ˆç»“æœã€‚
    """
    generated_json = state["generated_json"]

    try:
        # ä» Markdown ä¸­æå– JSON
        json_content = extract_json_from_markdown(generated_json)
        logger.info(f"æœ€ç»ˆåŒ–ï¼šæå–çš„ JSON é•¿åº¦: {len(json_content)}")

        final_result = json.loads(json_content)
        logger.info("æŠ€èƒ½ç”ŸæˆæˆåŠŸ")
    except json.JSONDecodeError as e:
        final_result = {
            "error": f"JSON è§£æå¤±è´¥: {str(e)}",
            "raw_json": generated_json
        }
        logger.error(f"æœ€ç»ˆ JSON è§£æå¤±è´¥: {e}")

    return {
        "final_result": final_result,
        "messages": [HumanMessage(content="æŠ€èƒ½ç”Ÿæˆå®Œæˆ")]
    }


# ==================== æ¡ä»¶åˆ¤æ–­å‡½æ•° ====================

def should_continue(state: SkillGenerationState) -> str:
    """
    åˆ¤æ–­æ˜¯å¦ç»§ç»­ä¿®å¤å¾ªç¯

    Returns:
        "fix" - ç»§ç»­ä¿®å¤
        "finalize" - ç»“æŸï¼Œè¿”å›ç»“æœ
    """
    errors = state.get("validation_errors", [])
    retry_count = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 3)

    # å¦‚æœæ²¡æœ‰é”™è¯¯ï¼Œç»“æŸ
    if not errors:
        return "finalize"

    # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»“æŸ
    if retry_count >= max_retries:
        logger.warning(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œåœæ­¢ä¿®å¤")
        return "finalize"

    # ç»§ç»­ä¿®å¤
    return "fix"
