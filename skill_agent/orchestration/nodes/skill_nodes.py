"""
LangGraph èŠ‚ç‚¹å®ç°
å®šä¹‰ Graph ä¸­çš„å„ä¸ªèŠ‚ç‚¹ï¼ˆgeneratorã€validatorã€fixer ç­‰ï¼‰
"""

import json
import logging
from typing import Any, Dict, List, TypedDict, Annotated
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
import os

logger = logging.getLogger(__name__)


# ==================== State å®šä¹‰ ====================

class SkillGenerationState(TypedDict):
    """æŠ€èƒ½ç”Ÿæˆæµç¨‹çš„çŠ¶æ€?""
    requirement: str  # ç”¨æˆ·éœ€æ±‚æè¿?    similar_skills: List[Dict[str, Any]]  # æ£€ç´¢åˆ°çš„ç›¸ä¼¼æŠ€èƒ?    generated_json: str  # ç”Ÿæˆçš?JSON
    validation_errors: List[str]  # éªŒè¯é”™è¯¯åˆ—è¡¨
    retry_count: int  # é‡è¯•æ¬¡æ•°
    max_retries: int  # æœ€å¤§é‡è¯•æ¬¡æ•?    final_result: Dict[str, Any]  # æœ€ç»ˆç»“æ?    messages: Annotated[List, "append"]  # å¯¹è¯å†å²


# ==================== LLM åˆå§‹åŒ?====================

def get_llm(model: str = "deepseek-chat", temperature: float = 0.7):
    """
    è·å– LLM å®ä¾‹ï¼ˆä½¿ç”?LangChain ChatOpenAI å…¼å®¹ DeepSeekï¼?
    Args:
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°

    Returns:
        ChatOpenAI å®ä¾‹
    """
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        raise ValueError("ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY æœªè®¾ç½?)

    return ChatOpenAI(
        model=model,
        temperature=temperature,
        api_key=api_key,
        base_url="https://api.deepseek.com/v1",
    )


# ==================== èŠ‚ç‚¹å‡½æ•° ====================

def retriever_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    æ£€ç´¢ç›¸ä¼¼æŠ€èƒ½èŠ‚ç‚?
    æ ¹æ®éœ€æ±‚æè¿°ï¼Œä»?RAG Core æ£€ç´¢ç›¸ä¼¼æŠ€èƒ½ä½œä¸ºå‚è€ƒã€?    """
    from ..tools.rag_tools import search_skills_semantic

    requirement = state["requirement"]
    logger.info(f"æ£€ç´¢ç›¸ä¼¼æŠ€èƒ? {requirement}")

    # è°ƒç”¨ RAG å·¥å…·æ£€ç´?    results = search_skills_semantic.invoke({"query": requirement, "top_k": 3})

    return {
        "similar_skills": results,
        "messages": [HumanMessage(content=f"æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸ä¼¼æŠ€èƒ?)]
    }


def generator_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    ç”ŸæˆæŠ€èƒ?JSON èŠ‚ç‚¹

    æ ¹æ®éœ€æ±‚å’Œå‚è€ƒæŠ€èƒ½ï¼Œä½¿ç”¨ LLM ç”ŸæˆæŠ€èƒ½é…ç½?JSONã€?    """
    from ..prompts.prompt_manager import get_prompt_manager

    requirement = state["requirement"]
    similar_skills = state.get("similar_skills", [])

    logger.info(f"ç”ŸæˆæŠ€èƒ?JSON: {requirement}")

    # æ ¼å¼åŒ–ç›¸ä¼¼æŠ€èƒ?    similar_skills_text = "\n\n".join([
        f"æŠ€èƒ?{i+1}: {skill.get('skill_name', 'Unknown')}\n{json.dumps(skill.get('skill_data', {}), indent=2, ensure_ascii=False)}"
        for i, skill in enumerate(similar_skills[:2])  # åªå–å‰?ä¸?    ])

    # è·å– Prompt
    prompt_mgr = get_prompt_manager()
    prompt = prompt_mgr.get_prompt("skill_generation")

    # è°ƒç”¨ LLM
    llm = get_llm()
    chain = prompt | llm

    response = chain.invoke({
        "requirement": requirement,
        "similar_skills": similar_skills_text or "æ— å‚è€ƒæŠ€èƒ?
    })

    generated_json = response.content

    return {
        "generated_json": generated_json,
        "messages": [AIMessage(content=f"å·²ç”ŸæˆæŠ€èƒ?JSON (é•¿åº¦: {len(generated_json)} å­—ç¬¦)")]
    }


def validator_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    éªŒè¯ JSON èŠ‚ç‚¹

    éªŒè¯ç”Ÿæˆçš?JSON æ˜¯å¦ç¬¦åˆ Schema å’Œä¸šåŠ¡è§„åˆ™ã€?    """
    generated_json = state["generated_json"]
    logger.info("éªŒè¯ç”Ÿæˆçš?JSON")

    errors = []

    try:
        # è§£æ JSON
        skill_data = json.loads(generated_json)

        # åŸºç¡€éªŒè¯
        required_fields = ["skillName", "skillId", "actions"]
        for field in required_fields:
            if field not in skill_data:
                errors.append(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")

        # Actions éªŒè¯
        if "actions" in skill_data:
            actions = skill_data["actions"]
            if not isinstance(actions, list):
                errors.append("actions å­—æ®µå¿…é¡»æ˜¯æ•°ç»?)
            elif len(actions) == 0:
                errors.append("actions æ•°ç»„ä¸èƒ½ä¸ºç©º")
            else:
                # éªŒè¯æ¯ä¸ª Action
                for i, action in enumerate(actions):
                    if not isinstance(action, dict):
                        errors.append(f"Action[{i}] å¿…é¡»æ˜¯å¯¹è±?)
                    elif "actionType" not in action:
                        errors.append(f"Action[{i}] ç¼ºå°‘ actionType å­—æ®µ")

        # æ•°å€¼èŒƒå›´éªŒè¯?        if "cooldown" in skill_data:
            cooldown = skill_data["cooldown"]
            if not isinstance(cooldown, (int, float)) or cooldown < 0:
                errors.append(f"cooldown å¿…é¡»æ˜¯éè´Ÿæ•°ï¼Œå½“å‰å€? {cooldown}")

    except json.JSONDecodeError as e:
        errors.append(f"JSON è§£æå¤±è´¥: {str(e)}")
    except Exception as e:
        errors.append(f"éªŒè¯å¼‚å¸¸: {str(e)}")

    if errors:
        logger.warning(f"éªŒè¯å¤±è´¥ï¼Œå‘ç?{len(errors)} ä¸ªé”™è¯?)
    else:
        logger.info("éªŒè¯é€šè¿‡")

    return {
        "validation_errors": errors,
        "messages": [
            HumanMessage(content=f"éªŒè¯ç»“æœ: {'å¤±è´¥' if errors else 'é€šè¿‡'} ({len(errors)} ä¸ªé”™è¯?")
        ]
    }


def fixer_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    ä¿®å¤ JSON èŠ‚ç‚¹

    æ ¹æ®éªŒè¯é”™è¯¯ï¼Œä½¿ç”?LLM ä¿®å¤ JSONã€?    """
    from ..prompts.prompt_manager import get_prompt_manager

    generated_json = state["generated_json"]
    errors = state["validation_errors"]

    logger.info(f"ä¿®å¤ JSONï¼Œé”™è¯¯æ•°: {len(errors)}")

    # æ ¼å¼åŒ–é”™è¯¯ä¿¡æ?    errors_text = "\n".join([f"{i+1}. {err}" for i, err in enumerate(errors)])

    # è·å– Prompt
    prompt_mgr = get_prompt_manager()
    prompt = prompt_mgr.get_prompt("validation_fix")

    # è°ƒç”¨ LLM
    llm = get_llm(temperature=0.3)  # ä¿®å¤æ—¶ä½¿ç”¨æ›´ä½æ¸©åº?    chain = prompt | llm

    response = chain.invoke({
        "errors": errors_text,
        "json": generated_json
    })

    fixed_json = response.content

    return {
        "generated_json": fixed_json,
        "retry_count": state["retry_count"] + 1,
        "messages": [AIMessage(content=f"å·²ä¿®å¤?JSON (é‡è¯• {state['retry_count'] + 1}/{state['max_retries']})")]
    }


def finalize_node(state: SkillGenerationState) -> Dict[str, Any]:
    """
    æœ€ç»ˆåŒ–èŠ‚ç‚¹

    å°†ç”Ÿæˆçš„ JSON è§£æä¸ºæœ€ç»ˆç»“æœã€?    """
    generated_json = state["generated_json"]

    try:
        final_result = json.loads(generated_json)
        logger.info("æŠ€èƒ½ç”ŸæˆæˆåŠ?)
    except json.JSONDecodeError:
        final_result = {
            "error": "JSON è§£æå¤±è´¥",
            "raw_json": generated_json
        }
        logger.error("æœ€ç»?JSON è§£æå¤±è´¥")

    return {
        "final_result": final_result,
        "messages": [HumanMessage(content="æŠ€èƒ½ç”Ÿæˆå®Œæˆ?)]
    }


# ==================== æ¡ä»¶åˆ¤æ–­å‡½æ•° ====================

def should_continue(state: SkillGenerationState) -> str:
    """
    åˆ¤æ–­æ˜¯å¦ç»§ç»­ä¿®å¤å¾ªç¯

    Returns:
        "fix" - ç»§ç»­ä¿®å¤
        "finalize" - ç»“æŸï¼Œè¿”å›ç»“æ?    """
    errors = state.get("validation_errors", [])
    retry_count = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 3)

    # å¦‚æœæ²¡æœ‰é”™è¯¯ï¼Œç»“æ?    if not errors:
        return "finalize"

    # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œç»“æŸ
    if retry_count >= max_retries:
        logger.warning(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•?{max_retries}ï¼Œåœæ­¢ä¿®å¤?)
        return "finalize"

    # ç»§ç»­ä¿®å¤
    return "fix"


# ==================== Action æœç´¢èŠ‚ç‚¹ ====================

def action_search_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Action æœç´¢èŠ‚ç‚¹

    æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°æœç´¢ç›¸å…³çš?Action è„šæœ¬ã€?
    Args:
        state: åŒ…å« requirement (æœç´¢æŸ¥è¯¢) å’?messages çš„çŠ¶æ€å­—å…?
    Returns:
        æ›´æ–°åçš„çŠ¶æ€ï¼ŒåŒ…å«æœç´¢ç»“æœå’Œæ¶ˆæ?    """
    from ..tools.rag_tools import search_actions

    requirement = state.get("requirement", "")
    top_k = state.get("top_k", 5)

    logger.info(f"æœç´¢ Action: {requirement}")

    # è°ƒç”¨ RAG å·¥å…·æœç´¢ Action
    results = search_actions.invoke({"query": requirement, "top_k": top_k})

    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    if isinstance(results, dict) and "error" in results:
        logger.error(f"Action æœç´¢å¤±è´¥: {results['error']}")
        return {
            "search_results": [],
            "error": results["error"],
            "messages": [AIMessage(content=f"â?æœç´¢å¤±è´¥: {results['error']}")]
        }

    # æ ¼å¼åŒ–æœç´¢ç»“æœä¸ºå¯è¯»æ–‡æœ¬
    if results:
        result_summary = f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…?Action:\n\n"
        for i, action in enumerate(results[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸?            action_name = action.get('action_name', 'Unknown')
            action_type = action.get('action_type', 'N/A')
            similarity = action.get('similarity_score', 0)
            parameters = action.get('parameters', [])

            result_summary += f"{i}. **{action_name}** (ç±»å‹: {action_type}, ç›¸ä¼¼åº? {similarity:.2f})\n"

            # æ˜¾ç¤ºå‚æ•°ä¿¡æ¯
            if parameters:
                result_summary += f"   å‚æ•° ({len(parameters)} ä¸?:\n"
                for param in parameters[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªå‚æ•?                    param_name = param.get('name', 'unknown')
                    param_type = param.get('type', 'unknown')
                    default_value = param.get('defaultValue', 'N/A')
                    result_summary += f"   - `{param_name}` ({param_type}, é»˜è®¤: {default_value})\n"

                if len(parameters) > 3:
                    result_summary += f"   - ... è¿˜æœ‰ {len(parameters) - 3} ä¸ªå‚æ•°\n"
            result_summary += "\n"

        message_content = result_summary
    else:
        message_content = "æœªæ‰¾åˆ°åŒ¹é…çš„ Actionï¼Œè¯·å°è¯•å…¶ä»–æè¿°ã€?

    logger.info(f"Action æœç´¢å®Œæˆï¼Œæ‰¾åˆ?{len(results)} ä¸ªç»“æ?)

    return {
        "search_results": results,
        "messages": [AIMessage(content=message_content)]
    }
